{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=8,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                #print(inputs.size())\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            print('{} Rajat Best_Acc: {:.4f} Epoch_Acc: {:.4f}'.format(\n",
    "                phase, best_acc, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                \n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            #collect losses\n",
    "            if phase=='train':\n",
    "                train_losses.append(epoch_loss)\n",
    "            if phase=='val':\n",
    "                val_losses.append(epoch_loss)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model,train_losses,val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            print(\"modi\",inputs.size())\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def plot_losses(train_loss,val_loss):\n",
    "    df = pd.DataFrame(list(zip([i for i in range(0,len(train_losses))],train_loss,val_loss)), \n",
    "               columns =['epoch', 'train_loss','val_loss']) \n",
    "    list_data=[df.train_loss,df.val_loss]\n",
    "    plots=sns.lineplot(data=list_data)\n",
    "    return plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_output(model,train_loss,val_loss,experiment_name):\n",
    "    model_dump_path=experiment_name+'.pt'\n",
    "    torch.save(model.state_dict(),model_dump_path)\n",
    "    csv_dump_path=experiment_name+'.csv'\n",
    "    df = pd.DataFrame(list(zip(train_loss,val_loss)), \n",
    "           columns =[ 'train_loss','val_loss'])\n",
    "    df.to_csv(csv_dump_path)\n",
    "    list_data=[df.train_loss,df.val_loss]\n",
    "    plot_dump_path=experiment_name+'.png'\n",
    "    plots=sns.lineplot(data=list_data)\n",
    "    fig=plots.get_figure()\n",
    "    fig.savefig(plot_dump_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.5518 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.7049\n",
      "val Loss: 0.3631 Acc: 0.8366\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.8366\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3339 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.8366 Epoch_Acc: 0.8730\n",
      "val Loss: 0.2109 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.8366 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.3653 Acc: 0.8402\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8402\n",
      "val Loss: 0.1984 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.3148 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1999 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2830 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1745 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2697 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1658 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.2359 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1556 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.2398 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1540 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.2282 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1582 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.2498 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1503 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.2389 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1571 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.2213 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1503 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.3178 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1551 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.2247 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1546 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.2725 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1531 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1991 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1538 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.2079 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1646 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.2383 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1599 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.2366 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1602 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.2423 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1530 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.2659 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1621 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.2768 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1586 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.2477 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1664 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.2545 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1608 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1718 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1534 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.2071 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1582 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.2082 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1602 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.3001 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1629 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.2679 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1534 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.2447 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1581 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.2791 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1509 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.2231 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1540 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.2565 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1643 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.2843 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1655 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.2262 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1637 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.2238 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1605 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.2340 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1588 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1946 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1623 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.2286 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1561 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.2117 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1530 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.2219 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1589 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.2345 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1635 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.2458 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1610 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.2026 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1526 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.2984 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1601 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.2296 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1561 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.2194 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1799 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.2503 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1570 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.2215 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1529 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.2230 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1674 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.2439 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1621 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.2834 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1569 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.2911 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1571 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.2074 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1587 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.2702 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1589 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.2280 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1564 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.2822 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model_conv,train_losses,val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n",
    "                       num_epochs=300)\n",
    "dump_output(model_conv,train_losses[0:50],val_losses[0:50],'with-pretrained-resnet18_lrscheduler_lastlayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet34(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model_conv,train_losses,val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n",
    "                       num_epochs=300)\n",
    "dump_output(model_conv,train_losses[0:50],val_losses[0:50],'with-pretrained-resnet34_lrscheduler_lastlayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.5865 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6434\n",
      "val Loss: 0.4696 Acc: 0.7582\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.7582\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3424 Acc: 0.8525\n",
      "train Rajat Best_Acc: 0.7582 Epoch_Acc: 0.8525\n",
      "val Loss: 0.1935 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.7582 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.3391 Acc: 0.8484\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.8484\n",
      "val Loss: 0.1773 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2627 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1736 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.3029 Acc: 0.8648\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.8648\n",
      "val Loss: 0.2081 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2341 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1736 Acc: 0.9150\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9150\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.3456 Acc: 0.8525\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.8525\n",
      "val Loss: 0.2076 Acc: 0.9085\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9085\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.1995 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1361 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c68d8e7d3dbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m model_conv,train_losses,val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n\u001b[0;32m---> 20\u001b[0;31m                        num_epochs=300)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdump_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_conv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'with-pretrained-resnet50_lrscheduler_lastlayer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c907b7687a7c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 340\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model_conv,train_losses,val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n",
    "                       num_epochs=300)\n",
    "dump_output(model_conv,train_losses[0:50],val_losses[0:50],'with-pretrained-resnet50_lrscheduler_lastlayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.5424 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6885\n",
      "val Loss: 0.2711 Acc: 0.8954\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.8954\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3108 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.8954 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1992 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.8954 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.2225 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9216 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1775 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9216 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2838 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9281 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1507 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9281 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2076 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1983 Acc: 0.9150\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9150\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2152 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1403 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.1819 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1347 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.1626 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1342 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.2334 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1481 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.2067 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1507 Acc: 0.9150\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9150\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.1553 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1413 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.1864 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1376 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.1843 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1410 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.1769 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1500 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.1923 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1476 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.2130 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1379 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.1990 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1266 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.2358 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1271 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.1519 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1336 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.1756 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1436 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.2680 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1295 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.1929 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1532 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.2741 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1470 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.1502 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1502 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1690 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1304 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.2041 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1325 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.1565 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1506 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.1725 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1588 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.1948 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1481 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.1866 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1489 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.1802 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1287 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.1635 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1373 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.2271 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1581 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.2066 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1469 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.1892 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1426 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.1593 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1438 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.1731 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1469 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1725 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1345 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.1762 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1464 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.1871 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1396 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.1630 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1451 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.1540 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1503 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1703 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1354 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.1839 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1398 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.2012 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1363 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.2149 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1518 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.2033 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1362 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.2270 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1196 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.1689 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1359 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.1564 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1346 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.2419 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1263 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.1449 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1457 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.1417 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1382 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.2107 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1494 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.1676 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1491 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.1723 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1280 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.2015 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1405 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.1682 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1349 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.2118 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1296 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.1960 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1580 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.1520 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1363 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.2081 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1647 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.1879 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1400 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.2264 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1324 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.1798 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1285 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.1538 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1328 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.1978 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1510 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.1709 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1414 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.1540 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1402 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.2440 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1514 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.1757 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1436 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.1842 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1332 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.2259 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1528 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.1844 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1493 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.1736 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1247 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.1465 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1368 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.1618 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1431 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.1850 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1592 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.1942 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1439 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.1580 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1567 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.2072 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1445 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.2068 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1333 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.1897 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1334 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.2034 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1307 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.2056 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1509 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.2344 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1414 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.2368 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1526 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.1511 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1573 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.1835 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1259 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.1850 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1320 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.1575 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1609 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.1825 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1264 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.1442 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1318 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1705 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1319 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.2659 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1404 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.1964 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1472 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.1732 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1303 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.1937 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1378 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.1594 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1374 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.1993 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1358 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.2054 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1357 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.2272 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1530 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.2067 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1522 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.2084 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1406 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.1856 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1639 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.2086 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1409 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.1573 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1493 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.2232 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1311 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.1681 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1390 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.1439 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1488 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.1916 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1580 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.1914 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1505 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.1439 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1402 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.1915 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1426 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.1876 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1412 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.1996 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1479 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.1908 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1290 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.1896 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1327 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.2041 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1597 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.1916 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1613 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.1623 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1529 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.1527 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1383 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.1780 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1393 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.1960 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1260 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.2118 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1435 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.1707 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1485 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.1898 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1441 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.1848 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1451 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.1519 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1307 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.1861 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1294 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.2030 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1402 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.2129 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1317 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.1416 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1346 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.2168 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1323 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.2242 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1454 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.1982 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1318 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.2493 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1302 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.1862 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1442 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.1858 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1447 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.1983 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1473 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.1586 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1421 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.2169 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1301 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.2193 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1459 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.2543 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1345 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.1553 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1466 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.1942 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1398 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.2399 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1405 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.2059 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1372 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.2005 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1592 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.1941 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1402 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.2207 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1425 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.2159 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1346 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.1740 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1502 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.1484 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1494 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.2029 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1234 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.1906 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1287 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.1298 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1409 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.2565 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1559 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.1457 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1452 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.2212 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1367 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.2036 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1383 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.1987 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1292 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1332 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.2250 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1245 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.2296 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1530 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.1905 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1395 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.1701 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1493 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.1377 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1464 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.1899 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1426 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.1759 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1282 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.2345 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1527 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.1673 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1452 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.2377 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1377 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.1567 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1570 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.2242 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1355 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.2039 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1394 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.1136 Acc: 0.9795\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9795\n",
      "val Loss: 0.1563 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.1716 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1621 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.2066 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1313 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.1648 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1398 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.1687 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1294 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.1518 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1407 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.2124 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1364 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.2081 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1388 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.1990 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1372 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.1948 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1539 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2013 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1525 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.2106 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1386 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.1826 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1335 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.2011 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1238 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.2062 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1304 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.1307 Acc: 0.9713\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9713\n",
      "val Loss: 0.1337 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.1704 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1453 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.2340 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1423 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.2617 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1435 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.1667 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1450 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.2270 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1393 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.1995 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1327 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.1778 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1448 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.2145 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1464 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.1755 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1427 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.2062 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1383 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.2040 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1576 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.1963 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1420 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.1704 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1526 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.1909 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1174 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.2100 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1584 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.1615 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1506 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.1643 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1375 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.1984 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1394 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.1676 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1408 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.1992 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1310 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.1824 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1457 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.2386 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1421 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.2201 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1417 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.1836 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1316 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.1786 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1372 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.1736 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1371 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.2553 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1251 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.1695 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1453 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.2042 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1606 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.1650 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1429 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.1724 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1639 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.1910 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1508 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.2333 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1393 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.1814 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1423 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.1924 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1382 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.1838 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1285 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.1800 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1450 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.1779 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1213 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.1567 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1326 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.2276 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1334 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.1488 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1396 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.1746 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1408 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.2166 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1396 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.1818 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1465 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.1590 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1367 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.1746 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1344 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.1930 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1298 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.2232 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1320 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.2494 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1506 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.2290 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1301 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.1665 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1362 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.2218 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1458 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.1441 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1404 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.2042 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1354 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.2071 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1507 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.1557 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1470 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.1954 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1392 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.1716 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1317 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.2451 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1431 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.1480 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1387 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.1651 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1383 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.1675 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1551 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.2141 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1515 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.1520 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1592 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.1914 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1434 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.1763 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1607 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.2160 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1398 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.1773 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1305 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.2273 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1325 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.1700 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1446 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.1983 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1375 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.1875 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1418 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.1830 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1301 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.2427 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1213 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.1355 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1515 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.2345 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1378 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.1897 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1568 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.2250 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1337 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.1553 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1474 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.2161 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1329 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.2692 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1350 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.1526 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1357 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.2759 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1465 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.2036 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1417 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.2264 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1397 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.1554 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1335 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.1958 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1403 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2190 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1463 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.1928 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1323 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.1743 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1308 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.2360 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1471 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.1909 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1553 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.2414 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1409 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.1585 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1300 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.2102 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1420 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.1685 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1318 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.1737 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1327 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.1882 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1420 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.1440 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1360 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.1831 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1540 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.1410 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1450 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.1936 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1403 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.1681 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1305 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.1821 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1433 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.1974 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1386 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.1966 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1336 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.2094 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1570 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.1941 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1561 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Training complete in 24m 22s\n",
      "Best val Acc: 0.967320\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet101(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model_conv,train_losses,val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n",
    "                       num_epochs=300)\n",
    "dump_output(model_conv,train_losses[0:50],val_losses[0:50],'with-pretrained-resnet101_lrscheduler_lastlayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.6275 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6230\n",
      "val Loss: 0.3241 Acc: 0.8497\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.8497\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3070 Acc: 0.8566\n",
      "train Rajat Best_Acc: 0.8497 Epoch_Acc: 0.8566\n",
      "val Loss: 0.2428 Acc: 0.9085\n",
      "val Rajat Best_Acc: 0.8497 Epoch_Acc: 0.9085\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.2820 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9085 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1434 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9085 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2793 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1515 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.3001 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1184 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2807 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1619 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.1820 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1162 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.1730 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1009 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.2064 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1148 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.1811 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1029 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.1925 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.0970 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.1832 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1002 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.1562 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1140 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.1776 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1086 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.1810 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.0997 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1748 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9385\n",
      "val Loss: 0.0923 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.1697 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1051 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.2153 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1016 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.2038 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1062 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.1868 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.0981 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.1823 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1010 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.1670 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.0990 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.2584 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8689\n",
      "val Loss: 0.0980 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.2079 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.0988 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.2100 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1011 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.1831 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1086 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.2007 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1074 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.1877 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.0979 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.2195 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.0977 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.2751 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8852\n",
      "val Loss: 0.0985 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.2062 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.0988 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.1420 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1011 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.1814 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1061 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.1875 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1125 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.1871 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1071 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.1483 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1007 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.1568 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1028 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1815 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9344\n",
      "val Loss: 0.0986 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.2473 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1051 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.2168 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.0951 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.1339 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1070 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.1649 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9344\n",
      "val Loss: 0.0949 Acc: 0.9804\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9804\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1548 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9385\n",
      "val Loss: 0.0982 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.2015 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1007 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.1858 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.0987 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.2114 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.0991 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.1638 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0975 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.2838 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1006 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.1582 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1029 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.1783 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.0962 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.1730 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1030 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.2492 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1052 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.2232 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9016\n",
      "val Loss: 0.0996 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.1820 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1009 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.2071 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1050 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.1479 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1002 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.1639 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1000 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.1978 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1050 Acc: 0.9804\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9804\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.2224 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.0973 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.1890 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1050 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.2222 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.0981 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.1455 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9549\n",
      "val Loss: 0.0999 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.1482 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1017 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.2051 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1034 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.2002 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1008 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.1864 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1042 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.1898 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1032 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.1667 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.0966 Acc: 0.9804\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9804\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.1880 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.0987 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.1717 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.0959 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.1751 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.0972 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.2312 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1017 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.2014 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1021 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.1599 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1006 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.2636 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1097 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.1645 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1021 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.1921 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1049 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.1652 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.0955 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.1761 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.0967 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.2159 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.0967 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.1690 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1040 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.1793 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1056 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.1932 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.0962 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.1285 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1038 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.1549 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1048 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.1410 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9508\n",
      "val Loss: 0.0995 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.1644 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1038 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.1468 Acc: 0.9631\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9631\n",
      "val Loss: 0.1038 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.2245 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1029 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.2164 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1019 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.2355 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1112 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.2075 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1047 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.1325 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1052 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1575 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1060 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.1724 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9426\n",
      "val Loss: 0.0982 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.2140 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9016\n",
      "val Loss: 0.0975 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.2081 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1000 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.2042 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1210 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.1614 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1032 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.1728 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1035 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.1624 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.0968 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.1789 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.0956 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.1394 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1096 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.1622 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1028 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.1462 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9590\n",
      "val Loss: 0.0996 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.1692 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1065 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.1933 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.0967 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.2165 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1040 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.2537 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1077 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.2121 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.0986 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.1946 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.0992 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.1763 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1043 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.1966 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1013 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.1901 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.0964 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.1584 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9426\n",
      "val Loss: 0.0982 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.1303 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1005 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.1775 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.0965 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.2486 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1031 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.2135 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1013 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.1648 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1059 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.1654 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1036 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.1831 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1048 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.2251 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1054 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.2386 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8934\n",
      "val Loss: 0.0956 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.2952 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8852\n",
      "val Loss: 0.0993 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.1772 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1050 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.1733 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1002 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.1671 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1016 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.2029 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1052 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.1654 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1018 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.2025 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.0992 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.1714 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1034 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.2087 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1053 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.2142 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1077 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.2240 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1048 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.1470 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1120 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.2140 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.0955 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.1824 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.0995 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.2152 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1019 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.1439 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1022 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.2608 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1031 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.1894 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1000 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.1417 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9385\n",
      "val Loss: 0.0990 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.1764 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1130 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.2179 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1046 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.2032 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1088 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.1906 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.0949 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.1840 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.0978 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.1959 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.0995 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.1926 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1026 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.1695 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.0982 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.1881 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1068 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.2436 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.0964 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.1472 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9426\n",
      "val Loss: 0.0978 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.2317 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8852\n",
      "val Loss: 0.0985 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.1798 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1032 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.2459 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8934\n",
      "val Loss: 0.0957 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.1923 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1040 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.1890 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9385\n",
      "val Loss: 0.0998 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.1780 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9426\n",
      "val Loss: 0.0983 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.2313 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8893\n",
      "val Loss: 0.0927 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.2268 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.0953 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.2309 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1016 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.1569 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1032 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.1826 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.0946 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.1494 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1052 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.2271 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1008 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.2432 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9016\n",
      "val Loss: 0.0965 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.2486 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1060 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.2675 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8689\n",
      "val Loss: 0.0968 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.1620 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9426\n",
      "val Loss: 0.0987 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.2501 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1022 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.1731 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1091 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.1842 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1096 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.1990 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1039 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.1516 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1021 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.1712 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1048 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.2364 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1005 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.1806 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1055 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.2000 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1079 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.2062 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1012 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.1809 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1073 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.2101 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1032 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.1590 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.0943 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.1708 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9426\n",
      "val Loss: 0.0942 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.1747 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1033 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1732 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1049 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.2257 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8770\n",
      "val Loss: 0.0953 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.1806 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1022 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.1609 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1091 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.1960 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1007 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.1619 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1026 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.1878 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1146 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.1650 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9426\n",
      "val Loss: 0.0968 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.1479 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9467\n",
      "val Loss: 0.0978 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.1926 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1047 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.1669 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1026 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.2209 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1027 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.2729 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1002 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.2156 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1023 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.2245 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.0991 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.1376 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1027 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.1736 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.0948 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.1985 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.0984 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.1885 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1020 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.1756 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.0992 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.2615 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1002 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.1855 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1014 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.2144 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8975\n",
      "val Loss: 0.0957 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.2002 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1014 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.1532 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9467\n",
      "val Loss: 0.0934 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.1574 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1069 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.1914 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1033 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.1657 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1025 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.1662 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1031 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.1345 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9467\n",
      "val Loss: 0.0979 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.1545 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1033 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.1766 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9467\n",
      "val Loss: 0.0974 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.2350 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1051 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.1852 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.0989 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.1626 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.0965 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.1586 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.0914 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.2209 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9016\n",
      "val Loss: 0.0976 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.1638 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1008 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.2143 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1026 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.1856 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9385\n",
      "val Loss: 0.0952 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.1596 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1061 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.2268 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1043 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.1443 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1018 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.2011 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.0981 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.1947 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1049 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.1876 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1001 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.2176 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0944 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.2248 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.0977 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.2239 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9016\n",
      "val Loss: 0.0958 Acc: 0.9804\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9804\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.1808 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1175 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.1910 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1052 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.2249 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1015 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.1894 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1059 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.1889 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1056 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.1321 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9508\n",
      "val Loss: 0.0958 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.1818 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1054 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.2176 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1011 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.2032 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.0967 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.1646 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1036 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.1668 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1004 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.1732 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1061 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.1866 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1038 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.1810 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1033 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.1788 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.0987 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.1304 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1019 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.2182 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1028 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.2259 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.0985 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.1534 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1022 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.1970 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1029 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.2007 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1032 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.1494 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9508\n",
      "val Loss: 0.0995 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.2253 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1060 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.1871 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1054 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.1970 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1052 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.1842 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1036 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.1869 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1058 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.1455 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9508\n",
      "val Loss: 0.0992 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.1976 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1053 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.1907 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1007 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.2469 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1154 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.2669 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1011 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.2083 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.0985 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.1523 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1032 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.2232 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.0978 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.1899 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1013 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.1263 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1050 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.2006 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1064 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.2097 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.0990 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.1572 Acc: 0.9631\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9631\n",
      "val Loss: 0.1095 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.1690 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.0980 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.1405 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1060 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.1772 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1029 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.1975 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1046 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1771 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1059 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.1560 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1006 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.1871 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1188 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.2056 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1168 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.2345 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8934\n",
      "val Loss: 0.0945 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.2163 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1047 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.2232 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1007 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.1572 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1065 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.2224 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.0910 Acc: 0.9804\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9804\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.1937 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1019 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.1880 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9508\n",
      "val Loss: 0.0998 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.2039 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1027 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.1999 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1078 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.1908 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1106 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.2187 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1103 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.2010 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9098\n",
      "val Loss: 0.0996 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.1797 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9303\n",
      "val Loss: 0.0984 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.1718 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1070 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.2763 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1042 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.1651 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1023 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.1956 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1023 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9804 Epoch_Acc: 0.9608\n",
      "\n",
      "Training complete in 25m 16s\n",
      "Best val Acc: 0.980392\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet152(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model_conv,train_losses,val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n",
    "                       num_epochs=300)\n",
    "dump_output(model_conv,train_losses[0:50],val_losses[0:50],'without-pretrained-resnet152_lrscheduler_lastlayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
