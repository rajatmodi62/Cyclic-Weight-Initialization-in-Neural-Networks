{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "---------\n",
    "\n",
    "The problem we're going to solve today is to train a model to classify\n",
    "**ants** and **bees**. We have about 120 training images each for ants and bees.\n",
    "There are 75 validation images for each class. Usually, this is a very\n",
    "small dataset to generalize upon, if trained from scratch. Since we\n",
    "are using transfer learning, we should be able to generalize reasonably\n",
    "well.\n",
    "\n",
    "This dataset is a very small subset of imagenet.\n",
    "\n",
    "\n",
    "Its a cat bees dataset, constructing a transformation pipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=8,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a few images\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n",
    "Let's visualize a few training images so as to understand the data\n",
    "augmentations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAABZCAYAAAA0Gj+BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOy9ebRtV1Xn/5lr7eb059y+f/fd917ykvdeGtJHQugCAtIIomAJFraoOEChFKUsi1L8WVVaWtj9BNEStRQVFOkEQhMMaUlI+/r+3b45957+nN2tVX/sE31EIAkJBhn3O8YZY58z997ru+aea6655lprH7HWso1tbGMb2/j2g3q6CWxjG9vYxja+Odh28NvYxja28W2KbQe/jW1sYxvfpth28NvYxja28W2KbQe/jW1sYxvfpth28NvYxja28W2KbwkHLyJWRNoi8mtfQ35GRG76t+b17QgR+VMRedfTzePbASLyBhH54tPN4/FARN4pIn/xdPN4PNi20a8PEblQRFoikojIj369c78lHHwfl1lr/zOAiOwUkTNPF5EnUr6IPEdEbnmKyn3cHZmI3CIiz3kC9935JKg9KTyR8kXkKdmY8UScRN/5vfMJ3PcNT4bbk8ETKf+J2Mg3A9s2+pj3+YZs1Fp7zFpbAG59rOu+lRz8NraxjW1s4ynEvycHf7WIHBKRLRH5PyKSeUQgIi8VkftFpCYit4vIpefJJkXkQyKyLiKnReTN58muEZF7RKQhIqsi8ltPlqSIvFtE5vv3vFdEnnWe7J0i8jci8mci0hSRgyJyVV/258AO4KP94dfPi0hGRP5CRKr9un1JRMaeLEdgWERu7nP4gojMnsfxor5sU0SOisj3nSfzReQ3ReRcX19/KCLZvmxYRD7W57kpIreKyJOyLxH5IRE53Od5SkTeeJ7sOSKyICJvE5E1EVkWkR/qy34c+AHg5/u6/Gj/97eLyGL/fkdF5PlPht+/UJHfFZG6iBw5/54iUhaRP+5zWxSRd4mIPk/+w/36bYnIpx55DpLit/v1qovIgyJy4CngmhGRv+7X/8sictl5XL5eO1Ei8gsicrJvi38jIoN92baNfivbqLX2af8AFtjzdeRngIeBGWAQuA14V192BbAGXAto4D/2z/dJO7B7gV8GPGAXcAr4zv61dwCv7x8XgOuegrq8DhgCHOBtwAqQ6cveCfSAl/S5/jpw56PqedN5398IfBTI9c+/Eig9SX5/CjSBG/s6ejfwxb4sD8wDP9TnfwWwAezvy/838JH+Myj2uf16X/brwB8Cbv/zLECeJNfvAnYDAjwb6ABX9GXPAWLgV/rlvaQvHzivnu867157+3Wb7H/fCex+kvze0Ofws30OrwHqwGBf/mHgPX29jgJ3A2/sy74bOAFc3Nf1LwG392Xf2bfbSr/uFwMTT5LrO4EIeHWf638CTvePH6ud/AxwJzDdt5n3AH+1baNPr40CtwA/+nX5PZnKPVUfHp+D/4nzvr8EONk//v+BX33U+Uf7yr4WOPco2S8C/6d//E/AfwOGv4l12yKdX3ikkX3mPNk+oPuoep7v4H8YuB249Cnk86fAB877XgAS0s7zNcCtjzr/PcB/7Rtw+3yDA64HTvePfwX4h6/3HJ8C7h8G3tI/fg7QBZzz5Gv0O+mv0nj29OU3Ae5TxOcNwNL5ToLUib8eGAMCIHue7PuBz/eP/xH4kfNkqt/4Z4HnAceA6wD1FHF9J18ZTChgmdTJPVY7OQw8/zzZBGln4Wzb6NNnozwOB//vKUUzf97xWWCyfzwLvK0/7KqJSI3UECb7sslHyd5B2vgAfgS4EDjSH1q+9MmS7A/HDveH1jWgDAyfd8rKeccd0mGz8zVu9+fAp4APiMiSiPxPEXGfLEfO06W1tgVs8i/6uvZR+voBYBwYIY3S7j1P9sn+7wC/QRqRfro/VP2FJ0tSRF4sInf2h9M10o79fF1WrbXxed87pM7gX8Fae4I0En0nsCYiHxCRya927hPEou23tj4esc1Z0qht+Tx9vYc0kqcvf/d5sk1SBzVlrf0c8HvA7wOrIvJeESk9BVzPf+4GWODxtZNZ4O/Pkx0mdbhjbNvot7SN/nty8DPnHe8gjZwgNYRfs9ZWzvvkrLV/1ZedfpSsaK19CYC19ri19vtJG93/AD4oIvlvlKCk+fa3A99HOgyrkA7Z5XHe4itm5621kbX2v1lr9wHfAbwU+MFvlN95+GddikiBdDi7RKqvLzxKXwVr7U+SDoO7pEPhR2Rlm87mY61tWmvfZq3dBbwMeOuTyR+KiA98CPhNYKyvy0/wDeqyz/EvrbU3kDoJS/rMnyymROR8To/Y5jxpBD98nr5K1tr9/fPmSdM15+s6a629vc/1d6y1VwL7SYOQn3sKuJ7/3BVpyuURrl+znfTlL36UPGOtXdy20W9tG/335ODfJCLT/cmddwB/3f/9j4CfEJFrJUVeRL5LRIqkw+VGf+IiKyJaRA6IyNUAIvI6ERnpRzO1/v2SRxcs6XKmP30cHIukObd1wBGRXwaeSOS1Spr/fKTc54rIJZJOzDVIh8Vfjd9z5Ikt3XqJiNwgIh7wq8Bd1tp54GPAhSLyehFx+5+rReTivo7+CPhtERntlzslIt/ZP36piOzpO7tGn+dX4/oGeXxLUD3S/Os6EIvIi4EXPoE6PlqXe0Xkef1G2SN1BP+KX/9cK49/eeEo8Oa+rr6XNF/+CWvtMvBp4H+JSEnSicrdIvLs/nV/CPyiiOzvl1nuX09f59f2I+F2n+9X0+XOPtedj5PrlSLyqv6I8WdIO6A7eYx20uf6a/Ivk8AjIvKK/vG2jT4NNvp48e/Jwf8laYM51f+8C8Baew/wY6RD2i3SIdgb+rKEtKe+nHRCaQN4H2naBOBFwEERaZFO5LzWWtv7KmXPkE7sPhY+RZpbPUY6VO/xlamlx8KvA7/UH17+J9Jh5wdJjfEw8AXgq21WmSGdMH68+EvSnOUm6aTYD0Aa4ZAa6GtJo6UV0gjC71/3dlL93ikiDeAzpBNDABf0v7f6XP7AWnvL1+D6mLrsc3kz8Dekz/U/kE6ePV78MbCvr8sP9+vw30ltYIXUMb/j0ReJyHS/Dg89znLuIq37BvBrwKuttdW+7AdJncChfh0+SJq/xlr796S6/UBflw8DL+5fVyJ1VFukdlQljRIfjZm+fPFxcv0H0hz2Fuk8wav6EfhjtZN3k+r+0yLSJO0Uru3Ltm3039hGnwjkK9OHTw9EpEcaTfyOtfa/PN18zkc/gniAdBIperr5fDWIyPuAv7XWfurp5vJYEJFPk05CHX66uXw1iMjrSIf4v/h0c3ksiMgvAevW2vc83VweC9s2+tRBRC4AvkQaPPyUtfZPv+a53woOfhvb2MY2tvHU45uSohGRF0m6SP+EPAUz1dvYxja2sY0njqc8gu9PthwDXkC6DOtLwPdbaw89pQVtYxvb2MY2vi6+GRH8NcAJa+0pa20IfAB4xTehnG1sYxvb2MbXwTfDwU/xlStHFvq/bWMb29jGNv4N8bV2UD4ZfLVF/v8qDyTpy3Z+HMBx9ZWDgwWUAqXAxJDJuGhHkwQgBrTrYBHEWGIx2CRBGR8jAV4mQ63VJk7AGIPvKRwiRClMYtGOIooMhXyGJLHUGj2MMYAlMZaBfJFW0KGc14gxoCxRAsvVDoVcjjCMsVi8Qg/Tr4mj0opaC9YABqwVHFGIEkAQ0rpEcXqesaBdg3gJjsoS9mJ0JoIEbKzAMSCglKS8cXHFAa1I3DYmEjxyBKZLGBsKRQgDTdY3aKVotxKCGLr1f3kIinQhrQJ8ERJrcVHk8LAInutQyJcI4xBHOyRJjEXoBR2UUSRiEAOiBK0dXM/HGouVdBu047lYC0kck8lmsNaSGEMUhWhH42cy1NsNVnqbWGMxEYgLY4PDdJpNlBLKpRKdTpNukACWjJdyi+IYRymiJEGLSvViLEorHEfjaMH1s4i2aGNxXQ9RKt3GbQ1YiKOIMAjxsh5aa+IootUJyPk+SiCOEzqdHs1egOO5FHM+IppeEGAsZHI5Ol6TfNYlbHkkkWGuMEevGyJWwCpEKzxPiJMEEo2bE6wRVMYQbxm0q7ElS3ctBARlwIjFiCGJY6wWSAyJrzFJRDNqUHIKIMLIyABxLSawSfpQlRDEhqiyhLM+SGIN1gqx16TodcFaRAlJ7KJ0jGBTW1AgSiEiWMDGlrAXoh3QyiKOi6BIoh4WodayeK5Ls91lesc0SmlMWMMYi7UJiXiAQjAoDElkWdiyWCv/3NyNMYiASPrcAPYMamwUgzWIVmnjtgYRi5gEEyfYGNxKiUQcMAmCYJIkbUQYQKMcQQSwCWJTTtZxibZa/1xfqwANSkA5WazSoBUmDBENJBYDIKmORNJj7foc32ql7UdAK0FUWiv7yOtdzvdqkl7reuBoCOP043qgtCDKPvLosI+UY2EmM8uZ9lmUBmPSzyP+T7oQqfR+xkIcpDWXflnahcRAuMGGtfaRnbr/Ct8MB7/AV+46fWS33FfAWvte4L0Ao2OD9sWvvoko3ETHG4xXBlCMMzlr0RtFki2fHTfuo5jPcfbO2+hZy/juA5AoCqMZPnHrwywsb5HN5ojDDmNjYwzZM4xWYGR8hqjd4+BaFkyNC3aM0N1a5O7P3s2dp1tM75jDC7aYuvAKLp7JM71rjDP33Mlf3tVCVJWbXvlsvnzbMd791x/nV/77HN3QYEWTswnaT523qzRh11LOeDhBERxQ9YRBv0g28lgKApot6MSarW6DC76jyfyJEtmMYmhihchCsJSHoRAvC74HcQgTzgRTzgixGI4tbPDyN4xw92freCMuJ1aPMXlpwnX572azKnz+vo/QtBAvTHPvF1boBCFg+50NZIEcCkcS9psMk0xTVC5zOy4gP1BkemKW6kYVHIdDZx7G7RnCJMCSpdSLMW6e4niFkbFp2mGNTqfD5VddQafRZr26hqcU1z7v+YRxyMmTRzlx/DizYxOEFn7r9g+w2N5k5hKP+YMR4sCPvP57OHvoXky3RTbr4ri7WN4KGMxpds4Ms1lr4/t5Nja7KA0532fXZIVqtcZWs03XGCrFHM997cs48fkvkM/m8fIOU5OjmERoNqqcObJOux1SSxImh4sUCxXOnllgvb5JQQueEQ4vn6PjjlByhFq9zf6L5pia3sPhY0eZX1zkO77zRWxde5C58UGaPdi4ZZLnjv0w1ydzbG30yBvBKs34RT6bD8c4bpPS8AzupYpkpgPvh44f4r3VofU7IatHQpxMTE0abK5u0FExg3O76WhouVV6nRBna5NQB9hcyIv3v5Dxyz06IXzyg/fRFKDdoveTf0Xuf7yGZq5H3O2xNX0bL9pzEMdtU1/zMdowO+nS84skkUfSWkCsRg2NY9QwYWeF9bMJrmngqZCRQQfKA5z6zIN87lSLofFRpvbMcseXH+Stb3k9pfEDNKoPUa9B2DvK4OSrSMwG8w/8PTun8gTtJv/308KnjnokuCSmizIQBj2U1qAcspkcf/78CJV0sVpIghp+RqMzPXRQI1hfo3tSKLziRaDzhK0tVGgxnRXEKjr1KplcgpsfIFMp4Yils1FFB/OETXCuvJraJ28jEQhDi+sL3iA4JQ+yFZzBYVR5iO7WEsQKdAFlAoztIkS4XkJpzxVkBnfyjJ/7XbI5TWlIkS8IyrH0EkiShASBxGKtwQKupyiWYd9+zdyQ5mwnQsoee0cqFDJ5PHH4/U8eQWFxRYht6qT/+Ln/Hz97zw+gHaFW0yRJjKtgfG2aW4+vkMmA44IkoALI5jUUEvxhy/Cs4PjwkdfHZ7+eM/5mOPgvAReIyBzpBozXkm4A+JoQEXbt2IFili/e9jk67XUqkifsgRO2ydsK7ZOn2TkzSbsbk80pdEHT7obc/cA67U5ArlLAhAEnz2yBzrNj717mVx9ks1uljeaTN99KJVekW1thYLDMA5tQLPkkwTJe3ueNP/QqNlaXUBmXP7p5iYW1Fs965jXcfdc51pZW+YnXvJKJSxxcE5OohMgoOjVIYshlE8TV4IBNoNtIsF2N7ob4RWGkkEUnMYG3wVCxzvwqOMMb+J0ihFkypQBTDvApkXMVzaROYhK6UUDsWTI2ywXP8cjPOAxfZLn+oqs5/vHD6BysNh4k7AwQ1acYmptn7oZFbv9MAg5ocYiiNCruAh6GYQTQhMQYNOfmz/GCC7+LrYUV/IEiXiVPdLRHTwwSRpQjH5vPonKaXN6jEzUpDQzQa7R56KF7ECcDcUy+kGdh/hQb62vEvZDL9h2g1qmTtcL44CisL7D4cDpCMb009LnuwCwPHTrOZfv24Tiah4+cpF6PUVJkemqcYqXEZYUi9x9+iNXVBudWW3RaLSaGK3hOhk7cIYkDshkXdMziUov5hRXWl7bwdQ6UT9YTvG6Htfk2x9unGSmX6Na7bEU17ji5yYtuuJj9e6c4cmiRgpch6HZYXDxB1rVcfSDdZPhLjb/k/qF72L08ROnqfTz44D+wFe4iO5Rl7PIcRz66SeFki5Z1oB7RWV9jcnIEtwa1RoPS3hEanwsILvGJj6yQdDOMZEtsepuUJiZ55msm+dvfv5/MuMXTHm7Wwxih1tvkjvuP8/KXHCDjeYh1cCQmCTX2l19A260j5TIlr8AW4EqH2Ai9MGKoUsUMXkNB+7RWTuINTWBxCbs10AFSn6cysZN2NYuKVzh8OGB1dYGxYYcbLh/k5ME6XvEMN06NUDt9J45YTG+BOJnCUw5h0iGsHyVTKIBTRmSDK/a4vOxFF/KZW07yuWN5mr0IEU2SRGhH6HS64A6Aa4AQ7Tho20Mbi93coH4Opm6ao9uJCMI1RHl0m1ska6s4KsQkQtju0F5ep1UH3wGvCJGBK374bZy97xN0FXgWvGwaEber4CUhGVnBxhl0pMGvQHwSkhYiBZTt4ORh+MBN5Mb3ITqD6WpUEbSAKIitIImFREiMRRmwokCD1obREc3EgGaikmF2rEic9/EyA4CHEo+3v+qZ/Nf33oHrJFgFvUj42If/gMVA85uveA9zmRF+6o4fY7SUpeF7XL9rL/ms0Is7uCpHIe+SxBusba7RiEKSpubxrI95yh28tTYWkZ8m3dWpgT+x1h78eteICIPFCr7WTIzOUK9X2X/1JZx56ByH5g+x31cs19doL9bY5Q6RE037bJvbayucOHgc3QvZd8k1nGyv4jshxx68l/XTAdN5CNUyS+tdwk6Hucv2cGa5zplz6/Q6ETvKGS65bj+XXjDNqVOHyfp53vuu97O52iKTcRiaGODokQXyZYf1lZNMXwWOEnA1OoFaGwadItVug6KfkCQeRiCuC50mOBlFzodsLiKbEXboIk2VwxRWiWKoO3Wy8SB5CTF5w4gax8vHqC6sd6pUOw0GpEgjajBcGWWrGlG5sMORc4eYu1BhQ0Ojk6He6qAybYb8QebvMwhVpgpllhtNlAIPTWwMkQJBgUmI6bJmWgyEZU4cepjpuTmUk+WeB25D+zlss86IGSQogS8+UnGphnV2FIc4c/QwAyODtHp1TLPD+M6dxNUGn/3Mp5ib2cPExBirC6fR+XHm105w+5kjQIKJ4IqbNOfq6ajiM7ceppLNcv+hU1grDA3PccneUcoFh/rWGm5UYzBf4JoDB/jQ6U8yX62ScT1Wow2yOY2xBTZWz9FoddFOnnq9gdsNuHx0iPJAieXqJgsbAY0gIZdxyPvw4Tu/RESGa55xMT9+3TPZt/tCYhOxsRETtTsMjw/gqCytMKBT20zts1rhxMVfYObc2zkbNEjCS9ksnGFkaxwezDBZrqC8DkQBvSDL+C8V6by7R9xeJhzIks9EyO4Q5/YCKlRkshrRltHSBO1ajY//9m2gIxw7SNOskyhBiUK6Bn/QxZQd3GVBjIPkPTztYHMJobHQs8TVLbgI3FyZoFmlsRnSWMwwNzqAzzJWD1KojKJNQjVpYDt1dGU3zZUqw1NDHHlwnIUzJ3jWi6+iVt3igc+fY7hcoHLF9ZRGr6d57iM4yw+gTEDZ7VKa3Mty7QFa1dM0GyU8WvQ2FddcM0RdBnjx88e4uHSEX7m5gFWWMLG4JsEkCSbewHTaZCsFTGIR6UIvodsymK6lq0cImx1MXCXpVuk2LSbYhATqCxaaQhJbBCEsg9MFFCwu34aTF4Z2eTRPhUgkiLYoJWCgG0DZRoTdDbQ/jDEGKx6GTdwMTFz7GrLjlyCSg0RQ2qJdwfUE11FpGlJFmCRNxSbWIhg8UQwPaEbHFRNFn3I+T9HPEWQc2uKgVergtTi86ydewVv/8CO4rYSWgXtqR9jMCdW/+Ti9C3fzlpHn0Sxbjs5ETOYHCEyblW6dvDjYZI3lWkyrGxPH8L1N+OP4sd9i8M2I4LHWfoL0pTuPC0rB6sYSWaeIjrMox2NgcJCmt0m9G5C4Gt2IObe5xsiOCvlwALdZptE5DAE870Uv5L4Hj7C43OaqSc2HjpxmrDJLeWqc0akJpnsR9WqDQ0dPs+/iA3iqR6fVo1Wr0d2ocW+rzaV7J3hg4RgLoYGMJue7tJY32HPJTh5+4BCe0gRBzIDrk8tBEEKoExLdpLeZ5spylRgtCZGnsdqhq4XEeHSjBM9LcMXHbwidzTG8WcuqWaMhm1AF3xmkkh/ET4TAWHTUIo4VS9UqamCcS6cmCThH2R8n1uM0Djnkxoc4GWyQycDM5S0Shnjw7gVyxhI0WgwW8mw0mvRIKCmhhGbSqbAR1qigqTsJM7kCsQhKZ/nS0TvodLroIGaSIVoE+L5PLarjtnzGy4Ocmz9MqxvRXVmkUC6RdTVBs87kBXNs3Fdjcf4UmaxHxyacPfglPBOj/uUvLvjy5xLyM+kPviMgPbQMsH/nHBfsnSaT8zl94hQjY0XiBLpbTbI5l32Tg7Q7CVvtNhdd8gxqW1We8ZJnc/OfvJ8r5nbzT3d8kaOrdXIZTXtyjFYS0+sFHFzaYrlmGBvMccOBGX78ta9A+R67L9iJoyI6Aewcm2BsqEAUh6B9ol6L9maVE2GbLlA4kOc/HPtJPrf6MS676DIG5gcoJqCdAqdXNrGRMJTJ4ydC1hXUQwFBJ0btq+AuwdqZBt5mjtqJBQb2FGks9/ArI2TX2ixGp+kETbpJh0I8Qk4GscWQdr1Gz4Y8uPZxnq/3Ez3UJSFERw7dMCBfHiVYWSabG6S5lnZEreoR3vuBhFOnu+R0wn3vew9vevYQN71igGRkmMQpI1t3ky1WaPU2qWQjzh5eJRPWePWb3kq2MECvU2fuwvv45Ac+QzaJcMwxdlz7HFTk0179R3Qe4uIcJdcSNs5x/N5DjF/j42Z8NjcMhUlDlB1lx8UnGX/AsFrT5F2fJIlIAK27OEWPuLeGWxRUJMSdRdobMHBDkbAnJHToVWv0qlVEQa8BrXWgLWgHnJwQeeAUgSJc82M/wNqRIxAkBO2wP1cBMYJ2QSuwAr3eIr4epLd5GF/AK1rELbLjuW/CK8yCclHKxaBw85pyySFf0ng5kCAmSTRaYqzR6dyHUjieYXRaM1vxGcxlKWZyZPwCvta4OHS1j+ATW9AYfu9Nr+H06lH+55/dx/fT4lxphHa3ihytUiOitONqdnkNHJ2hF/UoSsD85jHOnV7i0IM9eokik0vf0xItPLZv/aY4+G8ESdRlcbXOcHGYpc2zLDQbTFw0y9Txg1TDJsNuBXtgjFylRNSEh/XD3HnLXczO7ubEydPMnztDYh3uPl3l+ssvYmbnIHGkCFqGoNngzIPHKeU8lk8eYfKCC8EKl+yboDxSoZtELG62aGw0WVjcJIgTcn6Bhc1NJisj7JibYGthAwnrSDbBtR4oTS6TsBa3cB2XIDGEGhzVwc3nCOsOSrlEYsm6GgKDk7W4ro+vu9ROJFQuyeFIC9mqUPBK5AYVOcelaj1wBPyYIMjSknU2e6cY8g2LtYeY0EOE3d1kVxSDviKRBoPDV/GZL9/DjTdewd9++su0SEgaTUx/JigGQg33xTV24XJKrTEeF2m2Okxfsot7TtxHu9nFc4WhVg6bU+SGCkztmuW+B2+HsEsNTbW9QaE0hB+CjoRQWTr1Guu338Gey5/B2ZNHqW1tsbXRZGi4zNbaGlPFMuudJmDTevXhxZrS6Ajf8/KXUi4Kzc0WW9VVslmPuBui3Tz1Zp3amSqONTjacNW1N1IpC17GYkxAYkIai/fy0osHuGSuQKwKrIfCBVPTjA2UuImI+YUtwjCinC8TxAlXX34B9foKZEvMTA+jSZjcMUDcC+l2I5TvI+VRMrT40hZs3VFn6MU7OPX5h/n7T3yI97/u43RWaoRHQgrlYQIaDEwXWDnVYPS5edrnahR3VTDfF5H8L0tx0hDUi+QKdfR1ObxPgm8MTWp0wx7r9WVKM1N0kwgXMH6OuL2A6YVIJWb9zzYo+yVsHBFGCY51cKRMQS/RatZQ5fStvj/z82u84KYxrp0dwWZHObC0wOHjazzfm8O0V3FVC3/sWXQW7sQooVlX3PuF0zz3heME9QWyGUFyw/idAZJslvGxAm5eaJ+6HwYvhPLLqB/6EwaSLIkzSNSqceNNBbQq4+TKhK1FVh/6JAN7n0eUHeSmC0P+9l7oxj2SGHzPxzEO1rYRR6ETi41DggYEXXBUgSRqkXTWieImkQumBu1FhfQMroZEAQKuD4lOUwRLx5YwzRo6XCI4A5JPz3Nt+r7mKAIVgxdDGLS4+IUvw82N4OZniPQEWvf/q8S6mASUm8X3wM+A54PvC0kiOJo0/y4GsRYlhsqAYrCsGcw75D0fz/VxtIdSLh4KrQr08NNcrg0xiWViaIrfePMIt599mIl3dLktdzvXzhaYv0UzODxNedahY3osVk+xvHqMU4drnD4uZLOKsJ4Q1xy0Tp6eFM03AiEBHeMXDLQCLhjcx2XTc2SzPnsOXsSJtXmyUYa9+3YyVCkRtAzV+7pcd/1VtBpt6huL9FoNNpaXmJ0Y4upL9tKylrXFFiuqxbkHz1Ap5YkwnF2tEatznDy7xM6ZfRRzBcbKBXJDI2xtBWTyeTpbder1Ovsv20uuWGb+lvsxRjMGKJ0Q2wSrBFSCjSBquiRRQqecEGkDkVoAACAASURBVDccnFIdpzSIkwCOoHSMUpY4EbomQCkHrR2S0xGVfQHZsYDeimFts0k+77EV1Yitwc04ODEMlZqsHp3H7B4iqO5BZ7Ps2TGJp9MVLofXNynbmB1j09x9171sAVkj9NI5exQQAlGU0AMeJqGAYlppesbw8dv+jvGxHSiTUFz30IUiO/fPkmjF4uJZPPFpxRGmtYnrOuQzLirvQWRIrGGj0aSYy3Ls7oewBdhaO8vkjhlq1SZemCGXSXhkXY8TKrpLBoAbb3wmI+MVCnlhcHwHXrGLUQavYzGqw4mDh1HGkliYuvhKpiaGaXVWaK2sE9aFavU0npuhjsOs7XFBVmhsnWHX7B5GLpsDsfTqXTzHxXUcxCuxtLzEg/fdTcn3KI10kdFBvEwJlYSQBAwMVSAU6ptNbDYPW1CY9vAyhp959W+wwRqR6lEo51lpBPjLNexAhmC+h9QjNm+LGP6hIdQDkBxWqDGL+f4Evxxjf9YhuCummMuztbxKkjWsLxxFF4p0WxFeSePZAAeXDl323GD56BeOc+L4Om5rmUQJSaOD6SWEvS1UIGRdh26S6vaNP7KPaigMjM7QOLdI1oZ8ZD7ibQWFKk6DWyZTXyUYHmHjyAoDwz7Pum4O7eZJTELsjBE2HyRuHuSma6ep7LgMG0bo4jCb5x6kZ5dZPtymvX43O67cz9hghdXNGl5lhAHPoHJFbNIj6IzTWP0nrnvmc7j6suN8/h6fv7u7ShInJL1NbK6EZyAIOxSpsb4KJrSEzRVC6RK263S2QHegvQJR16A1RD2LzguRBrqKXtegAzh2y+cpjwInLY4nmAhcBdYFYy2gcZyEIISZ77iKQBXRmSlihtH4YDVxv6W4bgasi3I0SgtKpxkG7Wi07qdEVJr6yWQVlQFhIKfJeArtOv+8qEEAVwt0trC6iOvmiSTtIGLTJbYbXDyRZdd7s3z0vTUWP9XiA8+E2Xf8HTvePsmoU+SLBxs0GnU2a7BzaDfesXlaOeEfZjQvX0r+OXj7eviWcPBYi1YxvmtwKoqrdl3BqCmztr7JlB3nXH2V0Z0TxMdbVCs9bl44iYQB6+tNyqUct3/xLjJ5l4KbZXqsjC6UWDu1zOXXXcbqwgrdIZ+LLp7io7fcT87RDE/OMLNjiqS7Qewpls8scPa+g9xz+yFa1SbdICKX8bnz9i+xf+8cF+/Zy/0PP4TnglgXUZAEAUo75AYVFZXlzClDsNlDjVnCHqjyJnEYYvQAnUiRBDGm45EkUX9JlBA1siSrCpODgqc4eXoVsiFJro0uJygnR9MkuIeGWY7XGMxVyHElqlAll3Uo+QVaYYuMGgLTpVutYa1m0vc4F/ToL+TEAB5CU0FsLA4wYoRl6iTAiCrQaG8xHGTxchn2XHMh2XyZJIlh/SwtYorGITEOu3bupeB71FsNWkmL+lYb10aEoQMKnA3FjpkpAhUT2C4DY4O02+s4OU3cSYhJULnU7K55yQsoFzLkvQxR2CQXwtTsHIWBYdYXjxFU17jv6CG87AjnFk5x952fodmxxFZ4+Q+/muOfuJlKxefSy6/j9InT5BvzXLJziqS3Tu+uj9PasZ/s8DRFSaivNQiMZW5smszIKIlpkXNjXAeSuAP0aHcjuq3TaC1EYcBQJQ9LXUxksMdzmGoPteLi7k0IbRpOZm8cRo730K2QjPtZbDxM63dvwHXA+3UX71mC7Sk6X26jPQ9tfLq9Uygnwx2nPkF2NEeoKvjjJdrhJgPuKIGraQcbfPTEzcwVL8KIZbPVwZsYImyfRekYYwxGQdRJUE76FwZTs2UmdZmN5QbtgZ20F+Z576+9HOX1cHVE0KsT9gym65DNGHRxgkLhKGr3jSzdfTOdhUN4A11qrTE2zp4jO/opclPPwkZtdLDK0uImsvcyBqZi/IlrUO0zDCZb9GqnCClBUoV2G3HuIFsUBqbnkKDKy8pdDlbzrGz1kGwbjwYxXVynx3q1S9wCK5JGpNIkbIPW0GpALwITpe8g9l0hbEPUhlgbygPpskPTSF8PObpXCLdABxDHoHyL8gRxElDC+HUXYHJDqCghSSyuuMS2hqN9RCpoDUG3h591MdaQWAeSdMljrx0S9CxJAtZofN8wMCIMjihG8ppcxsFzHQSNGEmXs0YhZxZPc3BduHbf9zI9XGGp9Y8Yo7CJxTF5enaDG35wkNMvrXPxOxI6A4abb13mAxMxN13y07zp079MzjGcq5+EYYVEhjeesxjV70Aew8l/Szh4i8GYFtZVXDq+jx25CVzHo1iKmdk9QdwImC6OktcFmiXFqaOfZKwyQrtWo5jP8erXvpL3/dH7mB4ZI1cps7rWxIkTVs+c5sjBo1x5+RyL85sQGq554bMJinPUD97D1uoyYbfH7sueAUt12jG88tk38bG7bmVmcobqyhKN1SqxbDI3MY32jqCVStfnWksYgHHAG6mRPFwmWIfxqSKrvRYzk0XW1zboaqHVymBrCREx4ICx+A4Y0aye1ZRGYbbos7C+RaYS4eiIinUQDdkcbNkaEzOWpcZZFh9a5tXfeSOOLUCi6XbaJHHEyZOrHLyzy/R+w4IYHGUxRjCkE8PagPQjehS0TcKI8shTxFcFsh1NmQLPeNb1NHsttlaWsJ7Q3qoxWRpio9NETEDYblKLXbKuT7PbAu1igxAncLjosv0snDkNrqZgc+y+fDeHH36AZh0qU5qN42kkb8I0EtJJjDYRVmdwszlylSE6m2eJoiZecYCpvRcxuf8ZZDJltCt8+Ut3cu99hxitFIijOl7GY6Pd4uzKChcf2EsSzXFiZY2h3hHG/DYDqkj5iisojg6jHIvn+1h8bL3J2YP30+hmqbcCXJokiTA06KCdURbOrbE8XyeyGlCURvO0Hgho06Ois5x+AOaeZxkuQxJZvGmFO+WQ+av9LIxW8XxNNKDJYkhiSDBsfQhCz0W6DapnakhGIfkSiWRBazwRVMmFjRpSHKW2sURUiokmuoRnT9Ixgywe+gIm5zHmjULo0qm3cXN+f2E16GYHp5RlZG6a8VaVufx+VPMBiKbpbNZJ4lWiXpbWxlk8ZYh6S4RhRKujGJ/biR/38DIzlIJlToYZjER0qqcIq8do1DrsvOR5uCaG3nFIekhmL0H3frRK6C0fxM9oAjNDpjSEZ+pEpz4EQ7uIbcRLDzT535+2iPawugsIjglwU9/LyKXQ7pCu8U8g3oSzq3BXTaGt4ZqiwjWGsGtxXCETAxWILOgIWvMQ9mD0Qpg4sItDnzpFWQk2AVGaKDE47jA2DNGygelNkuSrODJE0GsiOkEyw/iSIe51CKOEXidBuaBDS6+niEMwicFxDZkcVCowVBBynsbTuj9WVv3to0IQRixuNbj1swFLp/6CN7zi5xgqvoy19ofBGAwdrLEkiSGbczg6D5VdliC0vG5hjR89/HuEJTAKPnj5f+Qtt/4FS3lYU4ZcBuJAkyZfvza+Jd4Hb0kIkhqD/iizQ9Ps3jmLaz3mV88xOF1h/74Lybt58sqlVerSbRu8jMfA0ABBp01ts0oGh5tuupq56WnQCZdfcxH5gSE6Edxz52ma7Q4mgNnZHezbtRO/XCTuRWytrnLrh2/m+JFzvOCqK/jHu2/lrb/wYzz/hVeyf2qMN//IS3jmFRcxkvdAK6wYUCGJGMKehcTQCSxhLyFs5Vg82oNywvGFDXbNTUKxAbkGG7rJibWQZmCJBYzVaO3iuh5hQ5NVOQISOmFM2DMEUYKrNNl8kfG9muufMcGeKzRxx6fd2MIpVam211hbqtOs91jarGGsppzXFA34pM4dwGDRWAqOg8Hio+hhcZWQmJgkDihRYe6GA6h8kUImT75SYHFxngsvupQLDlxK1nepVCosb6xz9uxZ1sIW9UYb0wxRsUapkPkjZ5i94AJKwyX2X3MxldkJ2jbhuy+Y4adv2IM/2yfUTaOOcrGAo13ido2o3SVotXBz4xSGpshVhpjec4DJqQkyGYeHH7iFxeNHyWSE0f1XUTs5T65QIFfM4rqKMGkTmh47Lppifud3MPBT/5nhVz6X3Gge5Sa4GQ/bM5i4izOYY1wVEBOQz7oMj00yPj1KL1BsbPWIbIbJ6UmOL58B4Nx9bXi+YfDtOTKvyuNrl/ghjS4p3K7G3gj2qIKRPLIwi/IVjmiq7w9JYkOvFzFqCxAbEtcnW8qxUt2gHccYFWJMg876BtKq00lCwuNnWA7vJWg26QQhnyv8A0udm/mtQ7/Cu+/5ZUwUYXWIzvWI4zjdNAFsVrdYO3OW9omH0AMV8pN5OmsekttDtxcRdyKMyeAah/WFFutHFrn9S5us/tPHcJ0a1nZYPlMg9g4wvXMni0sZ4qDKxtIyp4438AYuxq3sJvRmqLanaGyugFeiZzus1IboOFO0y5dRGJilMHQAspbN0wcpD+7huutn+YMfVySmS9L08BKIkoSgBUEEkoX8QJakC3ENTB7uqysmijBdgvvqcFsHeqQTrQO7QKK0c5AM6Az4iaV1wjJ/71nKw4rI9v841kvwRiBsbqAIIM6jTEDQ3sBEPVQieG6FJAKjBMfGmBi6vYSgbQm6EIUJYZyQ2NRpFopQLisGMg45z8F3vDSlIw6i0mWu9VaL0ysx1SXL6ukWb/nPv8rhI8sUnD1Yp46lh1/bxF9ssPNwRMX12P0Ch5xKAMOfZNdBDG8uvYEPfuI2ph8ENHgaIgGVfZpW0TxRuF4eRBjzR6hki7jaYShbZLQ1wtKpJcZHR6iubdJca/PRY3czPDzMybMNxkc0Z0+dpljI8ZrXfQ979+5AxS1IIk6cWWX+2DlWlzbYPz7HC8Z24q8W8BNh6ejDtJpdigMVZsaKHLx3gbd8z+sYGRrl4LGT/Nn7P8fq8kl+eu8l3HvnAwxMQrueZas/yeMkkBhLL0jX3DbbUCwbWnXIDWoKeU0hsRw8uczuSzRt6eFboRh3qdXAxcPxBY0h1unWzpKfR2mdbqToKoJOAnlhx9UDFHIOruPjc47pqz0Of6pK76JlXvg9OVY+XiE4V8ctCMMThpznExUy0F/iB+my46zj0IkTHITIJGSVUI8DxpTDbGGMwniJUqkIErMZ11l+8BRTe2coToywtrTI4NAIrVqDTDHD1lZIuFDH0Q4RDWKTZTA7yK59exkeH6K11aDVbFE7u4QvDv/32DwXdiCKNE45Ie4vk9xYX2VqaoqQLK7rkiRhums37qINrC4e44E770HZHitLNbbaHXbPjOFmO+R0lm7ZZ/9UhcGBIuuNFldefRktT3jhDYPYqIuXLyGdBGV7SK0Lro87PAjikr/mAHz2dvyMj+cLQS+i3QlYWKrS7mzx+du/jJYskxfvwlUdMpdWqH1iDffhQbzEpRNp3IcjnOes0plfxLmrAs4y2UvmCOpdHCky8FMeZjkmX3BJ3C7J8jk26usEi2vYwRwSxrRQKJWuNGpUN3DMDKs7buWhL28y4bqck7O885or+S/33oznaMqZHHg5ut0aiWTQouiZdNel6WyxZbL4A7sYrq/QbUQkTh5tQrxcifr6Kt7ULLY1D55PYbDIpRfFRJ7HmYMt/MIrGb5wFytHPkyMpTIzx5F776M8MMK1zykSnHgf84uCn2uzwUmGp0fJlxyyegeTO0eJjTASnqTdHiY3sgOPNTLdRQqVPXhOjrD6Jzi+QbmQxF3iqIXo1DlHBrqNLoY0Yl3fhFgMQZA6M79o6HXhSBbCbBFWOjxvwlJxDKIhCBSxMSSA20rITbjsf9lNZMrX45pFjt7xPohCkk6LSHVxC+PkCmNEQRvtKOL2KtqfRvU6xAhRDEGQ4LqgbUIUP7LTVCE63UVeKWpKOY2fcXGdDErl0MoljqHb7bLVanHqZNqJVav8P+be+8uyuzzz/ey898nn1Kmcq6uqc1JHiVZGCEmWSAYsopwYYwbbMLYHewYjLzOAx2Nkz9iYYA/YgAEDsrAAIYEEyupWd6tzV1V3VXXleHLY8bv3/HBk33vXumPPXev+oO9fcNY63/183/d5n/d5WLvi8vu//UU+/Ic3MD7Uhy9dpZjSaZgqQ9at3PfWGdYaJYYOLXHtdGvjW3lM5sye42gr88hCcHuksMPq4s+8Jdz/gyGr8uCDD/7/j9j/H88Xv/D5B+//+V/A8DVymTYcKSBwXFRforpUIXGoC21LAqkccKIwi2PXmLp8htffcRNdnZ3UKlVed9N1bK6tk+rsYHlpjdLmJk98/1l++dD9DO7pRlvcJHukhyidREomqDds4nJALp8kmzJ5y9G38uRzL+InQpyaTblaZWc+T20jpGCX2J05wHziFfRQwkdCBAr1RkitHqIoEp3dIcVVndATSH6E0SVjmDpV36e732D59FZEsEq8w8GTZVQ/hiLLmIqOFioMDnZyZW2VKIoIHIUwlHECj+E9Ll7YRFHq1P0m1ZUU0y9rqDtW0cwssewaHcOjiLqEH5bIxSWmjjeJRERrlxXimoIUhJRpScYSkUR/pHCDvI2h9gGkuIaqqOi6wfMXXqK4tISaMNh3+EaynTnKm2V0Q2NtvUTQDDFQ8NUmprBQDJ2BgX7a+jowrRj2ZpWtu7czO30VdI14ro2vnH6R6aWItnaFuiSQFYkbDxzgxgPbEbKE63lUKxWQZRRJolxY5uGvfhlRqBLaFeaXV9ioNEjoOqm2bpDqpHQLCY9IUugZyDB+8y6aeAgE9mwDLW5gn17B1Gvo8RySlUGNJVHjKWRVBk0ibRqYqTizU7OUikWmZ1cIXI+l1Qp7t28nH9epS3HGY+20VQZJ7k0SuJDtV5HxsdYNlNMx5H2dmEtVnll/gZ33H6F6zUOUPKyyivuIQBmEs989gRtF2HIRyc1Rl6u4PQY1z+eFwne4VL1ASIPp1dNMbc6RU0OGhreiCYnnn5kgdDO8+brf5VjHm5E1Fzt0IIJQBCiSjJ25RrI6x8lzBQ7cvQvNSKPLJqnBDBvXqqxMTmKpc4TeIvVJieKVBESjNE7OYhTiJNoP4TY20M0iShTRfWALxZWrBJvLZIwAK6YRT6cxhY8bRtTrAc7MMpXVSXq2taNavVQKy0SRRm35J8j6HhStjmxtpbYyiZnK40QSxsplIkkQKgKiGo79qpVAEjw7ImhIhA6oEVxYgfGtvXQePETfzjEuX10ivecYJ6+u4ORHuGgNcNbJsk/aYHUjYtaFDgvUlIwRyNSLZfxGGZc08fbdiI2X8OQ6ECewK60uum0nD//xx9l94200Nq8RYBEpEX/+rYfRjJaWHlqWJ1EYocqQzEBPv0xvm0o+rpGMJzH1GKqcIBASTduhXK1yaaXIT59ycYoSldWI6FWK9OSTC5y8skSu3yamB8hRROPTIaefnWDf76lMlm1ieYhiKve9+128+J2fsloNiBIyJ4dDXqLBNj1PMWxSv8jKgw8++MX/Hba+JiiawPepF12adoNNt8J8c5nltQXqdh3LkJDVgM3yKu6woLe7l317dzM2OsY3vvJ1Lr50grHRAZxqhdkr08xPTVP16iwubHLPoRtpe0MeK69R7zBI5FLUqnVScYt4Ika11sQWDu/4lQdIjbRz9M5jaJHG4dsPs/vAATbaGhz9uXt4950f5OY7bkBzdDxCdBVkVSKViDHU10EYhrimTyIfYKQCAsfHi2xUzSYoKdg1ndGbV2mLj1CdyxOmbEjayHLI4LDMwWPbOTe9yvBQBwECx5Hx6xK5WJJmBcorHlNnfeo/HGJxOmR5vcb0gk1hc5P1mo2Vc4j8JroWYigqoRD4soIsQxqJdi2GTyv+JQ4MynC7tpeYGUPTVWJ6ktCUODlznJgdkImnOHjsDhpeHbvcAAMuXjjXUs14TaxIxwpSpPJxegZ60eMx8EI6u3K4kcPC4jKhG+BvVimtTkOmdbE/fG8/+BKh9+ofL8mEvoOGRC6dpLx4hVce/0cuP/c8aVllZvoqy5sFRrbuY/+B/eS721G6exjr2UrP0AiZbBu79+4gNtzFwull1tdKxAyYu1QF38XaqmKNj6G159A78ijZNKgaETXsxRpaPo8WhZQqVeaW1lCFwKls0JexKC4tsXhtA4Ate/chehzccgXTEET9HnKnwE4XCWwB33Zx1xscUnbCWZ1mDdbMDVYbFdTXw5nffJpm0cat1KkGAdGIiqEFxFdlfrj6F9x86CYsI2S2PsPIL5kk3l2hK5Fi4eJlgok1KqJB6OrIkkokVQh9nxCJUJJRFAWv1gRg5Eg3d97Qh1t18Bt1fNnHbqoYGZ2+7Tvx7DxG++vZ8f6P0T+goDkeWjPAa67RsT+FEVaJD7yOesmg1tjK1ekUItSolpuoyTZCNUJLxkkpgpzm4KhxlJG3UC/n8fwGupUFyUWPNJrl52mGW2k06yi6z8rlr2D7GdAN1MhD8nxkCTQJEhkgpEWVyBDYLSuhg3sSjBy9g6hjN+cvXqGtP4OeSfPxz/wRkVrhG9/8ItsPjvH5ei9tvQqrWCysgOIIBD6KXSTYmKO5+Bx2dYlaxQfHJKg3COwmhmaw+OIPOHzLAfxajVh7H4G/gShvIgmJwIfA51Xfqn/2qoFYElIJiaShYRkmmmwiYeC4PtVanVK1wvxmhal5F6cSItzoX+hSwoiQiLUL8LefEkwteFQ+18nojW/hPb93PWtuDdVXyLbB0UMhJ4t/z/737sSO5Zi5HWL/pBAEgunKOtH/QVrrawLgm80a1y6fo1ioY7anKFdKFNZWKCzO4TgF7EsbUPOQfItUOo0UteRLqWyOzWKVnbvHWJlbYGzvbhZm57jvTT/P8uVFjk+8ghY6CC+gbrr07jtIJpdlfnqJtqRJPhviVkuosTjzK9OM5TqIxQwq5Sqz0zNMzjYJhYfih4iyjaQI6grUPEEgAiw9Te1Shh0DXUQCcu0eVlzBMk1UV2fmaZh7MsFqyWZgFPbdv8ieI0OULiZY32jy7CsLfPv7M8yducLe4U60hoyl6BiygqYbSFpAecHC1Q0SVpxnlopMzhZQNRsltFhdbhK5IVMvncNfSRAjh2akiRIWfijY2tmBkBXqXqv1zcqtsM9Buhjfs49EOoGmG1SCImuraxh1lZQeY8uenZRLq/i2g9u0uXphAk01sFSLFDFCOSKZSHB0+Chj6hA37biJzt4+NterpJJZaoUCmVwHviaTlPOM7JIIUwF/8JczsAlGR2vRqbK2QHFlnVpxmZXpK/ihQXpgHA9AidAsjbSVYnnpGrVqkb7+AURYI2ZZLE5dYm3mGl5WQjh1vv8P50nGM6xfrGK6NSxTJ9U3gqJYyLoFYZ1I1Akdm9rsMnJSQTQb/OTRn3Dl4gx2aZNyqcpmQXDu3CLFkksmGwNgY/EK2phFuMMgykFYCnC2emg3J1EdgSxURFkmfV/I6pOXwJfY+75hUg0dY0mnZjcIpYBIkkhJOSQtid7ehyTDu3o/xD+c+CcKtSaqbvLEVy9w6eEaBSQCSaYcgutpWG1FlEjB1GLUzBARRkRxD9sO4FVVUm2pQtuuLoxGmYa3gqt51BfOYnsLbEwXUDpHuPrMaazcKMPveBvrmyfZ/tEBDv7ODcx89wVcP8PpEy8g9QxQLVa46/5f45ELEhcKMRZPn6A8u4qHgZJIQ1ghqi0zf+oHXLpwkZXpCWolQRiG1JsqQd3DKV8hngpJWiU0H779D4+gCAXf0AD3X+gYzwchgaq3NOvGCMhaRL4tyc++/y3MqEbf0bs4dOQIt999Nydefpqt7QrPv3iet903zGf/4hN89pLCc+utQevyhoRwwROCMCgQ2k1EfY1AaHz1Bz4f/q9l1tYG+fyf/QhZrNE2OECAjVcuYRJH8xsEAQgPhA8ikFsSSlkllYFcTiZjKcQ1BV2NoUgmTVdQadTZrFSYXitz9lqdiQseodNSzBEprW0rJGRJQRUhYQPeduF1HL5cITl3hqk/PcGmLRgYhew/aawUBPW1iJc3LjHe04/kQPWNAklImHJLDvpvndcEBx8RIcdBr6l4jkPpygbSio/dLCFEkd03jtKRynD67CSdbXGef3aCQ9cd5LEfPso7f+kd1OpVgiCkUa3jNB3mzp/B823e/aF3IikSkqSQ6+5ldXqBsLLJ3h2DfONvv8/113VRaxSZmTzP+Nj1rL+8iq9AVyqOFvk0mgEvnPo+jf5buW7HOIFQSGoCLdBZ3wyozK2Syugk0h69Uo5CVEfRBapvYU8qtO2pcrjzV1iN/obJqw77t+WYks/x+rvezKTzbdbnFSLNQr5tDX85z+mJeXr608QSHkbMQtaLxHqLyIrO1Ikye9t7OBk6SIrOliDFysICuco+2j1IZnwqkkSjqRHYq8R0DVmxaNN06r7NsGwihR7b1DZu7L8RSVbpHxpkYmEGp94gi0HD80lt76FcrmEqMSrFZeZnr5Iy4ghc/LAGhsyx4R106X0ETZ90PI49vUKuM83Vyhw6gkx7jtpGFacY8N/nv0r6JhmrW8K90hr4unOt0uOVk2cp1zdIWClUK0kQeiQNmc6RHXT39vDUkz9FsmKsVRYYSSeoprJwdZJFr2XBMHbH60CTmLha5dZf2Ifr1JANlUMPXEcou2jxFDIQBg0QKr6ngbtIcnAQz2mi+AG1msd6sc742FZkV0IuTZPNJcim01hJk5oL7X1jNB6tomsJxFiV6CkT+bJH9AETLwTN9vAGMsQLI+S7PZbKZdzPQWPtKpVYH67ioBopgoSJZSQhaaBlMwyMbmXuzPO874ZfZGZuktNzLyNUk3CtweXGFI2qR8bSsd2QrR1vAK9OzdCINXwWaKBWDExdJYxaddovPjTD1z4a0LXvFoze3aw9/xU6dw9z/B+Okx0oUJjzOPbLH+HUVz9OZkgltz/PZrGf0nMX6bmrDzbSSL5P2kwhx1NUlma59+Yd6N55Hnmqwp3XKyiLFSSpgZ40Sfb3IKNjNpYpl5N07d7BxvSjdG97HU27ym+hEQAAIABJREFUgKHWcOYvQcfNCG+eq6sh7NRQ3BKSHhJ6reGq2wTVjgiRaNZBtcBrSPgbK1xaU1h+9gy5zhRLs8vcqKT45EP/ky//8f04Ez/gglxnqK/EA796F1/5wqOsNiHZjJhYkzh/Sud0OaQzL/iDjxzjN/5ynnznGJ3qywy0z/PGG2wybRa6lURRLEBFAFImiR9E3H7bKOPXK8jaMkQCS9JRZIumL0hFNgkriYRGpVHDdWC13GBm1WZ+wWXqqk1pA0JXJhIQSQJZkiEKieSQP337O/kP3/kWm5sZ/D3X8eOf/IjxD0jk20NiVYX11wkGYrfz4+LPqJcEi4Vr+L5ACiQiEeG40v9tQ/x/f14THPxn/9tnHhzdniKvdxNsRjR9mfxAjkJpharWoH2sn1g2y/ziMp/81J+hKDpKFPInn/8UqWScUydOsVkosXPvbq679XZOfutpjr3pKLl8Gs2QcN0mccNCuBmisISSyFGtNxFqhu4OgWND/8AYv/exT9BUBMtXZxnZOkil3MTSPSK7zMJqjZI1halIqCmPaimiVFLIjLhkczIpS8GtxhEBFNdTZPMOPcMOq+cjbnirybVnq4wdFmwZTjE9N8+lxxVC1UOVZcpTGdT+MpWFkPywSTyepPfYIn19h5heWEBaN7FtDy/0GGtrJx5TcQJB3MuT1kyy6SSqBUEkaLgB1VdKmLJM6NpEkmBEjnE4tg3Jd9gW3066o5OGvcHGegE1DAhDhYbfINBVvMBDU3TKzgZ2uUh3coiYEScWJTCcBFolpNtqR9FV7FITJ/CQwoBUJoVn23ieoLC4zmJuiS+rX+P8jEdbF/T3RyxOASGoOTi2/yB7xrdw6dJ5NktNtm/fRUc2y+lXzrK6OEtSh0qjwtTcKmPdffiejBPVGe0fxa1tEhsZJzIVapUGlmFiGSZRNWDrDePo8Ri6mUJRdAhlqrMFIkXCqK3hlWXkTILArjP1/Wd48vlT9HX3sTg/z9rCMpahk4hZDA+109vVwdWCzZHgLqIVDevmgGhChVCmobuI6fPEl3MQWEgP6HAygkZA/P0Gkz84xWMrT1MuFTBibUSGimTplFaXSI6PEEskuPTMI6QGOoncOD+4+DA11SHyBU3Po9qoU7ddwkjDjQzuPvBrhL6g4S5SbdToqGTQdiSoNXwCSYHMPJlglv1bUkjeNZDXkZenqZUNBnd0s/XYfex502+jAgsTzxHLZQhEy0y6VHXoHW2n6faQ1jR8W0HP5FHiCfSOAV459yIpZPKDI1yZLBD5NnK8GyFscp0WZixDtrON+toiXakQpz6NU66gGgGRLFFZLlJv+HzxsSV+8bAMsg/YBH7Qoi4icBsgPAlZg8YaiChCtiWKZoKrzToJcY1PPvQpKnWVqzOvMLIlT37ApL+vnWalSveOnYzEVNxrMwRC4suzCaJt1zO1vIlbtzlz/Dj3bfP4d3cuctstnTh2E7uwQnb4IGrcJKw3sLJ5yuvL6PFe2u74KfmhIqZZRpMCYlqEqvl4ooqiNEF3aUg19vb9KTOrP2Ol0OD8TIPJiw5X50KaVfDtCL8mE4UgSQr/TNREgLZyjqaSYO34Zd71mx8g09/O+s4r3Di8hUybQeXHNR5emUEqqPzmpIqupJjIO0h+RKYOtiEhF6C2+K9z8K+ZCv7Qjhuw9CSpYgItrYFWJRvvhFSIrkQsLy7gND3asmnWF+eors1RWXoHc5cn2VhYJZdvY+u2Ma5NXWLwyCgKDXCaCM1EM1QcXFKbEiWjgVSrocoqTqlEw3JRNActJlBVlZXpFbZvG0FXJJqNOm6qn1eW1ukaVhA5FyIQVYXO7gyZfIPQC6mWPTKpiPqcgteEhCpjWBaluRrDR0/zg+/43PHm7Vx98TIHbxDcdLPO43/vINNytFxcWGW40M6RNw7guzL1/Hl0LcX09GWyIonvCoQvUfZ8bKfCUFuWpXqDHUd7UOUMOhAsOgQiwHc8OuNpZhoV0p7HqNlFRuSpVRvsie1FTwkk2cdvhIz0bCNMBzhNmJ85g5XO4vg+vlvEnnfIWl34cQnPE7iujdv0sRS4tj7PeCQTGhKBE7DphVCu0dvezdlrU/zF5UeYlTZQVNi5Hy48Denr/tmZXkKttcb/1y6/SH97ByJQsKsFAlXh8IHd4NqIqEHCiLN1SGP/dXuZFQb9YQFVtjFHt+JnFMqlElePVzjw+j7ybSaBp6KbOpqmAQp+zSVSFcx2kIWN0reNWNAkiiJkXeHS0jpDQ124XgNJNZFTCezIJ3RDPLtJU259Hk4hIHM7iGsW8lUfZSwkNZSE3qM4L1TAKFH+Ug11JMXm+eN0/+gYH37i19nSdoh7RtrxGlUUOYniGNTcIpw7R277EPb8JkoszrXeM5SoYKJRdcpIvoxuxBCE1N0Ggz1dKHENv+5RdRQMRaagFTBWoUPKshG1VDSf+I9vpLY8i5brYunCHOESYK7RM7QNXaniLv0Qp7yJbzdZOTfL6I034Fdm8RyDQPTzyrmnObz3JlTPQzYUVEVGlD02V5u88eYdOLUmmxGk4xmUaJ3u/XsxSOAVruF7dZKZDoQSJ5nNorXrzF+8SDrlsOrcxtY9d/HQHxag+CnkyMC3i0gKyB4IHUJTQjRbPrqqASKSKDagP1Xjgf/xbV762h/xj1/+Entu2E4228OJU88x2j8KRg3fVkkXNnnzB3+N52MeH/n8i/zO7R5a9BR37oJ8HlIJaItDNQBjtYCuZbA6ttKsriD0FIqXQC5WSGUGCBol1jyTHj2OZiSQqKBRxBMChQhT1llZk1gq2RQLH+KXb/oKhjXMez91E5cuClRTIMkRwpEIX+XJo0igSPyLvcATdYW/fOubuHL6GkuPf4kXGmXqEzb7r/NpFOtUSxI3vmTx7LjLX3Wp9L6wQSIQVEahKIF1RUMvqkDzX8XW1wTAx/QE+0cPESFRSzWIJmQqUQG7GdKdHcGK6ZSmKmimwUB/P42qjSwciqUmLz37AoZiUZpbQwlU6rNFYpZGKBk49RJGPCIeM5CkiGRDpmskh5AEI2N9XLs8S8P20fyQzblJRCBwq1U+9IHfom1fH+94/d145TKa0GiWykgDYFcU3JpOa1Pfg0imUolhFxTari9SnlSobOqUlhTW6yH33rmb5oUm9tkutJUCi8dH+f73TnLLrTlOPV/HisUZPlJjy9EYX/rERR748AEqgcHyNZfx3DgTE3PIaogiDHL9FqNbdtFuByyUTrPzUINrq+fxw21ULkkEIkAL0mhOg2F0rssdZaE8w0Ami+Vb9Iz3s7m+yOLFWe5+xztIJ7O48jJPPv0S6Y4e/FoV0XRoVCQk38RWHCI/QpYhaIbIvotm6SRyMUpOFYSJ3/TI9/Zg1xpESkg2luRjt76P9//s04zvBywZREhtFeReCebAiVoyyWy2g7X1Gh29ndRqRZKGiu6qlGo18u15jISD5WpUS1W8yKVjuJua2+RieY34qkFxQ7Dvdd2Mbe/CjCdRRyN0LU2zUkO3QAo9VGkdJbkVWU4i/AqN6lV0M0vgyWQyBrLew+TVOXxPoiuTpOxEDA12ku/fgqSpsDKJXrORf2RRCdeI/0IG/xtVlGoO8Y8Osheg7Y4TPj3F0sQr7Pqt+7j4ze/y3l89yJNPvIL5rgQxcy+NL7tMmZdJZLto6DXqL57EHOymXq4ysbbBnbUUj+eaqAkL0fBRAgPdilBUj5q7Bokc0UaJfKKXJbGCHlOJVWFJmSTyIwzAi+9AlSZZmJuka/so/u5bWXziG6yX66jJC2hSL4VLPyXbs4WLFxZJXHoeVW1nbbZAOnGcuL2CV40I7XXiTjeVub9F2fIefv7nXkdCnUBvF+SyXWwU45Q2ptCnrpJq74RoEzP7FkxlFj2/k5g1xsrkl1hftJlQurj1DW/ko5/7PBOnL/DyB2N4QR1NA0+GyAVJizBUiYokUStFVMsSvivTNxqSr0Be2STbm8XKHiST07DSLqtLK/zgqQs4tQaKL9izbxvzyxu8/SN/ws6n3klYnCUwIRUDpRIRmRIlG3IeyGpI0y7Q1p4gbsWh4SMlISTAXjuLkLP891/bQNbBygoSaZl8h0S2XSZtSaimhKUrBGGChauC93zvPWyuenz1obcx/8bHaIRNmkSoqKhRazjecALe2PnbPPTiZ5i8LDNxJuCn332Ut8RtBjNpKh/UuOYoKFJI0dHocxI8awrGL0v80ZceZuHMOX73px9nVzWiNpzlpt4833h08t/E1tfEkFVVVHKxGJmYRcaMo1k6AwNd9HRksRomRj1G23AXqaRFX2cPRDaVYoViqUibnae8sYG/UaGzq5PeLcNUniqCYmLFU2TaBxGSRChF1MtreK6AsE5zo87y0hrJmEnNrrO5WUA0bT77V/+FviNDVGdX0QqCqwvXyHUkqCw6ZBNDdCS2MjyQI5GJMNQecAZRSr0cfwoiW6ZcFdh2hVQ6ydZdCeaWA6TI5aXj08TkQVbP2zTKEs/9bAMnEJQ2SnhFwRN/t84HP/xmWIX21QyJmMvl8wUiJ4lX18n2WrzhzTlOPX+eRLvK/bfeiC+uYqQdPHeDKAJNkonFdPrzA/QzSEkpsKNrhB39+2gfyFBYXKS7Y4Drjt3A/MI0D//wa3zru48hHEFjvYhrC9qdBLKjoqgtHbDr1Wk2BYFwgYBGvUppuU7BrkLoEhkBbrOBFPmsLK2Q0jQ0W+Z79/0ZIox45bmwFUXTlLn5Nr11416VFFyeXsTxqtSLZZRQIlQ1irbLtfVFyvUNDClCUQTrVpod2TiFYp0LhSJpQ0FO6GRNjdFtWXTLJBbLIRHDadpofoBe9zDbulEW0shqHH92CUWzsOKjOPU1ZL9CvRmyvHKNuCLYP9pGf2+KHQOdtLVlSaQtTLn1Q03DoFJbR6r7RF+toqGitOk0pRrBeonSNRvZtEgmshS/9QzJKMEH/vgt3P+hI3QOpTg3fYpPTH+WxEYSbL+1wJRwqZfWqVRL9BntPJ0O0a04GjpGxiAUAbIvgSuo1lz+7rnfBTnADyChZ9FDk1VlkV47g1g/DkBM9mgoA8RTo6yeWiQ7vJOZJQnn/EUazQTrDQmxXieVyrFr6xYWXrrK5WdPMNitMnGizGxVIkqoWMO7qPgeV5dBDo5TvfI4RnYnUpAhm4sx1OfSnY9TLtexDIW68k4iwyfwIsqzzzNz/iu88tQ0vaNtfPqvT/D6d7yLmZMniSezoNTQtYBIEmBHCDlCfnW72kqBFEG1CQUv5MxVmaqAzbUCpiXzmc/+D+RkknqtiR9BWyogP3wzaibLS2eWePbE80xfPsvXvvdNEjEwNNAzoOckJDNCN1padh+BiEKkSEGKZ1CTEqHnYiATa99BW8cwkde6p2EoQygRuNAoS6ytRyxNCyYuCM6+HHDuhMtnfv/LPPLVn6cknuHRyQYf/2PBxz8W8tEHBL/xgM1v/EqV//SxOq/72IN0FARGf5OHfjPN+3/jfVwUHt+aKDB7rs6pScHZMyWWbcGxd76JJbXJZizi0V9+K1Of+c+87+h7WdIlupfL1JQiH9o9+P+Cpv/P85oAeFmSCFyboFYlMmTMvEbveB/Hfu56uoe6qZ8pk97aTUc+xeUrF6kWq2i6imTX6IsN4JVteg/s5PHf/ht2X3899z30y6hLAstMUZo4gyKHaJoCCQlFxGjaDlpMIpWO4VfrxJQQOfBBF6AJYrE4qBoHbj5AR36AyuUFckInCiIkbYlACNaXBW5ToOo+ycEYiqJw7kmJZLuCbkC1FLKx4HH+/CRLjQajHd08O3GROVFiYCTF9Xu7uHu/TjN02VhW0II6JycfZlJ6iXCgglzoJSZiBMInk5TJjllkcmm6D2/g2i5+UKLieEQB+GGJWEzDMhJoUkTMVZGGdfbmtjKQG2PmygRONcCyMlyausi52TNcWLiEj0ANQoxGRDbKkXAs3CAg0jQCSSdUJdAlnKBG4LutLUtNoRY4VGtNVlZWqNZrLM5ewa45EPNo1KoYeoRUr/Pzw3f+X5JIIbHmB2y7VUcOW9OhbcM9REJgezaWJhOPJNoyJsMdeeplG1XWac+P4taWieXayXZ0kGmPI6oqmqNy+O39WIqFaSSpFZsETQdT9jHyGbwNF3d+EWEmaP70EurWIUTo467PoWoxLn/9CZqVOnI1IKWq5FNJtm7pZGy0jd17RsilVdKpVj8dKTLpjjZShomVSCGZEsqlDXRzE2uwDa0KipxENQwkRScI4LlvP8q9H3gXUSiRiWtk4h00Ip9GvUzoh0i5bsjkMTvSdKc66ZHToFqEsorva8iaRqhoKJaFqkHDqaEl8yiWjBLINEWVnNHLXE+BZvtWAMxEP4MH7oVyje7DO1j50Z9zwy2dvFxs5+T3z1K7eJzBN/0qXbsOMnpkDwfvfyND44M4JYfElhG2j/ag6jkIaqTaBxkZugFL2aRRN6gvvwyJOLLiIyKXUJYZOPg+lM67iLdJWGYCz29SWlrA0PLsvX0fab3JJ+4fI2YZKJrAdx1Q1ZZxo2jZDMgyQERktpKLrLSELEHBAV0OOeHLvPDSGWoNeM+7DzH3ygVW1xoUl1zKBYtbbr2DKNLRVUFxuUBp9QxhY51AbalyJDtq2QrLEpLZisPTjFcthDWNoFrErxWRsXDlCFWLUykstOL5BERBywiy0YBqKaK4BksrgvmrHrOXQ370xY/w5996F4eOPYxh3sgzz4eEYURotwI5QlqzBb+sUJvQMJ4V/P7gJ3j8U0Uee+TLnDmg8M4//DwXH5XYvtjH/o2D7Jfa+eu1JfoOKnT2x6h3Cx6NS/xg5u/ReuFyV0TMvA/P/bfh+zUB8FEUUa9vUFpf5+zqPMKo4xUiTDPG6O4Rukc7WJ26yuKpCeKRRkxR0JAZ37uNqljk6OFDvPs3H6D77bv4L5/4DHPnXmH/LfsYzo1w5Nb72bPvTsbGD6MKgaWDJEKGtrSxa+9eNsoyPX0umZ42PvhH7yLb1Y+GTEdfG3krz+6+IT7+k29juxsoZo1GUadZc0jnJFTTRVFdqpsrOJ5NsWChV9LEkwlqyjQ1MyAWi6Pq/Xzv6TOkjU4mz5d55WKFmpcg223wczeOo4kkQgE9EZHTkxQ2XMprMk88OUUo1Yn0NJ2dCUpFiaGtFjISz509wcKigrOaxHV8UoYESgpLEXjNgHs7jtET78CpbOIpLitrK0yvT6KYLeOxvJemU3SQDRIIJ3o1q7RJI4jwQkHTadD0HYJ6hGEoyJqEKscJfQ07cnCUgLokqFTLREmT8yvnqTebbNgFivUSm7VVrqvfyKc+fS9Wv4wSE0R1FbXHA7lFTNZrNXq6e0kpLR2/sCt4dRtTN2lLpujIZXF6+hhJxTBNn9mFK8w8vkGqy2Ln6/N4FwOaisnGwhU0s4mVSIKZhbqDsWMAvacTdbgN88YxqlcX8O0qSraL5otTnFkpYcUsBoba2LN/lKFtvWTas8TiFp7nYLtlfCMBgDJqQa+BLUEgGsgxCWFFqEMJ3PcJEveoGKqKoccRkoEwDPbsvpm1F05QWBN89YkZEjnBJ89/nLWYhIdJdWYRQ01gZoZw8imOdt+J7NeREfiRh4gEUuihKwaRq1JcK1KoLnPm2imkIEQxU2x4BUplF0drPUSb5z/LygufwuxIoYYe6fG91Cp17vmNj7LtAx9j6PAOIjUkkduCb5eIxXyEEWP7G+9m19HriVlbCbQyXiKFKiAzfoDiK5eohgaTF31mT7xE8cpVGovr/Pu/mkbWHGIxiTatSLR5HDQTNTVIW+8hkokMpqrQm4wIQgURWIRBgGTXwQ3QzVamriRLrVAOtbUlbsagt1Mma0XccQ/8u7eFfP5rj/OJh56hKnbxJ5/9DpoqMXmtjBsK/vaLf0DDdwksm4NHduAFa8xdfR63AboftSyQlYhXRTLErNYb09aWQ5MVzEwXspFGS6jg1vErCxgxueV9GgG+hFOLaJRkSpshG2shpWUorYGeEPzhQ/8Nswfe9jGfv3vuuwQbCoQSwml1I0ArHzZqeRL90YTK+QvP8HBRIXv7r3NpUuHbf/FbbK8GTP1gjfbsMKbcyUHtGTJOL6pUY3R4J7fd1s7rc/ey98Ykx2Sd5I+/QnXuX03rA14jHLzwPOYvXMF2Q2qeQt+NWwidkMAJ8GwHNRUhVWQkowdZ0bn9ntu46c4jlEpNVsIVxqZHeOHbj4Hr0LEW8bNHTjB//jRvuflejJhMIGSSMQkqEQP9WxjtOEA2V+fCSz9DlSSaFUGlWGf/zW8hl2lDllshyuPju9mzPUb1iQvsu/seyvKzpFINAllGCBnd8PBtCSUARRO4NsQMG2HnGDusUC5GBIGPLM/TPZDmwsQmW8e38PSLp5icXOBzn07wxS+W2dEf5+y1BpUFwe3363z2DwKq5TmO3Gkg5Dp1tYERlxG+hhIpGEaCyaWQu3NHibet0GjKuNImDnnSjkXCiHH+/BWE6lF368QiDdkAUzHIRkkUR0HgoigSXhDi+TWQFGTLgNBD8W0Uw0RHaem3hUJgRchBBJKK50qEQUBTFuTiKaJEq5OYLyzRnkihyXE0y6JUn+O6ywd533uW+NKXTnH5EcHuD+hkh1plvWEl6BgYJKYNs7awiplI0NOTJQhdVq5MUauWKPtrDI33IRk6NRQkNHr6NUxFpthVRfKukFC6iQQ01+ZhDqRNidCfIfW2fdSeuUhsdx+xzhiybBEun+Ob33uG9s42dJp0Dg1gxPJUm2UUJ0SiiVdt0mzIuEEDAFkS4GsI2cWuhriKRupwGumMh3ZLkiDuEk/nCSQJ1ylQUmoYq3soOTFmZhz6u4ZZmpnkYx/9RR760uf46P7PYJpQCzdQnCpWdzsvrp8A30fVVHQlwFM1JC3Cc210Q8MNHH5c/yTuqMCbu53xgbvxmw2eOvMIrqjwK6P34G2ssrwQIzeygS1liLwCGys+27hGtvgiyBaBM4/f3AXFS1QLNrouuPSTZ5E6j5Ldto1M3yBuPUZkqXhhg+mNOLtGE0xcmueFcw5nZmp0RPD2m0dpbJ4hphUwFQ3XkahuTlClj2LpLLKUwVXjfOHFErphoWs6gWIi4hJaVML2HaKgVeGGEkhRK6Wp3AArFzLgSyhxsBvQUa4xp8JNN9zIaH6Dx5+7RmTohIGNochofp2hLPR3xVmcX0c0MmQTYMUkrDZQdYlAhbgJJEDTFDQ9jXA8vNomiqEj2xZCyaGYEaH/qv5QRPieBJGE8Ft0ne/yqrY9Iu4bfP8xj9eNRKQTMNQhoQFNF6JARpJCFFpdQCvPW0GRIroLNT532wH+7HN/yt9/5TO86cH/xP2Dd/DWt2qousfukV+g0HiZHaeWKX6sDWn1Mvs73osXfIPEjt0s/eg80pE38b2HH/43sfU1UcGHQrA4N8dSeZZurZOVRoFMl47tV/GtJk2lwkisg1p1kXu338T+3SMUJxb4mw88SI/aiaqrPPuFJ3GmJdr9GNemZugcTbF+eYlP/PqnkRWHIBCIxTqqZUGtSXU5pHcgjmg4tOfH0c0M+VwWFRlEhKbEOPy2W1icmuLa/En2jI9SDpZQ4h5WHMyEwDAVdFNmdskml2v5Mz/xtML8pQ20uEGuw0CJNGIJ2FhxKdZq2HbA/h3dXHjxT6is9DA142KNKLS3eVw4ofKFP5bRTbj1zgRtGRlFVhBhDcPopFhZZX3CA0nQOaBjJ69x9ZJBm9xFSa7RDBwixeHIyDb27hgnbhh0tGWJJUzGBsbYkh8F2cBtOuD7OI5DGDoITUePm4QhaKFKhITwfTRZxdcNZF1D9+JEsoYkRSi6gquqBER4ZkjJ89h0mlRLFcpNDxH5VJtFVB1OnTxFfq6DTFbi0F0QdwIO36MDsOfobWzdcx09W/ex99jN9A73k+7IolgyWw/eQO/YIE6lhtPcwG0W0M0G+f44ERlKG5ukUglSIkesPU2wWUfO5IkdHiR+7xjxt24jwiV2ZJx6RQY5TmDb/MWn/5aBgT76R8bJ9Q6S6d1GLajgy0kSnX0EehJBDgBDdgBwjYD1k+cwD5XwtsjUB5r4uoc23AW+DDMCoavEkiaqbjK15RTv/cj7WL/4GM8fP80NB29B7+jl6w8/yVB6gDAd4OVN4j29OIpDeX6GaxuvoOk6kqyhyAGRFKAoMrouE0QehmZSXJCwp2Gz6wKSLqOkUwijhKy1HiJvDYa2t7G6sMLG4iwXnrrMsQfeTuTMU9lYptFQiRplZn7y+9ixHI7cjxzlMDJtmMoSongSQ5nDry8T2kUmnnqMqbkpug6/h/1338Wx/e28/w1D7Lm+j6M7cnz37y+zMDPN9JVXWFlv8v7/Osf9v/MUs1c3WS8u8sOnNzk7UWlZ+hpxVMVEFQoiijB1BVltOXfJckREq4qP56DegJgJ5WvgLsLHb4fPHwLzpU+zZ99eOswMt93Ug6na9HcovOH2cXKdKs8cP8FqocnffOFhvjM3wPFF0M0ILQnJBCS7WiZhniSINAc9IYiZIaaeQXiCKPTwqusoogy0ovlCHzxXxnPAt1vgHgkZVVb42GiacUMhQUhbF6STAgEIu8XbRxH/oqKRIhk5FLiez45b3kxPb5Lt40M0Nkf4+gPX84J4nj1bb+ZvXvwmv/pbH+ZrnxT8KJ9Ger7IufNQKn4XK6sz85NTHPulf8/m+HN8/crH/01sfU1U8IEQdFzXT6avC/x+fnz2FKM72ileqZMeiGHIebrCAW4Zu57JzQnwFC49fY42q4edmb2cLFzhzm1HuOXQEU4NLbA8J1MrT7K6uUlMjnP5f17g4uoFrm8/gttcRdcS6KbJnt1jTJ+fwIpl6B/eQeiLVkCAG6CgYSrwo8e+ya37b+aOG/fy7ZdcFB3ajGQrzAMHBYX5WYMoDFDVEMtSECkDrxZg5XSEomPGFQbHYW+oc35qgbcdk/hNtqijAAAgAElEQVSD3/uP3P+ecS4ugTu7QULOsmu7zNCujlYOpBLhS4IwBEX2WLu4iLNpY8mHOHPtFOM3qdQLENfzTNvPY0UDgE1ypIP4jInj+9xz331U16psFDZYvnoFHxURBlixFJVqBR+HmKGiORD4AaYCimIQhhFEECVNYn6EYik0qi4KEg4eQaiCFOG6Ek65gKSouEoTT4oIypsINSSIZIZTGTy7wdjVYfrycHlB0JyBsaMe17WDKssEPkjUEX4RKxlD1mUSyTRC+ER6hi2DNroa4dkCdalG275t+F6RpG6ixyVULYFw6qidKrLhUzhxnrbD48h6Ct9rrSJaSROWL/OPf/0INxw6ghWXsdJ53FDg1K9BJIjCJnZlCVNPUCxWSMRjiKYM1GCqwkJbmfXTIVo+RHcMjEhB9AdoJ2TKixZJS8Mnjh53uXnx59g5sp/td5XpHynwvUtTyCIk27ebXeUUSadCM9NF4PvYMjw2/3UiOSLSbWobDWTN+l/MvWeQJWl1/vlLn3l93Vu3vK/23s30dI/3loEBNAiERiAJgXArgRaQkAT8xUpaEEIDf+GRMAIBYgYxDAJmuntcT8/0mPa2qruqy1ddb9Ob/VCzH1fsRuzGciIyIj9kxJuRb+STJ895nucgaQ5RKBIKMpoBnu3iBhLRrEx9KQH7I0TR5017/pwfH/tbALpuv5PqfJN8b51Er8BITxY1qmGMvIHW0mnwTZabBonxWxCcRUTBx1J1kvEOBCNFa+kigXMJWdGot7bgePPcd/MWFo9+Z/XDVV3HXdfZ3JBPYiQ20JVyef5oyFm/k0d+/BSeGGKk4vzex3+BiIIoyeh6gggJ3/UIRZAMBdkL8X0JwQiQrAjfF4jC1VJgQEiiF1RzVfxZnF8181MEWH5xkrd84Gb+oHc7R5/+DlGzRLxLJ5tNcuSYz9wCzE0vcHKqwtv2SpycyXKPVkE1IvSUQDIl0W76dOZ6EfQsipzBLDZQsip6biemWUeUVYLYEFEgIiAgOhCKEdFruXAYrmblu8Y1Nl21k/ftOEtz5CrOhY8Ql0QCJyBoSwghiOFqHT4KV2s1AhKSKPKFv/4EHV7Euz/5NZ6Vfp/gnzv4xOfexG9/9MMMhxqJ4YCh0QBJKrJlSw+leYuO7k4S3feyv/wF5pSnmRcc/vQz//BrsfU3AuBFWWTNng2kOzooXxHZ1N/HqcUqo10GlYUVsmY/YjZg+pGXaCk+6+3rWFELtJUy2XwX+eYcSyWbJ351hAYhfaHI6WY/3WPdrLEHCEOJriBHrL+XL3/oX7Elk7d//HVoWoKHPvhuXnnhCD09GRAFxFAATUF0YerwOf78vZ9gx4270QQZIh3L9QgUB0UVkFUdW3S5/QEJMYjx2I/qrN8e0N+bRW55SB0SibyLLsucPVZj9/44Mb3Jqdku5hdKLJgiltNG1jqw3Aq37trG8nwJr0+CUEQIIfLAdC00XNphjnJxhqXLHjGvk6tuU5mqnSKrJPGJYbQFqgmB/EA/BD5L56aI5bIkY3FGN21gcWYJywI7NJGFiNAPMR2JCJt4oBGIAolsHDHwkYwkPd2jOI06pi5iBQu4hESuTCBG4EU4WCDLiIK7OgDaDRgd6uJiYQFbDClbRTrjGRZry9ya28+ZxSMYfQYXf2TC+0DLpHBti5iqYWRz+F6I6/p4vkIoy3hyir4hEUVWQRAYvGk7p08dJpdQaZd8Ki9bpJMmpUtltn9oL5EbYWzuod1qoGgegqghEuP8T/6DdhCxZfs6kqkYltVa5TYrcUJXp7url3JpGc8r0tOjkYqJeG6AoK1aFbT7DdYN7iJeCpgpFMnrBm0Dik9eRu2Ko69PwjkNER1BMQizfSRX2jz9lTYT6i/ZvvOtHHjhNGLUZMvGtzAzd4heJUMzKfBvz/41w3s2UKnUkb1OQrlN5NnIuoHn2YRChKDJyCL4nossw6ahLQgxCcmJESV9VH114Icmx0kPxFiS82j2BVL5XsonnkQYaq6OfpwvcGJCZMu6BeYvWOx77x1M/OBZrGSFbFLFs5tMHXoJLJUri0+wdt821FiRgT1/jOs0ua35RQa3vJMrR5+gUKmwbtcQN/e0uVXu5x3b9vGfl/uYWqhx9NirpJIZBFGh2bLIdPXhhyLVag1ZShD4HShqHcNzCV/zd/EEgVAKER1Q0qCszuImSMBPJ+DVBiQUl9vPHsEY3MTImhFU0eHK1CSnT5c5OyGheS6naiW2bruW81NnyLaayBpoSYGOlIgn+sSMJIKmIYkxVCmFlzCxKiUUZZlUqhdJ6cAJIoRIQAgiAl7zlEdAIEIUJQIh4IZ4QLxU5/z+WRR9HtGW+PH3JaRWQGSC4AKIRFEICKtiJ0BRBb7bEnEdiVrhvVj/nuH3vnUvl75Q5PP/8Be8848/zYFP3MBXvv8yj3w3oNWos2koQu/sYHHin3nsqfW8/aVeurUxPnXuh78eW/+/Au3/RyGDruu4EbRtm+nJSZ549L/wtDKNObAbJmbVZHDvFuJ6kmoppLsjjWm26OiLU/dC/MhnoV3GapaRJZ037d1Nf34A3ZMJLJuyXMFrmFSdBoqr8pOvPMe/P/xz4jGDSlXD99TVdnkQguvjA49/8mGCRgk39GhUTNwoJAplFFVFkkCVAnRFJZ2OSGQlfuvdWdZsN4jnXeSYT1hz4akykeLz0PvX0NOfYq7oc/JCmYWqx/OvnkYkRBNbdMQF/u3nr9DdmaTWjGibkO7YhKoIDI/JyFqCkllkbrHA7i19REGLxx+7RCIZUKmauKFCggRN28Qu1/ECByWRpFlt0LLrFOdKiHJEq9nEbFrkeweQNAVJbCNHEb7sIes6LdPESaTZsH0njhCiZNMYCKwfHWGkr4u4LqLhIQU2SVVFC0NcXMQoQNIEzpXmkGUVRTHwXShWKyzXK9zceR0iIorngrO67ZISQwgdRGysdg27uYLv2biBh0yEqomUyy66rgIiOaXFTVfvQEamc3uO7puy5G7JMf7OcdyWg9s2CSOR0BRwGh5Co8LRHz2OksvTkU0Tz+UR9QRqqhNRT2O1TBQpYqVYItczSjw9hGU6IClIUohnrfYKFvwZkjkNq7PK2N51CKJAQlcxewLM5Sbxa2J4NzWJNB9J0whVDae7k9KFSzw3IfGJh7+O64Ie6wRdxzMEZpsXaTVqEBe4dHaS0uISftTCSOukk3FiSoQUExBlefXPSgNRgUhUWZvfTYSLLsb51fl/Rn6NzklyE15jnnzjHJaVpN60sRLrsCpFTMvj8izc+lub6e6PM3BtD75bZMebbiZtSBSXVujMaTz/qytYjsWOe/bSnTXpSPTSrJcJFYns5r0EeoKLZwtI8QxHn1uhI58m3pGhe3wAvXSc9+8f4u133kbgBxCBGk/g2g6u65HNdSJIEqJiEOlZRFlCUgRkeZXSKL7GcFFDYABMBQ5cBluR+P0BeFcvPPLRj9E48Qiab3H4ySdpKyn+/T/Pk8/30/AVXDfg3JkzTJZMjjVDRH21tGJ7oAYqqVwaNd6FFs9DXEWUJHJ93USSQ+i5CBiAjpEPSK4PyW7zSa4LkDsCQkAI4e7dWxkavJaZt92C5G2kYgu0agEEeT68dYC3rdNI9+RXs/fXmDhEkOiUyHZFyCoMrBcJDIOdnWWuvPNf+GzhR3QfaHJgY5bR3kfI62lSPS4Hjwa8cs5maeoV3v0XAQfmLmJf+Dnnzr3C5x749P8daP3/P4RAxjRt3GaCZsWmIy+ya/AmWlNJRoYHWZmcod0sItYF0h0Z9CiiKfuM7tzKxz/+Id76jj/hxo/eyLMP/xLDd7CaJl6pSXZwhPLkEkIg0BN1UKytMJhN8eB9f8Dg4DBSV4KolqPbd0mkNUQUAEShzcWTM6zbsx4zsHnlm//FxrtvxGzYqB0Gohwiy6tqSTEI0KWAIFyd8iQS8NyjTfbem2RXOYa4IWImkgmELgzmWNcn0LQT9PUE3HBVN2GihFjuYHm5wYGXKoiRQp/UhWdExDpqXLNpnGLtCqrQyfXXxnm+CrnObiSrSJcQY+piE90Q8W2Psb4MdbuEIPo41YBkTwwx7qM7UCwUWJmdJ57JIIUqDbeBJ4rI3up4Mc/30LWIRKKTer2B5/pkEjHKjQqKHkOMGXj1Jj1dQ7ihQ3l+nloIqqghegKhoWDZbfzAx3LrKKKErBpIqkhGSrE8OcMdfh9Pi/PImdV9D70mkqKBDGa9iiTEcO06CnUiuYukEfHqtE1nzkUQA2Yur9DV00l9ziLWaSEIKgHKaxlsgB/EiRkZQsVm9tEn8HIjaPEcXmCRzXbh+m1cX0LREgSCiJGSqVcusW5sM45do9VaIRIEbEHBaVYRAw2AsZ5BaIFf72b+8ot0bttM9dlZ1t40wMzPClQXWvTe2ENw0EKQFURBBaHCcP92dqtLLK68QrE4y4CyHqFdJ67rhL6H4y8jazqKIuEL4aq7ZtSBq7g4pkm2V6GhheiigOALzF+pguohZjpwQwckqDabpOTVnoZXu4xTqdGqO7xw6iJb1/dguBUcUeals03uv2+cWO8+GLqZriuHscvzKNYk/WMa+X4Du+kxON6D6UbkkxZG5z5a53/G7OVfIScklhZKGNo0oxvGsJo1urvSBMYOEnqCVmhy454Y//aD77P96hu4dd9uVtoOE5fmaZsWe7ZtZni4ByF5BSUAWg0iJU9MK9LER3Yh8gVkMcIPBQQx4tSSwFFT5VM7Xdo1qLmrlgYnv/Fttjy4l+09AR//+hlScYM3v/UP+OLnPovVtHHdOr0JaFoimhoiJ8APQkqVkObiAj19LYR0gJLoQktuAaETLRODoAsXH1lNEdkSkQtKJiDdDX6fT7soUJ5S+NBtAiePrnDo0qe5ru99HJi5yOWzEsuXl/lUISBsSFw7UOdQ6KFi4OHQ3SfTk40wI4m9t6/jI++5m5K5grXmB9Su9PKuy+N87OBP2T9RofvR65g2i3i+gpIM8G2J//gPH2Vw9bm8Vwj4vFWl/vJnfy22/kYAvNsKmFmYR0t20nBsJmdmkO1RFporuGMeJFTmZo/TEQ2jKyqLF16l6C8i2CZ33rafp5/4F84ffpJz5XluGdrL4BuvxX15nr5dQxx/4hnWxgZwlRZB2OQDH/0HjFyGfE8GAp+Fi1e4696HkNoxhESIoETMzDV5+H1/xd1X7aNlNSBpUKyVycQM4oZKKKrYfoQiCEiajBSq+FGIGEV4nsCNb+jm4it1gi0+YlvAnzfJHX+R1tYOJhdBN1xu7APXTXPwkQL3Xu/y6OE6O7au4dJsgKS2Kfky+a5dzC0cZOO6u7gy9Tzzcx7b+65H8WuguXiCg12R2Dq2kUbNwE4LyKFPqjuLW7dpWi6h6VNcWqFcWUJWZMx6C9P1SOc60CWNVtgG38HIxnH8Nu2ay6Z112C2PAKvSVdXH6Hgc/7cOVRfwG4U0YwE+d5e0oFAR1yl3W6T7MoxszDPXHkFLbCxbItIcvAsiBk6A0PDPJhJcvj4Nwj7VwGpMnWGMIjI5lOIUYJEOk67UcSIdREhEIVtNqzZgCQUQOnA9VI4UcDQ2vU0T76KtmErdi0kVH1EBNqNFYovnUJNDyOPbUfzXGzbRpVF9KRKTI0ToVNcniMIAhQpJB7PUlk8jxQ6ZLIbiEKX6sxxFhZ16vU5UPsoLFbodCQWKtMkzk2iqSNU0wqHvvZzXve3bwJdxf92FXVLCu+yj2D5JPr6KZ84yfeeepzPvPkfaYcJIj/AUkRWyjVcwWIstg4lpRFEPtl0J3punEZ1itHcEFOFJvWah9e0cFFB9JAkg2QswXdPfoQ37P4LfnD473HrZaKBXgBWZk5RPzbNUmon9/z2VSwcPU3/9l6qiy3WDQh89X8e5k27ztB301b+4xuTvP6hHioXC2Q39hLLdBHLNrg6EVGrdJDoXoOuSywulPDTA/itAgPDgyiKjxUmkAwXs7qMWTyHl9yO1Q5omQ7PFFXWDK0lMXGEd75+BxXpPnTvMlb1Eq9OR4hqEzHI0VZlerc+iNcu4J38HpZfRzIi8AVEGQgEDp5VMJ2IZBeoKuQlkBTo6IfyxFFG45DWk9TqDf7pf/trUDziCuiKRDkSuWebRzxt8PlvSvzNhyT8RICoGGipfgTPxi2+RKSWSHTfjKzECQIHJA1JdLGqAR/4W4XxTRJeIJDjdbjNS4SBw4uHdEb2j3M6Os3Ty19hdjbkz6QNfFee5WjG5/rRJNPVBt+85yq6G/OcHd5HcaTJW992H23rEKF7jJXat/nHHxX56AMjdF37u3xw4pN85O9uRGquZ+6FA4x3GrwcWfSKYC4H1DIyv9MVMJ2RuevtV7PjRJuJ5f3Al/9bbP2NMBv70j//wyfHO1Xkdgq73WJk/XaMgkGtsIg/V6JrqJuYlcZvWVQLRba8+2bCLMw8f5Ydb7mf8auvYu7kKQxV4f7//YOMjfQhDGS48vXnKCcUxMIyI7u2cedf/REnfvJzfvmjr3Hh5KvIsRTZtQMERMxdnMGrNDC9Gn/6lneQjw+QFEUCQcBrmHiORXH0FVRDJpIkIkFAfI3gGooCiqSAoGA6Ib4Z0ZHLcvD7S8QzAn2lJqWRTrovFDjXELj12g389KlFTp0rIMU0XjhXBTGiVG7iU+T4VI10Z4Irk6dQBZ3IjHHm2HkWZ5sI+QbpMMGCv4gggCKp9It9BLaPqdYIvYgtla00qza+FVJ3Vq2AXTOi3irTO9KPEIn09A3RjhpYbZNQkYhkgciXMLQY6XwfizMTlJo1BkbHKZULVMpLrFRWkFMxfCRcUQDJxrQtjEyOrv5B4jGVAIuWaRJpAl4QYQkRjm1jej4DyW5WSstcocL+7bvZMdJJIpOm0WqSyHRB1CQKmoSuRxjaWI6MZKTJdnSTkLPEdZ2VpSbff/xxnjp+jOf+7Vm+8/DPeORffs6Bnz7Ozm034PkSgRjhN2osL82CH5DtTJDMdOJH4LZbKLKMLguUSnVSOgiCgChKhIHP8tIkqdxGYslxjh17FSnTxXW999GxNsWxmSlWkjkmVmaZuTLFV1/+Mm/f+rsoa2MIuxSYsJGu1/DjbbznJmjILTb3bKbdkjAMjWqoMDn/Mo4Q4ocCK+ESDWeJQBLQE3lEPY0RBdQbDaKgjayCKMlIqkKrUUeWVezAIggi5oqXMaMVCEUSnWk2dg8hXHyJGUtmZN8etGQe2T6BHpNQdIVSbB/X3HUNa/Z00N23lg3bDVwLZlY8lFgn+axGlBomkx1DTdt4chwl0jBGN3Lm4NP0jIwg6kmk0MQjR9gqsLhUY3jtGE4Q4Tp1fvbMCn982yA5PSRtOJxYsOgd2EAqbPFKsYNdQykGekTiW+4j3ruVQNIIRRElPoCWjjN1oYURtwgFOHUJLiyoxHR4/dUBShKM1GrZStUhklf7U15RY4ok9UaTKABNBM+NOPT5LvZsayGpPrfud1HVXn78K4WrdnUiSl3IqfUoqQ1ouR0oYgdOex6vUUTJDBBGMuWbn8DoDDk94xNqOhVrimrUZCFsEA2XEQczLFen0NSQuUUB8d/qhG4PfYLElek67rhH+cg802adN33uYTatFcnZfVy99zMUrDpzbYeH7t5BV9cW5l8+gCq02TAaJ68f57Z7vsLI0DC2+jz6UopFx8HqF3njrVmufdNWhkagOQDuySQ/fO70b77ZmGFkyF/azNiGUVbUCiEuVsxi9F27MefKIArIcoxgsUJg+yg+xM4G3Pjx9yC1XdxCkd7+Ydbdfw+5ZBZftFDzSbLvvZrq3x5k3yfeRfFcjfJMkZve8Vb8n2hcPPIcf/a5P+bDb3kdGUNH1HtRrt7Go+/8Il1aEnoCZutFBkURp9rGDk3MYQmdVY5sFPiAgOVIrLpbQ4gHoYKsirRKDYQITh21GRoKUNYIFLZ08oG1ZZ6fu8KWtRqvu6eHgc4KT7+6ng+8xeWm354klsywMemSUS0e/MOPcu89v89Db9+EY4es33A9kTyB55pIkYqsRbhODFlN0jUus2BewIpMnjv8EiIe2Y4OzFYL02xTaZVxcTg/cZG1w5swdBXPcjEDEwkZbJFETiOWTFMvzyMoKhFNXnjqAAEi8aREQtRo1Swi2ihqjERXHMdv4VeXSWRiWG2bgc4xNq/ZQbFSpmU1MBt13MijadU5dOl59naOcWDpCgBNyyGeW0NtqYCkV5ClJqHdQNUNFKkDkQhbszhy9hLdnXmaZhO/XWXb6DhfPfAU6zPd2GKAi8Bod5K/+/g/sn1LnqE7hpk/V+Z3Xv97qLKJnkgS4SLh0mq1qFVbWI0alycvksx2sXG4n0AUSCU8MrE8gduiVV1h67p+LjhQWVuiefYSQejheiGOqFGxr9DQGrTdGBlfwL7kwUoTPQzouitH1017KX2gSmdqlMuLE4Rth6OFU8hhSDzZTUzK89jiNxnflSKYdIgle0BRCGJdyOEsfijieW00PU0kCVB7zcoWCSSZdjiPIEmEskezWABgx3s+Q2PieYqXX2Z6pZNkWSQ7lEcwHfTiCwgb7sYKQxJijFhuHY54gcYzLjv3eHiJfrK57TitS4hhF/XpFcLMAm5hmWxOxfMDaBSIYiO0C2eZXayQjoHR0YtYPYGidnLH7kF8KVhVU3sRneVZMv4J3KiLNX0VSGSIj92NGDiIiowkS1heHEnNEFoy23Z3YFZrfOYbMOnI9OUFHtjvE88DHqiRgCatvmtOKcJH4M6ddf7rFwoj+SxSs041CLh5SEQWixjGal9NiiURRJ8HX5dmYRaa9Qbbrx8iCg0EoZ9QF9Az6/DDCBqXcTyNgZyEJMZY12dhei6VtktXUqVTSeJ7FueqTxL4EqoWsG2jzoF7bDZOzPGr+ZBAlkgWYN8f7uaDf3gzr3vwXkqXwPNE9m8bZvvOBd755lfJqN1MnH4fP77i8Ld7Huap8x+hd98dTE08TYd2LbcOb+bMUIy+3m52rO3D5fvMFGy2qa/n6r4P8p/nt/xabP2NyOD/4X985pPv/b23k+pIMdjRy9M/f5Jya4X+jeN4vomeSJDO5PDbLpLvYWgxkprB+J4+Xv3xARQ/wJZD8qk4xcISQSAxd/pVrILFur07GN6+gUa1jouFFAYkUxn2vHMDv/O2NyELEmdevcQzj5/m1BMvYqs2qpgi7mnEHQlfFBFyAs1ShaWBEpYb4noBoRfRdjyIRCRRRRRFkGQUSUUF0nGN3bfqnHnJ54ogkCvUUUZ0qhfbjOZDBscjfvAjhx1b8tx/fY2P/LXP7lsyvOePPs3s9Cvku7s4dOgJjrz8rxiqz9Ujb6GaPILsqiREGUsR8X0P2UsTCw3KxgpuuIKoR3Akg5Zps7BYwVRNiqVFhHD1r0NJprlSmEAWY1ihi992UHWVTLYLXUnQ1zdCc3llVQvgxcioMZbnF2iX6jTNFpZlEbgufmhSr1ZwWjb4EaqqIsoiiqET+qBKAgQhYRTQ3d3H6NBaNE0kdAS2d40hr0mToE5h6RJnXj5BvVCkI5lDElPYlkCj3EYUZMq+T6k0w4lzi/T3D/Li6Ul8VeGBe65ncuoksuwTyC6Vos/mfdcTekW8oEVkevzoSwe5MDdNrTrNSG+ez/7jNym2z1FuLnF2Ypqrt46hqwF1L6LSkDn64hnmVyosrLRpNEz0pErJUXkp8ThveOMf0VHQGRzqZebSeb54/nPYvs33n/oGb73rITRD5ZkfPMr56TMMnc+ydLCGna0QVkOcXI5i6yLW8mnONhYY6Ojnp9NfR0hIVLWIDiWPmu7DalVQ4nEsv0AQmhhSAllNUCpMQyggiyKiqOBHNmEQoWkagizitRx2rdvCmvQUvpLGKl+gJ+fxT/9RonfsJpTRBzh5xkdZOkks3YUfLmMW51k4Ok3Qt5WMXEDtyOM0F2lX5qnPV2hUChz95TLOYonHf1kirVVQFY92bY522yY3YrBpfYpk//XQXtUquCtLSKk0oQ9yaFNq2Uj9PaS4iB3pNOrLbFu/iSAMCREJfB9B0BEjkUiM8BrLfP2fC5ypRtw/GvDOB3xyuRA5IaAYEbIuoKhAIK7y6KWIkivylp0Rv3ze5ut/IvOO2wOu3SUi+xJqIo+qxdCUPpBTGMluEhmV7jVjSPIgit6PKMkEJEFNQOARyUlko4OicpC6YyLJOiklTtoQMJQ0ChZ72uOctAus77qGtr/InuYOpC1VasMCc2fBb0b05xWOnZjnq59/mfqcTFJLYfseP/nYfm7Z/THi3ds4fXkfhz42wdX9D3H6K59m7M158rE9HH/ye6ycfR7l5TF6N+5lZGQN8yePM5a+k5mvHKTH2MMvD32AnpFu/v3A4n+bwf9GAPyX/+VLn3zwjnu4OHOZStoiMzyMXG2RaidoVaaon24RpQL80zU61+bp2dRPWLdYOjvJ5nu3cvRfD9JYnqbScLj2tutYevwM85dWGBpZx8mjj9I1so6ufJ7yqSlcwUZwTJJDORKdPch5jZVilcnDL5LuW8Oaq/ez7tr9JAa7CRoWdhjhLleom3UKY1WiMMQXRDxfRgxBETUEEUJZQBIkfEEitDp45XsXISewVGmz7y6FySMOXTtFnOUUY7qEIPtImW6+/0SFhOPy9LmQgd4EE2eewW45aJ7Cvgd+i3L9AmOdN7CkPotri3gl6L+zgC9KlCvg1EK6jA4uuqcRZJm+7DhXHl/ipv09HDtdoGKtIEoJvCBAlxJYvosZVqg0yjTbTXLJFG3HRw4DJFEnl0nQdizynZ34goWYUHFwaLfrBEIIYYCmyfiuh+T4BM6qWtZvR4SRD0FEd18PLdNkzegQTiTg2A3OXjjH4soyeswgv34cMxtwzfpBUulO7HaDXP8wRA1su4ErymQ6U8STMkJqhFZznru2jpIUAjYN5Pj6Nx7n2rse5MzlC2y7eTOzV5q0Kw3a9gS66pLqzJHpyVNdXqHVNL7bkAIAACAASURBVBGUgO9892kUu0S6O83G6/Yy+cxRvGyAZqZYWVyhUSsgSBDXk4SBw/bNm+nv6eX8UgNtsESrJjB3pspC6jyXBh9hy/YUJDzeuO132Xnd1bgnqrQWS5ihRdf2Hs5UjuObFr+cP8S3z3+FE+0zTPhLOKrFJfs8mUERxxIhEZFKJrFdAd9socWyWM48iiwR+SKKomAHdcJAIBbL4Acesvoa7U4IV7nassiOkY1kGldI5gboHOhn6XyV6Wgtr56fpuH5+IbKgRcmyfRs5NiRs5x59gxHzzTYujtG5FjEOtKEokIk6NimjeUn2bxniN7r9pNOOmxYm+GFo8vMzDfI9OYYHkqjBS0EKUaAh1+5xMunHAK7Dk4LK4hRcR3iisqk34/oeQS6zrY16/HDAMmPCCwHAQ8xskGM4UcWG4fmuGevz5otq8NAxJSAJIOkCCSMBGbbJT26kdzAFv78s7P80TvWYLcrPLAbfvqUyPqBAIIIUdCQjSxyPIcgJ9CNJKKRQ9SHUI1eRDGLICn4QgeIIIs6gmAQCjKyEeNrJ/8nu/q78MI2CUVG8xOs1AokEgaFuE/BrGIoCjIW3zo0w6bMGNtb17NGGqSv12ei2uTvt23iox/4JHfv3cu+4UEevGEvT2R/yJZr/oYLP70F63tQdwzaj/ycPzw3SU79BXOVR2j/ZAC5mcbpNTh/9CVOHT+D7DWwHyggNz0qC+fRK23aXXUee7L9m1+isVttjj12CnVLRHCpTWhCBwZBzqJ80mFtLocwF5C6bw19u/NUHp8iPpqhdn6JWEYj06MRK+QJcyOc/MFFOlNJOs1ldt91PZeefhmzXSewGyTHsjTPTpPeMoBZ0LCiZcywSKE0zVO5CreFy1y35214tgOVMouLVwgNn01Xb6d46QLnXR9VlJEFCTyfSJAIZBdClW5tI1Ozr5LuSoFSZ92N3QQJh2vuECAMkJBA9Diz6JBPX8tG4xhyzqZYH2NpcZqOXAZoAyID6zrYtOltPP7kD7gqOcoV91mUjMLcSovNV8VJZQZxqDJ1GhRBJ5XXUSoJkoaBpod87r9u5MypEsFRl/PFJZL+MknSDIVpMtkMlcoSYmgTIbDcqBOJIbIdMbpuG7Yf4UcxzMjG0FO0mmWSyQ5KlTqy6CD6KpZtI+MTijqS6IEtUBHLuDQIooiOWgWJCDMKSSZiBJZNd28P7pxPMplg7thJjPEN2KZLo1XC8kNK05PceN02YrEYrXoTz7ap+QqJvEHoRCzNLCIpMnMrZT736Xfx2M9+wZvfsJGDRwWKTQ051YsYd7k0UWPPm0dwmi5b7hrHqVio8Qw3D2dZKswQ6Q5nD/2S6qzDX37h7Tz2L1+g1cqDBKISIwpturu7cTyXyuKqorG3a5ArzV9xruciXWEHkS+xWCpTc23e9eE/JfAD7v2b2zn0J0/j9cHlY2fpz3Zx6KXnOFl6kSDhEQky8XQWp2kjKRatRgJFDXA8i31blzjwgkOicx2iXCEmGwSOiWQo7Nlnc/BwQOQqBLKCqKtomoYiengCCKJHq7I6u61r4w609Dgx1caWT5KUTPq3b2D2ygIDa9ay9/bX8eShn9JenufDH7sdEncg11/gmW8/zs36Mom+XpYvTKOKAqnxTQxue5By4Qhbr1uPLrW5f+02/OrzYHRRniqiDXYjO4u4ZpPmQpuuTWkGRnr42TcO06pd4nSY56826nT0xLAbbaYKTSJFRQwi0ALESATPBCmJlkkh69eiyAHNyweRdYMwXkeRRWRFoG0aRHo3+b4sxtrbmHzxJT77d9uoLAbEM6M4lQbHpyu8kR46htdA0EZN9KLHNJR0N5GsIuIhxTsIm23ETD+BmEL0VcIAoqBNoBjIkopvmnR3CByenGdX36qOpBi1GehK4IVtwsAnkxTIxpYoN2W+9NAnqS78kLnHnsBb8tmVzLJUDHh/8yzCEx/gL0dc5hYlOhMR/Tdew6XFL3Lk5bUMXzlIbKfB5n/dwn99aZiF72S550MfodE/TeXIfzJXX2b8zX2UZ8/jDYIrZhHf7KLYIMRdao3o/xpUX4tfC/CCIAwC3wF6WLWO+FoURQ8LgpAFfgiMAFeAB6MoqgqCIAAPA/ew6kb/jiiKjv13a4gmhDEXZ9KlWfTo6+mgNldjVM8xbo9Ql5rIg0miXyyTXJdEvGUdraVZvO2bOHK4gPHAHeTmKqgbBnGmZrBO1BkYHEVQQtZcuxM1HaN29CJDN1xDPN9Bar2BHEClGbBUXqFQKoPoM6HWmJ69hHVumm17bmX0wxt55PP/xMa772Nf8WaefOkPkHQJlQAphEAIsAIBKQxZaFxGf7FG6t4ein4LdURBenIe87Ycfuiw7ncMlo9YuI7Es2eP8+E/TfDZzzZozlb5aSvimt0B1+z/HaanjtOd62Hfvpvx3Ame/MVzbLzFobFs0DdikB70qLZ8PF8l9ATWpropKpdJRX3EhQReZPHTHzyKE+6kUKzSS46EqKGKCQQ1IBI1FCQERSIMAnxaSBiEoUNvXx9tz8LwTCTJwGo3EWWZeEwl1CwiT8NWfSLJx/RCdNlDCHwkdbXZ3LZ8Ym0Xy7SxLR9Fr9KObCr1Epbn0JvvxG+D31zlbQcoXLw4SdIQ2DTWT7nUoCI5xHQDs+UQUxzC2XPk1Dh6vIPQqbFpwzYiu869b1zPp//kG/SubXHHrQoXz7dIqkA2xplnzrP7tk3IvTH8tsiF52fpXiOydsdmThx+hqiqsmdvhjP/+AXue+jtfPlLT5BPZxAknURSxdB0VgoFSu0QiHHq4iv05Pup10r0GVkiWaI/O07KkAhL4MdE3rfrJpyKibK7n9mVMkEk4ngqtbiHFClIvoAkeMRiMTbfMMixw0WIAiTP4NFfBfjmFMOdoyyZJgIhKD4TRycZ6VtLRISgxAjcBoEgEkQiATJ+ZCKEAqK0KmeJxDqq5tJq2Az2SdR/9gz9m9aSTchs2TLGge99lZWpSTKBi9RcJGj9CHN5FieVZnLZIFOYxYs0Ai+i/ORxunp7oDmHOXMcJ7ue0LlItZrCnJ3k9LTMDfE8biKiu2ccRe+HQo3qbI1tuzfwmQMN+ga6IbsOvXYRJbORMWWZIFABb1XVqUsEeIROm8iTUfQUsZ4bEWM6rfmXUTwN12ui62N0bdpJ4DUIvRZaaLPhhrtwG0uk+kQEXyKkwdevFkkkMyjEEXSZIHJRQoNQkxGJED0BoSlAdgwplPE8B0mPEdkBgSMRKas0nci2yYmwZVhipV2juFxjTdcYllXCkxRcsuSVOhk0dkZjHA4+T5AxyK2JI3tLfPe8w9/9yQYe/uEFbrlO5dxmuH/b9Vz5MayveURHvshaU+CIo1IYaPHop2ZY/wGJ/E3w+Gc/h9SrcPV7Pkzt2w/Tb28gd5XA1549jfVUkfV3SGweT5O0xxAy/++YjfnAh6MoOiYIQhJ4VRCEJ4F3AAejKPp7QRA+BnwM+ChwN7D2tWMvqzyevf/tCkJE/q5xar9aQM3pqA2fgZFxuGIjFmskNvQSnnDRbh3Ai8sQWJybWKZ/y1pULaRjpJv5cplOL8JSZaLAomssg2/alJsnGDc20nf/LcT0NHMTJyjVAlKKRiwdZ/7QFRpWi7SeZbY8y08OfZcBK4VfqnPbQ7/PO//yozSSIWpFwvFcJCFAVQwQQZBlxCBCkAICv0l0fQ6zUGTgzDLmehWpx4CEghO6WIUItQl7tybpSK3hL/7sNO//oMQP/1efDcNZTl6wSSQPEu8MsOwcJ199ghefnmLznXXcWgbVl3EyLrGwi5q1QFAZYd+6DFXhIhXmGOi/gXg6RlF4kW99qwU8h4RETE/SMi3uv20f5nKNhUsFihSJeQkEJGLIRKKE62lMXZggkTSQAh/ftwkIEEQFs+awbWQbE4tTuE2LwBVIieC5DqKsIYoigioRyyTxBI/SUgFZFZmcKJLO5bkwdRFJ1MjEYwwPr6FYXQLgyAun2bihi9379mA6MqHfxnR9pEhgYKiHxZrNQukKRstDSvfQbkVgVch19FBurrD52jWoGvgvX+At12zg8ZcnadRc0hvLTJ05T9/oEJqQZftta1m8dJlW1WTzVddy/FtPcfVN6xncv5ml5RPohobt+eiSh+PonJ6foG37jI5fRbFVo79jlPOFE2S0HhRPIJOJc8UtslAqcf37R9i+dw2T5xrcOeDDdBNV0DlfW6IjkyN0VbRIxrUc/FBHFD3OvFhF02KIuk7DbGCLPlQFjjz1CwhkAi9CFENEWePQwcvkx1OIvogjCQiRiiD5hPjIuoYauuiaAUDz+BGMrQ7N6cO46c1cv6mfyoVHSFYDLn/rANVjBbp12LFtkAvPnCW9fTfWfJ18ug9dDnBW5ujq6sYY1UiqEhcPvoQ1O0FsdAdrNg9TL8LEU6fZ9/aPMSSYHHv829xw304kKY4eltDVOU6eaPPV41Vu3zbKgpfiy49d4PffvAHbadJuupgrF9GTaXwvRAYkUQA9Ba0ySiwLkooS7SC+cTOiVCIMV5D1TUTJDG7zCl5zmZXjj5Db/b8Qy6wndGUiyUCVFYh8HNtFUhXEIInsVgklF03tIAgsJDUASUAIdUhKiF4NHBMh1JESMYIowgsE0DJ8+u9UxKSLokrceo3EmhumUeUBurNvYmrxUVLxNHXb43xqlsFaFqG5xMU744S3xXlAVPjS31xAaMiE81vwXjyFui/isP8MzzwbkFIk/vqT9yM0t3Hr2o2c7HkGepcpt88xlLkPZyCBcuQJOvPwrcd/zNqOYe64J039DggjuFS28Z1pqi3n14L3r1WyRlG09H9m4FEUNYHzQD/weuDbr132beANr52/HvhOtBovAhlBEHr/+7uICJQAX7CIIaH0xogaNt5SE7Fo0f3bm4grLqqnUl5YpjW5wsCmDSCBazpYxxYR5h3KCxVSXTmy128jito4tQa33v57OKrDbGMeq9HEzRuU6y2qTROv7iBZMa678z187I//nv7sAFca87hSm8E3XMOFyWN4lRZi08FIpXHtANMLCAlAAVmKEDUVYdV2iyDwWJZdJq/OU+vuYs1YwHWtXhI4zL2aY9u1u0iGJne/3uMD/2M/3/uBx48/NQiOzeYd17Nmdx6ZiJgr8+KhX6B6Bdp1AVwFI0oRb2Q5fXGSjJ8nlk7gCD6xtIFnhgx0DBPLOuTiBoIo4oQQhAGCGTCSGGLy5WVqMyIpOhhQRxFECREVQRYJsPHFNpos43smpZUKKyvTKEqMwLExdAXXthH9EDWKUCQHAZm0mkCWVCQ5RBZU9NBAQ0TRJGrVMh2pFOcvncYIY6iSimALFGYWGEh3ApDp0rj2zvto1EvUi4tUC00E10WTAry2SRSXWSo0CVpNwvYKiufg1lssTJzg3MklZFlm/93bWRkY4dLxOg/khxhzDCJBZfbsIhMvn2F5+QLteoVMf4xItFF1WCsmmHpsmrOP1fnK+39FKtGBF7pEaCzMTqJpGqoSMjN1GIBEPIEaKTjUmZPLBJrKeFcvvUaGdVddhRu5BHqbFV3AKvgM5nqxvBKPit8kFpdIZZIYqoKSyKDHNWL6KkNoeFjDjmxkJyCUAkRRXQU9SSaMJBBDlEghCDU8OUIRdFQ9AtdBzwoYcQU/knCTq4rbzIZepi9ewk1u5PyR5zm+LDF7ucCubYMMjEjc+4Y+6OrlxJLJ8XPwyg8Ps9DMQ9BGUVWSPVtQN3ajpExawQBNVCbqBoM7xomkAWSjn43bBjHUIpom0dOTo1maxBdsxMxmkt3j9GVjvG20RF/7Zd51i8BHbnf5yb8fxL94hr6BIbx2hXZhAaexiNds44WvqceNFI4LXq0IQhzXdHFsHaQRQlEiansoYi9OeRmhqSCrEihdyIkBlHgngtGBYHSjpfrRYr0gCSi5AbTkRgIxjkKWQEoRiWlCzyK0HXzJIpJV5JRGIEtIehpV1cBxGNvpksiA3wx45DsSv3hxPS/MFBCcWbYMfoquksCYluLnL1T5+iuzPHzWRj9eY/klh8QzFjfvilG4z+eed+/i/2jvzKPkuuo7/7nvvqX2Xqr3brW6tUu2ZCxLlmxjCItXMAw7xCcsCSQwWchwMkBgTiYJk5kDIUwCQxZICBAMNottMGBsxZZ3W7ZsrdbW3VKr97WquvZ67913548qJcJYNvKR1S2f+pxTp9751X3V3/s7t3913+++97v+Wyvctu1hHh9TjPsGd82F+I3PPUo+aGK25DP3w/tI79hHb3IL3/3hdzi0637+Ysc+lq18M5uNRpKrN7Lz6y579ijufGyBux8rs2NXkYFR9aIB/qxy8EKIPuBSYBfQrrWehOqPgBCirdasGxg97bSxmm3yjCIMm7G9R2h2SrhTFo7VRiAVdmMzrq+Z/cl+/Gaf1at6ISoIImW8sMPhZ57Gm58m+8w8Wz/8JlKpaWbmF1CdMYozmtTtuwlribysiZQ3TdCYwW+MMvjQHjZcvQZhNTI3NktHV5qVr1rFFRvewO1PfgfZ3UTfhnWMhYaZevIxVm65FhUWlMoGwlIoX2GFbLR0CAuFZ4UxygrPV0if6r3zVoXdRpzWmcP85hvgQ59TFObnGJvu5+OfPEIRi5uv2YjoiNHsTLD7sZ+zeuPFrFj5Op66/0ckVncRy4RRWmNrcCzJzIQk0rKSkZyL0fME/ugqeoxlrGjPcbBwL4V8BqlMykDWsFkelciIz/7ZI3iNjayQYdyc5M2vuYEDj+2iramRgdkTxI0GQgEU03mUV6a1vZGUK/AKGZAar5gl1tCKhUfB83AicZSrCFthAu0TtZtJtrQyMTmMFYviaEk4HGHw2BC266EDidQBll8hVSjiuEkAcgsFju7bhWlYWNIGlaPBjFMsK2ayaezudpZLSUtTjERDmKzn4RoWpinZuKGFdUELfiApzykmfYeRJ0d4z/veycSxByhf0sHjB06gvJNkpmeYHMozP1nhdz+6kWu/9l4cI8BNOazIrkdVskSFRHt5WpKNTEzPEQ5JrGiUIrBQmsL3BD1NXeiGaTydhkor6XKObVu2ct9joyRaLd7//d/kkUce59j/OMymhs3ssm6l4DURcRpoa08ioxZTIya5+Rwr1zVgNvgY2sAxoRhohADDlgjPwMAHCZ4V4Ktp2hp68ewAaSRIZz3aY5K5XEBFVTBqqViZ6KDrYoNvf/l2bn+sxF9+agN3PuhTzk3TmGhiYHCQtS2NbH/tZaQmFQlxgEeO5Qi1xMhPp7GbNfnjDiU7QtgaZ/C4R3srFGcfx7SbUJkR9t+/G2mlQcYolhXp4TyFkaNEk30gIBQKeOMGhWVdwtzePYy0r+Wm13QT0jmeeXyMlZsyOHYLVjgMVgaDVqRlQ7mMsCRYEXAkwXwZUy+A7Cbw0xDKQaWN8lQOw4kx+cCttP/GhzEsG8OwELIJw6vgqSzKEhihOFQclCUIihVEKIEVihOoMsJKEBQ98OIEgULYYaQ2CZBQLiEiUSIx6IlCxoJV3b10tdnMlVz+9HM7KJ94iL/46t8xmruVyy46wMXhNrzGaaKE+E1vEw+oJ/DnFB8clvzR0//EQk7Rn7PZHrK4e95HGz7Fkzl2/MP/4dCyCrm9Gba891rmxSN87DOf4He+8BXeubWBI+MzNBYrPPKth0m22IzfmyHWB348QBs2+j920zkzv3aAF0LEgB8Bf6y1zlZT7c/f9Hlsv7IaIIT4XeB3AToaWrj5mvey+7ZdZPU0lmWT93J4A7O0bFtNWcSYK4/gjTzOXMKlIdmDmfO4aHMD+x8IaL1xPemWMtKMErnlIHOX2ER9h754P73bVkJcc1IbZA1IWJLGQhzDjjF+YoLU5Cz/fvtXuWr07SxMziC0YmhmkOmRMdTEAsOj07T1F0mlpgkqgA3lkCLkShzLIpBhotokcBSiEqANFy1dfFx0syTVHuIfvu4znx9jRb/kkpUXc9XmHtxgjt3HDnDPZyP805eu5H9+aT/BVI494z9i66orORbdT6HQgFWobvBhS0l+No9TySDXlkkoj+71a8mm8gyUF7AiLp5nkwhF+b1PrudjX9jNiVwFmROUACM1zx4kzSjSD9zCJqOfoJCgw+ykt6WbcqkAhSJGo4PKVWiQFlq7tFid2LFqbZ5t/RtxVRlwyLiTjE3kcTxFpZxlIQ9lFdDS2kJ6LoMTDeNYJiUk2jAIhEDEY5DNMcU8Dp00NoJUZRJ2EaRNEE8wljaQpk3Z01i5Ip1dyzCFSzaVxdWQr5RpCtvk5yp4OLh4vO0jV/DETwfwu7o4FJqn1GSwohzmnRuvJz8xyrr3b2R23wl+9q97MPwm7vxfD3LtJ7fzrT+7k8Ta1UStCn7ZxVUeWd/m5NgMUjq0tXvYUYjacfobewg8ixbVTT7n4RkVOmMdlIuTuPPzpNNQNhr5s098kR899H8pSAM3pbj0OomyQrg6hF1Q9LTF6NrYge94DA0fxlQSJ1wiZFcrD/q+wvAMfMvGNBVCSOJWCM90CVuSVC6FCAyKnkSXC2hfYNvVEhtNy69Dhzr5wEcL3PSuPPd8dScyZ5PdtJxsWrAQJHGAX9yxly1XriW2/mqu6ClzZM8owgpTISCXymBoQcGIcHwsxYGnT3Kju4lE8G2+/e/zvOu1LQxMrifRuZWdO7/A5h6PdddeBQ1Rju0bwU1XOOl2sP26LkotfZAq07nyLXzx7z6HSi5wTbxCtMOEzBwhW2IXMzi4KCOEkAbheCuFvI8ZFQgnCkIhG7twM4NQKBAxJDLWTnbwaWYf/QYt238P21oOQQkPhUDi17Y7DKzqMyqyoQm/4mPKGLpcwbQdDMfCFHEC7aOyZbyojen7iFACoSRTJ6rpGmlrNm+F4dQ0T+6A0ScUdmuOT37yI/zRH16HE+nmkcwR1hX6mLXKFKKjhPE5GeTQa+OsbzZYKBkMPlGi2Awf/mfJP/+e5KOXm/zD02kSbR5d1wuWffVJ7NWbuHP82zj5CpesXo7X+XoevPUpGlZ3sGrNxYzvvAc9lKfig7xWMjJ7DhZZa8HYohrcb9Fan6oyPy2E6KzN3juBmZp9DFh22uk9wMRzv1Nr/TXgawDrl63UDa5FOVfED0kqqwystl7svjaaLr+I+cEZtr7+Bh747Be46ad/iCBg6MdHaHl9jJ7faiD3oGRmapr0yXHWmI2oI3nskEXz+gihkM2MPYMIx8kPTqDK0NDTQr4AdksnV910DSM7M3z/wX+lP7eCpkgjW9qbGL37YcLpKBe9cTvK9zjxzCFUAEKA8CQiLLCETVhKLDNE4CmMcAXPDfB8hbDBKyoCU5KTLumCzYNHk/z4sWE+9ZEbKJXGUCGPQxPH+cYPdrH5il5WtW3g5HCFSpDHIY6rDfzAwJcaT1SI2Jp4W5Ji5SSlfCNDs0fpXt5EbjhPqGTRkJT4uFTKDXi+OG3/U82pZU0PyBpFjCBMvjhHc0uSmWwOqJCImvilPG7gsGnVJXhZhTYr5Ip5lKFRMkZ72xo6W1sJm4rUyjTDcycZHRnGdWDLtsuYmxonubyD2cwkbU3NVNwK4YgDvqawUKS/uxvhCCaArZf0EDI9NHHAJJPJ4frQmuyBcISSnsLPuZR9j0AJpGPjITg+Oc2abetQwqJQmief9tn2lg0UZ1PsfnCc1VdcwQ++s4e4dYiuSyM89rf3sm59A1s+dBFuQTAx7/C9b91B0NbP+MlRVEcznuujjATZYgErHEJrTSrn0wHYhTl6/H6OexOcGMtQCs2T8Kv19OdmXWKNzUhtcDg7yA8PfpGyaaGVwggpDj40wWXX2thmH+gAIRyaYo2M56fYt2cOlMSwBUpJNArtGQgUUkmcqI02BNMzFTZus5mfEwQqIB4Nk1ko4pc9LNPAEyUAQlYruan78UWUueEhomu6ee2yfjpaK0hDs/LiPnK+JOSUOXTfM0yeOMmJ4XmSvW2MS0mk6IAVIV30yadnWb+umZUdUwztGeLSqzv4+PvfhJ3oxFUGlaG7uahvE0fGHiO86whGYwvFcsCKVa3kck1MpD200YJpzHDH3d9l46VbmRgbYf3FLex/aje23UnRyxBr6yMvLWy7RLzvVVTKCzgEBGYZXQmjI51IPKzmraSnHkdFYgS2RDoOC0f3YRnfoPV1f05gVZ8RCJQBXgnpREGG8N08wgOsKEr5CDOK9kP4XgEzrkBZSCURbgC2hVIaocsYgSQ3H+CEBYf2DjI5qchMgtFawmkAP6T44ld+BhXB2/+ggbQ5R1kpFuYXqASCvuYVnEwNYBqCxohk1TbBwad87v6F4rJ3S56aVBghQf4pG/sGHxUt0dDcRdcHP4P8l8+wa80Rdj/4l1zdrDFFlHC8BaNSoZi0ccOKZLvDzGiJ55k7/xIvmoOv3RXzL8BhrfWXTvvoJ8AHascfAH58mv39osp2YOFUKudMmFLgJwLWb15DeHkTuTaPQqNL6JIO5kbnQQXIiKT/Y+9m4WiBUipDy9blHPzaASaGDjE8dS/igTHyj5wkW1zAKAqyBZ+G7g5CoRhx2c700ARyOKBJN9CxupfiyCyVmQzr+jbxkTd+BLTg7W94Db+z9k08PDLC5PQICdFNc0MIq91gPDuJ8gM8LfEFGIFV25LLQuMjLY1jSwxhIW2J5YbxArCJ4BkG6VKY3765FWmGOXRsmFt+NkFCeaxu60Od8Jk5Ps34TI5R/TAls0TIbUSrMn7ewC+C9gI6lzViRS2mjoQp5WxGUseYKmYIh2KE41FcFRCUFE/sfBQDVd0q5zkDwDOgMRBkjSwuHq6rIOriBRUms3OMzIwwnRpjMjtFNlLAE5JQYxNNDd00J1swbUHez1IybeLNPaxfsZFNW7bT3tFGYSELFU0pVaQt1o1jSXqW9+L5FbAUHasbcbSNyFYDkiU8Am2iij7SiJNsaaUpbFNMzULUJ5POoIREWEmUKlDOlZkZn+fhRw5SedhQOwAAD05JREFU8QJA8dCdB/CV5he37sNqCOObOQYfG+FVW5swEo3s2pli936D+37hc8s/zbB3WpPrtXlqdy/5SgUzEmc+lUfZEebnJrBjYSKxEAifrvYEAH/194f58bNPcufufdx51zGyFc1MpUhIhbFEDFvHGRyZROJgYFBxAzAkRkggI5KDe2ZINlYwwybCljx1ZIz25iRQfZ5A+QbVB6QlgQGBUa3H4hsS31MYgUDEbAwzIJwwyOZzEJgobSCQGFa1KNrC1F0oI4xKDZNecNhyeRfr+01u+85h/v4rj/Klz9+DWFBgxRk4PM9cXpMIGfT3OaxoLnP0yAG6Y4rDjw5hV1yOP3mI+x+1efsfXEPnqg0keuOYjo2M29xzUpGPhNn8jo/SdPlVxLp76V7WRMEPiPX2IPIKvTCB62rsxgSjKsoCDh5w8aU95LNDeIFLYWEad2EcjBiZk3uRXo5KMEohNY0ZXYOWEYqZkxSmpjEsEyvWitQCq2E5Da1tzB/bTTk7BHj4QRnLNjCdRpR00MrDsi1EPIo2FNoP0NIAy0CEYhBU/Ya0ag9fCSzto4XBzFyAyknKKTh+UFFOgTDBCCsCoSnMQm5cMH9c8q3/XuLIeIlyRVIKXAp+kYGFKebLJiMDPlMjFWZnoGtVgNMoyUgX89IiV39MccdDl/PXv/V3NDX3ElEDdKn9LLtkC7Edy3hHSyszhYD5rmHGn72NeIPHinUmXb0One1hpP9i0RuE1i/8CyCEeDXwMHCA2nwQ+AzVPPz3gV5gBHiX1jpV+0H4f8D1VG+T/JDWevcL/Y3NG9fpu772dWafGSM7rUnHJynJgK62Naxs7CJQPieMOdJTk0TMDM19rbg5xeT9z2ANO/RGuwgv62LnbT/EftelRNKChJGktydJ/Krl6KKLW6jwzF176Nm+Hr0iIJeaJTIPLVGH6LIEP9rxE/7tqR/y8a538e9ze+nSmmuS72RkdoABY4y0nOMhZz9hE5INNvF4gpZYgpBhIxyjukGGdtEulPwCSgR4rk9LpIPWRJIThz3ciiaZnOHn9+RrdWQU/+3mi5iSPvmFGM+W7idm2kR1N8nScoqTebIdaTBMkoUwiajJgnSwGqZJFVOMDrqUKi4r2trIeT5YLma0zA/+MUOAAfzqIsxyQ7AiCNNvriKGgyNNpFOthJnKLWAYYFoRVq/YgJHyaO/swnYcVKlIyLAIheM0NDUjhE3n+i4qxQohLdl3+Bnm0tNkSyl6OpchozGmZ2dINDcRuBUykymQHjIRIubE2L+lwNaWDOvXJCm7YYrlAMuMUCiWcUIWbizK8RMnmTwyQHfPahIxk3SqyJGhQ1z76iuwu1pIT+dpXRFjLlPCCRkM7kuz/KIIj/x8iP5VUULNYQJtkZsvUfYKDO0dIaxtiraHLyRJ5RG1bKQjyWcXMGybjvZG3EKZom9gGAGl5Hq+uesnKBRms2DtpUUe/1GR93z4MvxJRTRmMnA8x4njGZyYIjepUaq6mbTne0SjITxD0NwUYS6b5YrtF9EW6ufex35GpWKhi4qELcnoAkJLAt8nCCS4iliTQ9lTYGvibYqmhiQ6ZzCezoIrMaVEKR8roXjfurdx/foRzI61HP3FTg4d81h3aYT2ligiHKGSD7j9b+/Djmtu+O1rcBIxpsfyPHv/Q7ieS9vKlVz51jezcOIoQmQ5/OijhCyTjldfQ+bxHVhSY8ooc0XNmhvfjZ1wGbhnJxRmKdFAJJ5gZHCE8SmXmz/9W8wWNEHJ56GnjpLxGljev5HDB57hOx/tJpgaQGnFgadPEG5sRIgw4eYmTL8MRoZEz0qseD9O12rMIINhtVJJT2DbBkJqdHECtzCLmj2GZwi8yTGSb3gHRscNyFAb2gMr1ICwDDQSELWLWUngVXCMgEDaBL6HaVmUMkUsA7SAQHsYCP7rrhspzLtkJyE1pZgcgUrRQAQBqmwQZE1URRIoiQgCCDRv/QtJZ5ciU4bAhIglUYSZT6WYOKzwHRszpMllPeZOagpHJVfcHHDJrZJtq6qz+uWtzXRd9DoKR+7miw0FPnHl5fzVo09Q8AwCAlbHJB/84MeYLzzIl7+5n+Pf5mmt9ZYzxdYXTdForR/h+fPqAG94nvYa+P0X+95fwpBMPzKJaYeIBZA66jGUOknXm/sId0epjKfI7zmOPz5PptXH6VmFHxKocJw179lO6btDBNNzrGpfxsLWXvR4CmNAMjh0FFZl6bU7WL1sLYF6muGDh0mEEnQ3xnEck/n5BYqewdaLXs9Nm99DU1OUy0bHWHAL5O8dIGrF0Z7HiD2BDAX4CkpeQNxXuCpACR/HNzGVTyA0RlQRLZvM54q4RcVwfpbZ2VlSxyN0rexk79OKz35qA3fdF2NdMsehh4cpx9P0rdyMbRq4vsITaa58U4InvqJ4202bGD5cYnrPLJFQgArPMZcNyM5KCrM2ibhkIV8h6+dxwgo7qv4jHfOrCFoCSRttJIjj2AJpgVty8W2NwsUKLAxVIZ2awK94qIpJX0Mfhh/BtG0qlQITE1kiDY24e0qEEiFsXxOzHRLJZYzOCoJ8gMAlrMLEVICHg9HtYFrtDIwMUYn6gMWxgXFsI0RJlQmkBhZQWmIYgrDlMnVyitXrN5KanWbk5BR2KMRrL9+CNCxKJZdo3KFQsQgZRWxhE4kXcMKtLMyVKbYaPLzzBK975wYe+OEBrvvwVppaS0STJka6gLQCZg6nCDAouB4EiiAQLJQ1uaJL3DTI64CeZPUaKAjAz/p4jk33piJ3fO8wN756FdqI4BgFGpskqQmNtsCU1Q2WzUiMjss2sGXLJl6TLPEnn7+dpw8OoMuDBMrG8BWlbIG8Y+E0gtIKUwB+GBV2EYaBaRlo36U0L2hsyFDyI4SFTVlXsOw4UhS5+QNbULugODlKZqyA7N3IU1+7hW3bNpJL25THR3n4ruMUTY0OAn7x/Ye4/ua30dJmceVbX8f99x7h2JTPViaolAuY0QQrez5NJvVlnn3wcS694V1M7XwQ2djC5W9aQ2ogD6bAc0dIjYeJJlIsf8t/oXPVQe781iPgz2Magm/dtpvL37wFcyHg8JFDlCPNlCY9TNlOsTDMyrW9TI2eoICJn56kKd6HqHQSEMN0AkSQJmQkyc/PQ7SFcCRKxZtFOEmEX8FzmrAsgRfKMLnje+Tnv0fy8quQ3a/FjC8jFE9iBD4yFEdaEQwrCkLglSvVzUX8Ml7BQiuXYjaNlYiD1vjK5bK1rXiBR6FcIu9qMhmX2RmXkT0243sgVzLRWkMQkGy2KfkVyidMkl2SGa9EuajxEjZQIByziHYpUidcTFPS0WZQKUDEDNi29xr+98D93Ggr3BUa/8o8MvsTQh/sp/mnh8lnFnjbcng8FDA7De9+09WsbG/jxKEjFIovHlpfdAZ/PhBC5ICji63jJdACzC22iJfIhaq9rvv8cqHqhgtX+9noXq61bj3Th0uiVAFw9IUuM5YqQojdF6JuuHC113WfXy5U3XDhaj+XupfGln116tSpU+ecUw/wderUqfMKZakE+DOWu1ziXKi64cLVXtd9frlQdcOFq/2c6V4Si6x16tSpU+fcs1Rm8HXq1KlT5xyz6AFeCHG9EOKoEGKwVnZ4ySCEWCaE2CmEOCyEeFYI8fGa/c+FEONCiL21142nnfOntb4cFUJct4jah4UQB2r6dtdszUKIHUKIgdp7U80uhBBfruneL4TYvEia157m071CiKwQ4o+Xqr+FEN8QQswIIQ6eZjtrHwshPlBrPyCE+MDz/a3zoPuvhRBHatruEEI01ux9QojSab7/x9POuaw2xgZrfTtjgaqXUfdZj43zHXPOoPu20zQPCyH21uzn1t9a60V7UX1AewhYAdjAPmDDYmp6jr5OYHPtOA4cAzYAfw78yfO031DrgwP01/omF0n7MNDyHNsXgE/Xjj8NfL52fCNwN9UH2rYDu5aA7yUwBSxfqv4GXgNsBg6+VB8DzcDx2ntT7bhpEXRfC5i148+fprvv9HbP+Z4ngStqfbobuGERdJ/V2FiMmPN8up/z+d8Af/Zy+HuxZ/CXA4Na6+Naaxe4lWo9+SWBPnMt/DPxVuBWrXVFa30CGKTax6XCuavh//LzBmBIa/1C29Ysqr+11g8BqefRdDY+vg7YobVOaa3TwA6qZT7Oq26t9b1a61PVTZ6gWiTwjNS0J7TWj+tq9Pk2/9nXl4Uz+PtMnGlsnPeY80K6a7PwdwPfe6HveKn+XuwAf6ba8UsO8cu18AH+oHY5+41Tl+Esrf5o4F4hxNOiWpoZnlPDH3ixGv6LyXv55UG/1P19irP18VLsw29TnSGeol8IsUcI8aAQ4uqarZuq1lMspu6zGRtLzd9XA9Na64HTbOfM34sd4H+t2vGLjXhOLXyq2xCuBF5FdSOTvznV9HlOX6z+XKW13kx1C8XfF0K85gXaLiXdCCFs4C3AD2qmC8HfL8aZtC6pPgghPkt1m85baqZJoFdrfSnwCeC7QogES0f32Y6NpaL7FO/jlycy59Tfix3gf63a8YuJeJ5a+Frraa210loHwNf5z7TAkumP1nqi9j4D3EFV4/Sp1It4CTX8zyM3AM9orafhwvD3aZytj5dMH2oLvG8Gbq6lAailOOZrx09TzV+voar79DTOouh+CWNjKfnbBN4O3HbKdq79vdgB/ilgtRCivzZrey/VevJLglp+7Fdq4T8nP/024NTq+E+A9wohHCFEP9WNx588X3pP0xcV1Q3SEUJEqS6gHeQc1vB/mfmlWc1S9/dzOFsf3wNcK4RoqqUXrq3ZzitCiOuBTwFv0VoXT7O3CiFk7XgFVR8fr2nPCSG21/5P3s9/9vV86j7bsbGUYs4bgSNa6/9IvZxzf7+cq8e/5grzjVTvThkCPrvYep6j7dVUL4P2A3trrxuBf6NaH38/1cHRedo5n6315Sgv810FL6B7BdW7A/YBz57yK5AE7gMGau/NNbsAvlrTfQDYsog+jwDzQMNptiXpb6o/QpOAR3WG9TsvxcdUc96DtdeHFkn3INXc9Klx/o+1tu+ojaF9wDPATad9zxaqAXWI6h4QYhF0n/XYON8x5/l01+zfBD76nLbn1N/1J1nr1KlT5xXKYqdo6tSpU6fOy0Q9wNepU6fOK5R6gK9Tp06dVyj1AF+nTp06r1DqAb5OnTp1XqHUA3ydOnXqvEKpB/g6derUeYVSD/B16tSp8wrl/wP1YVWssY8HmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model\n",
    "------------------\n",
    "\n",
    "Now, let's write a general function to train a model. Here, we will\n",
    "illustrate:\n",
    "\n",
    "-  Scheduling the learning rate\n",
    "-  Saving the best model\n",
    "\n",
    "In the following, parameter ``scheduler`` is an LR scheduler object from\n",
    "``torch.optim.lr_scheduler``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Training Function , the scheduler is lr=0.001, decays by 0.1 every 7 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                #print(inputs.size())\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            print('{} Rajat Best_Acc: {:.4f} Epoch_Acc: {:.4f}'.format(\n",
    "                phase, best_acc, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                \n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            #collect losses\n",
    "            if phase=='train':\n",
    "                train_losses.append(epoch_loss)\n",
    "            if phase=='val':\n",
    "                val_losses.append(epoch_loss)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model,train_losses,val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the model predictions\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Generic function to display predictions for a few images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            print(\"modi\",inputs.size())\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def plot_losses(train_loss,val_loss):\n",
    "    df = pd.DataFrame(list(zip([i for i in range(0,len(train_losses))],train_loss,val_loss)), \n",
    "               columns =['epoch', 'train_loss','val_loss']) \n",
    "    list_data=[df.train_loss,df.val_loss]\n",
    "    plots=sns.lineplot(data=list_data)\n",
    "    return plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_output(model,train_loss,val_loss,experiment_name):\n",
    "    model_dump_path=experiment_name+'.pt'\n",
    "    torch.save(model.state_dict(),model_dump_path)\n",
    "    csv_dump_path=experiment_name+'.csv'\n",
    "    df = pd.DataFrame(list(zip(train_loss,val_loss)), \n",
    "           columns =[ 'train_loss','val_loss'])\n",
    "    df.to_csv(csv_dump_path)\n",
    "    list_data=[df.train_loss,df.val_loss]\n",
    "    plot_dump_path=experiment_name+'.png'\n",
    "    plots=sns.lineplot(data=list_data)\n",
    "    fig=plots.get_figure()\n",
    "    fig.savefig(plot_dump_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E1. Train whole model, LR scheduler, 100 epochs, resnet18 \n",
    "----------------------------------------------------------------------------\n",
    "Resnet18, train the whole model\n",
    "\n",
    "Best val Acc: 0.960784\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=False)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "It should take around 15-25 min on CPU. On GPU though, it takes less than a\n",
    "minute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.6623 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.5943\n",
      "val Loss: 0.6035 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.6956 Acc: 0.5615\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5615\n",
      "val Loss: 0.7599 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.7273 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.5916 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.6264 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7960 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.6533 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6177 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.6195 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5554 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.6211 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6437 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.5570 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6001 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.6070 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6158 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.5664 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6086 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.5357 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6005 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.5503 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5743 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.5491 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5942 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.5514 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6382 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.5357 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6231 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.5649 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6208 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.5472 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6363 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.5479 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6344 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.5424 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6134 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.5506 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5893 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.5520 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6042 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.5456 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6449 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.5683 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6372 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.5300 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6225 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.5473 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6345 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.5564 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6204 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.5167 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6492 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.5319 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6279 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.5376 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6395 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.5267 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6219 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.5400 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6191 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.5275 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6232 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.5450 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6228 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.5447 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6252 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.5374 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6065 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.5516 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6227 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.5341 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6237 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.5788 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6097 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.5713 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6063 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.5327 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6249 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.5386 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6153 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.5523 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5998 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.5243 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6039 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.5759 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6340 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.5352 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6098 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.5638 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6151 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.5284 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6311 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.5678 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6325 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.5185 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6170 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.5514 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6039 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.5729 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6484 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.5477 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6295 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.5492 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6465 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.5557 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6115 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.5320 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6259 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.5136 Acc: 0.7623\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7623\n",
      "val Loss: 0.6158 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.5733 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6234 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.5632 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6426 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.5361 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6009 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.5279 Acc: 0.7623\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7623\n",
      "val Loss: 0.6263 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.5485 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6372 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.5630 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6219 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.5744 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6158 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.5639 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6236 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.5551 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6244 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.5485 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5867 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.5410 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5970 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.5454 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6401 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.5527 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6205 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.5345 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6161 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.5563 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6152 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.5356 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5979 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.5662 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6290 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.5604 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6284 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.5368 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.5987 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.5316 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6274 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.5614 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6026 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.5266 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6187 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.5351 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6347 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.5388 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6466 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.5558 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6382 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.5502 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6406 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.5220 Acc: 0.7582\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7582\n",
      "val Loss: 0.6278 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.5199 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6261 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.5438 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6337 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.5874 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6223 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.5685 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5916 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.5780 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6247 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.5604 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6386 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.5519 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6254 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.5825 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6499 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.5396 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6333 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.5523 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6066 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 93/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5159 Acc: 0.7541\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7541\n",
      "val Loss: 0.6024 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.5665 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5967 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.5239 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6062 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.5645 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6233 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.5527 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6169 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.5393 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.5915 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.5561 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5924 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.5587 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5867 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.5836 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6085 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.5332 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6510 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.5286 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6256 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.5653 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6142 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.5352 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6176 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.5359 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6185 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.5546 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6255 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.5435 Acc: 0.7664\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7664\n",
      "val Loss: 0.6081 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.5409 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6315 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.5747 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6443 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.5332 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6396 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.5242 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6521 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.5390 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6158 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.5280 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6189 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.5280 Acc: 0.7541\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7541\n",
      "val Loss: 0.6120 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.5600 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6203 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.5665 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6137 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.4830 Acc: 0.7664\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7664\n",
      "val Loss: 0.6121 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.5580 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5874 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.5179 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6228 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.5556 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6269 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 122/499\n",
      "----------\n",
      "train Loss: 0.5307 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6303 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.5653 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6361 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.5226 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6524 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.5558 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5997 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.5297 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6188 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.5648 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6341 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.5606 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6150 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.5250 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6219 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.5529 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6461 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.5594 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6155 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.5587 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6287 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.5641 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5972 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.5531 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6423 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.5163 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6390 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.5455 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6180 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.5502 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6295 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.5470 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6303 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.5227 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6679 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.5670 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5952 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.5536 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6085 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.5682 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6065 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.5239 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6322 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.5606 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6151 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.5426 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6001 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.5367 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5904 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.5422 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6127 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.5258 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6539 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.5656 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6284 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.5059 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6295 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.5413 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6359 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.5239 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6246 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.5522 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6237 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.5608 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6139 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.5843 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6308 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.5520 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5992 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.5495 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6246 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.5719 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6173 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.5676 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6048 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.5383 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6374 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.5340 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6304 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.5502 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6229 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.5330 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6069 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.5577 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6342 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.5455 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6394 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.5284 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6299 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.5360 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6275 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.5552 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6235 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.5396 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6222 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.5318 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6341 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.5612 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6182 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.5601 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6288 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.5249 Acc: 0.7623\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7623\n",
      "val Loss: 0.6311 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.5263 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6011 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.5650 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6237 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.5697 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6177 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.5163 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6129 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.5128 Acc: 0.7541\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7541\n",
      "val Loss: 0.6137 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.5343 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6035 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.5509 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6072 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.5310 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6225 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.5279 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6321 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.5389 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6209 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.5627 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6060 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.5564 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6359 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 186/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5227 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6230 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.5709 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6215 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.5505 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6097 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.5772 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6187 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.5341 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6216 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.5638 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6356 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.5101 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6200 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.5425 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6023 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.5416 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6123 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.5277 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6139 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.5272 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6104 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.5571 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6218 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.5540 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6116 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.5441 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5892 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.5608 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6132 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.5588 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5997 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.5291 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6364 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.5451 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6095 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.5819 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6233 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.5311 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6186 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.5310 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6172 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.5533 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6376 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.5681 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6275 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.5263 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6047 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.5445 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6025 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.5644 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6297 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.5468 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6245 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.5585 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6576 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.5370 Acc: 0.7541\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7541\n",
      "val Loss: 0.6088 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.5389 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6456 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.5197 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6182 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.5586 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6054 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.5590 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6161 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.5467 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6127 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.5482 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6098 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.5304 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6042 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.5197 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6428 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.5352 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6056 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.5600 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6182 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.5230 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6157 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.5525 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6369 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.5560 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6445 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.5475 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6241 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.5462 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5979 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.5286 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6253 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.5568 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6222 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.5691 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6228 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.5815 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6326 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.5331 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6387 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.5437 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6228 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.5494 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6148 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.5406 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6247 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.5684 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6102 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.5398 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6031 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.5640 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6401 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.5406 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6079 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.5597 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6279 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.5431 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6309 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.5319 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6725 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.5331 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6122 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.5524 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6073 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.5620 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5965 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.5352 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6277 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.5648 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6136 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.5307 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6432 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.5623 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6291 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.5570 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6069 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.5113 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6270 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.5675 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6131 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.5345 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6158 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.5459 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5962 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.5792 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5984 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.5477 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6430 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.5393 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6138 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.5346 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6115 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.5652 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6444 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.5426 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6168 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.5432 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5951 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.5215 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6145 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.5751 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6108 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.5673 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6321 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.5313 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6040 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.5309 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6135 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.5700 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6220 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.5500 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6413 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.5773 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6469 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.5645 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6203 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.5550 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6011 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.5366 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6011 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.5253 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6084 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.5716 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6153 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.5879 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6018 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.5697 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6352 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 279/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5343 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6095 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.5294 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6483 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.5373 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6225 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.5611 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6501 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.5047 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6319 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.5267 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6436 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.5232 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6068 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.5599 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6494 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.5458 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5969 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.5287 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6247 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.5398 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6041 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.5691 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6561 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.6021 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6162 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.5614 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6205 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.5523 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6281 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.5273 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6144 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.5390 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5964 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.5784 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6365 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.5716 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6092 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.6025 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5833 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.5272 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6168 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.5662 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6113 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.5254 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6056 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.5138 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6058 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.5871 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6094 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.5669 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6068 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.5363 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6329 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.5274 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6458 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.5300 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6124 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.5463 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6230 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.5486 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6356 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.5710 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6375 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.4994 Acc: 0.7623\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7623\n",
      "val Loss: 0.6136 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.5485 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6469 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.5396 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5956 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.5718 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6323 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.5106 Acc: 0.7705\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7705\n",
      "val Loss: 0.6292 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.5727 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6384 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.5063 Acc: 0.7623\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7623\n",
      "val Loss: 0.5991 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.5363 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6296 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.5524 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6309 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.5359 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6151 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.5472 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6175 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.5343 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5939 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.5325 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6104 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.5466 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6319 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.5353 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6189 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.5184 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6436 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.5451 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6214 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.5101 Acc: 0.7623\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7623\n",
      "val Loss: 0.6081 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.5464 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6359 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.5551 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6042 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.5670 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6160 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.5614 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6342 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.5589 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6341 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.5211 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6428 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.5217 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6197 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.5284 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6006 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.5735 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6138 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.5624 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6214 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.5598 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6205 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.5595 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6119 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.5526 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6190 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.5582 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6145 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.5254 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6168 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.5614 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6211 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.5615 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6214 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.5310 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5962 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.5360 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6329 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 348/499\n",
      "----------\n",
      "train Loss: 0.5136 Acc: 0.7705\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7705\n",
      "val Loss: 0.6091 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.5500 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6318 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.5234 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6011 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.5515 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6091 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.5570 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6208 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.5470 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5997 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.5530 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6327 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.5467 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6267 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.5497 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6145 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.5714 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5967 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.5342 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6189 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.5426 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6352 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.5531 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6040 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.5507 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6297 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.5644 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6022 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.5594 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6254 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.5416 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6099 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.5142 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6212 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.5090 Acc: 0.7746\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7746\n",
      "val Loss: 0.5953 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.5364 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6229 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.5466 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6291 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.5561 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6355 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.5313 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6399 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.5487 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6099 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 372/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5506 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6131 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.5330 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6084 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.5514 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6290 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.5352 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6003 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.5327 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6268 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.5612 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6128 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.5472 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6353 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.5237 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6216 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 380/499\n",
      "----------\n",
      "train Loss: 0.5601 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6215 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.5677 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6391 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.5116 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6237 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.5424 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6278 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.5788 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6206 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.5350 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6189 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.5625 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6126 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.5527 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6000 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.5543 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6051 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.5523 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6239 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.5836 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6117 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.5279 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6127 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.5463 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6173 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.5380 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6117 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.5877 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6231 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.5452 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6092 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.5526 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6262 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.5667 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6303 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.5594 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5950 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.5304 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5959 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.5395 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6087 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.5069 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6118 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.5367 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6269 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.5616 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6067 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.5855 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6244 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 405/499\n",
      "----------\n",
      "train Loss: 0.5645 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5833 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.5458 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6333 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.5507 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6159 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.5449 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6124 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.5547 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6153 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.5303 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6261 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.5373 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6354 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.5510 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6361 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.5455 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6293 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.5195 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6174 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.5699 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6083 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.5569 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6249 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.5557 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6293 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.5399 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6303 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 419/499\n",
      "----------\n",
      "train Loss: 0.5274 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6311 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.5650 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5925 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.5388 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6118 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.5504 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6171 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.5395 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6091 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.5427 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5951 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.5555 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6414 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.5363 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6325 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.5511 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6424 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.5602 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6233 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.5554 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6094 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.5312 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6207 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.5815 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6238 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.5555 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6147 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.5733 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6293 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.5568 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5891 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.5228 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6428 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 436/499\n",
      "----------\n",
      "train Loss: 0.5766 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6208 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.5227 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6306 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.5462 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6407 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.5521 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6350 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.5702 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6112 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.5778 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6169 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.5740 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6171 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.5709 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5945 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.5707 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6317 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.5831 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6426 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.5334 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6159 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.5996 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6349 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.5528 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6160 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.5236 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6218 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.5131 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6038 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.5220 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6003 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.5383 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n",
      "val Loss: 0.5996 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.5345 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5942 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.5399 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5981 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.5270 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6256 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.5342 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6213 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.5483 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6137 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.5590 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6168 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.5454 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6427 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.5579 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6103 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.5451 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.5871 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 462/499\n",
      "----------\n",
      "train Loss: 0.5601 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6092 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.5465 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6396 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.5569 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.5979 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 465/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5384 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6347 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.5011 Acc: 0.7541\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7541\n",
      "val Loss: 0.6381 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.5469 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6392 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.5459 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6431 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.5593 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6145 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.5550 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6327 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.5623 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6319 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.5340 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6151 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.5702 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5971 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.5459 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6303 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.5485 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6355 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.5650 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6054 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.5412 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6289 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.5572 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6222 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.5804 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6188 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.5464 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6207 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.5400 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6274 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 482/499\n",
      "----------\n",
      "train Loss: 0.5481 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6224 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.5089 Acc: 0.7623\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7623\n",
      "val Loss: 0.6108 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.5501 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6058 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.5550 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6496 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.5888 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6187 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.5552 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6218 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.5590 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6113 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.5351 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6071 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.5441 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6403 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.5707 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6543 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 492/499\n",
      "----------\n",
      "train Loss: 0.5566 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6011 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.5253 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6224 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.5377 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6119 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.5297 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6290 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.5307 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5997 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.5402 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6073 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.5545 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6154 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.5439 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5959 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Training complete in 17m 20s\n",
      "Best val Acc: 0.705882\n"
     ]
    }
   ],
   "source": [
    "model_ft,train_losses,val_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dump_output(model_ft,train_losses[0:50],val_losses[0:50],'without-pretrained-resnet18_lrscheduler_wholenetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E2)Train lastlayer, LR scheduler, 100 epochs, resnet18 \n",
    "----------------------------------\n",
    "\n",
    "Resnet18, train the last layer only\n",
    "\n",
    "Best val Acc: 0.954248\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=False)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=2, bias=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "On CPU this will take about half the time compared to previous scenario.\n",
    "This is expected as gradients don't need to be computed for most of the\n",
    "network. However, forward does need to be computed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.5934 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bashturtle/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5887 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.6134 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.5873 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.5925 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5925 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.5995 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5852 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.5977 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5997 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.5919 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5919 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.6013 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5909 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.5821 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5899 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.6118 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6393\n",
      "val Loss: 0.5970 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.6023 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5948 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.5865 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5895 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.5992 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5921 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.6179 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5916 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.5900 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5867 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.5984 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5935 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.5944 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5899 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.5960 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5971 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.6188 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5866 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.6003 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5886 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.6118 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6434\n",
      "val Loss: 0.5856 Acc: 0.7190\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.7190\n",
      "\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.5796 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6038 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.6016 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5933 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.5898 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5921 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.6058 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5906 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.5946 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5964 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.5862 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5946 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.6044 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5864 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.5996 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5931 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.5866 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5937 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.5863 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5862 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.5722 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5973 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.5856 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5937 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.6136 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5898 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.5877 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5930 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.5988 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5895 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.6121 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.5854 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.5872 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5902 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.5845 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5884 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.5898 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5903 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.5858 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5972 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.6052 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5920 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.5912 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5937 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.5828 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5936 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.5821 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5935 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.6220 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.5900 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.5982 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5912 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.5782 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5914 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 47/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5948 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5904 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.5941 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5968 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.6090 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5907 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.5893 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5935 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.5996 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5988 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.6087 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5883 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.6069 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6020 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.5868 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5977 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.5802 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5910 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.6019 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5869 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.6269 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.5871 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.5958 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5936 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.5979 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5851 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.5832 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5938 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.5848 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5928 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.5956 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5892 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.6043 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5936 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.5936 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5939 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.5884 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5866 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.5873 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5891 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.6028 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5959 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.5871 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5918 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.5871 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7459\n",
      "val Loss: 0.5901 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.5821 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5995 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.5882 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5874 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.5898 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5900 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.5875 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5847 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.5891 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5890 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.5810 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5894 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.6023 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5875 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.6015 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5884 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.6046 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6311\n",
      "val Loss: 0.5925 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.5871 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5892 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.6041 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5845 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.5958 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5915 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.5913 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5982 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.5937 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7336\n",
      "val Loss: 0.5922 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.5849 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5859 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.6027 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5920 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.5744 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5885 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.6019 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5906 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.6030 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5907 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.5829 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5905 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.5994 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5915 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.6128 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6032 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.6030 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5893 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 93/499\n",
      "----------\n",
      "train Loss: 0.5847 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5999 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.6076 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5899 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.5957 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5965 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.5903 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5929 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.6036 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5900 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.6154 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5890 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.6087 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5913 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.6132 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5965 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.5953 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5926 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.5895 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5861 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.5751 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5896 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.6057 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5950 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.6002 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5860 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.5981 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5949 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.5835 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5906 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.5866 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5893 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.5860 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5944 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.6003 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5975 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.5789 Acc: 0.7582\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7582\n",
      "val Loss: 0.5957 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.6077 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5872 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.5735 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5903 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.5969 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5949 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.5951 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5933 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.6282 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.5907 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.5921 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5892 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.5922 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5950 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.5784 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5894 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.6028 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5967 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.5782 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5933 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 122/499\n",
      "----------\n",
      "train Loss: 0.5938 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5894 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.5994 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5955 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.5968 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5943 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.5939 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5940 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.6015 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5984 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.5974 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5914 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.5879 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5896 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.5984 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5877 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.6105 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5911 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.6054 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5909 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.6048 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5921 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.5989 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5887 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.5843 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5855 Acc: 0.7124\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7124\n",
      "\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.6113 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5930 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.5975 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5932 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.6122 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5966 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.6221 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.5944 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.5937 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5886 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 140/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5897 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7418\n",
      "val Loss: 0.5947 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.5792 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5946 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.5957 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6019 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.5867 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5864 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.6004 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5961 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.5864 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5930 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.5953 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5896 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.5908 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5915 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.6018 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6057 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.5939 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5943 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.5999 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5940 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.5847 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5855 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.6117 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5929 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.6073 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5884 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.5905 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5829 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.5931 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5897 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.6004 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5898 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.6049 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5911 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.6049 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6019 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.5882 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5925 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.5899 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5958 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.6016 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5963 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.5987 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5880 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.5978 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5965 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.5979 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5953 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.5949 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5972 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.6101 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5864 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.6064 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5862 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.5903 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5920 Acc: 0.7190\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7190\n",
      "\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.5768 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5913 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.5823 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5900 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.6013 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5872 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.5906 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5935 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.6124 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5931 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.5853 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5885 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.5830 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5893 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.5983 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5913 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.5858 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7336\n",
      "val Loss: 0.5992 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.5986 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5928 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.6023 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5850 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.5912 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5850 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.6130 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5894 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.5949 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5889 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.6029 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.5917 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.5988 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5951 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.6115 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6044 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 186/499\n",
      "----------\n",
      "train Loss: 0.5960 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5874 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.6079 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5992 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.5879 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5874 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.6051 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5930 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.5908 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5969 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.6174 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6005 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.6004 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5917 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5943 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.6042 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5926 Acc: 0.7124\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7124\n",
      "\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.5857 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5927 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.5879 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5945 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.5839 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5886 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.6031 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5962 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.5845 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5950 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.6028 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5951 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.6039 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5906 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.6087 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5946 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.5681 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5982 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.6196 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5867 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.6082 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5850 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.6027 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5925 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.5859 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5950 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.5966 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5985 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.5887 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5848 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.6074 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5895 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.6308 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6107\n",
      "val Loss: 0.5871 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.5808 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7336\n",
      "val Loss: 0.5924 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.5995 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5957 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.5796 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6043 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.6038 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5859 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.6006 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5902 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.5926 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5893 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.5819 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7377\n",
      "val Loss: 0.5895 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.6043 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5957 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.5780 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7500\n",
      "val Loss: 0.5940 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.5980 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5907 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.6118 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5874 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.5872 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5920 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.5954 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6001 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.5850 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6035 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.5864 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5916 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.6017 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5879 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.6086 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5957 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.5978 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5933 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.5955 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5912 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.5975 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5936 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.6108 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5911 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 233/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6068 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5838 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.5833 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5946 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.6230 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5915 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.5981 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5925 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.5979 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5905 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.6043 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5878 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.6024 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5969 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.5816 Acc: 0.7582\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7582\n",
      "val Loss: 0.5965 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.5914 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5903 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.5954 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5856 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.5855 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5958 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.5817 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5902 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.5961 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5911 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.5965 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6001 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.5966 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5823 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.5828 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5986 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.5937 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5900 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.5866 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6044 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.5897 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5905 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.5796 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5923 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.5859 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5875 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.6258 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.5955 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.6048 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5940 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.5951 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5850 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.5925 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5938 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.5988 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5934 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.5775 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5919 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5926 Acc: 0.7124\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7124\n",
      "\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.5945 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5829 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.6024 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5928 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.5904 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5942 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.5942 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6009 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.5884 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5956 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.6021 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5961 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.6048 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5846 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.5961 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5921 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.5886 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5915 Acc: 0.7190\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7190\n",
      "\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.5936 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5913 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.6036 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5967 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.5910 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5985 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.5843 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5897 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.5931 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5942 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.5992 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5951 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.6081 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.5853 Acc: 0.7190\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7190\n",
      "\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.5840 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5876 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.6135 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5914 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 279/499\n",
      "----------\n",
      "train Loss: 0.6082 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5840 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.5799 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5854 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.6017 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5956 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.6113 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5847 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.5836 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5924 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.6099 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5971 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.5934 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5921 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.5841 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5821 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.5982 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5872 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.5861 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5846 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.5972 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5956 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.6183 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5954 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.5787 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5956 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.6003 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5981 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.6063 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5842 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.5737 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5953 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.5904 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5854 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.5886 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5892 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.5889 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5941 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.5710 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5887 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.6047 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5867 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.5905 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5858 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.6159 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6311\n",
      "val Loss: 0.5929 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.5998 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5882 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.6069 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.5942 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.5946 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5906 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.6011 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.5886 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.5954 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5928 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.6016 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5872 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.5845 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5882 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.5965 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5919 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.6062 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5979 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.5878 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5906 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.6049 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5907 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.5981 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5949 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.6004 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5899 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.6151 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5910 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.6071 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5861 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.5727 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5952 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.5980 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5871 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.6155 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5833 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.5876 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5920 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.5910 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5964 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.6078 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5868 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5844 Acc: 0.7124\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7124\n",
      "\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.5884 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5966 Acc: 0.7124\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7124\n",
      "\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.6116 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6075 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 326/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5958 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5914 Acc: 0.7124\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7124\n",
      "\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.5999 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5920 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.5945 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5925 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.5952 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5933 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.5806 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6000 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.5934 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5961 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.5886 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5927 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.5928 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5970 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.5870 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6006 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.6045 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5924 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.5911 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5906 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.5782 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7336\n",
      "val Loss: 0.5921 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.5967 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5924 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.5932 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5853 Acc: 0.7124\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7124\n",
      "\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.6127 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5872 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.5863 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5906 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.6096 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5927 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.6052 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.5925 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.6006 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5952 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.5738 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5912 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5924 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.6171 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.5883 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 348/499\n",
      "----------\n",
      "train Loss: 0.5992 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5937 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.5808 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5912 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.6144 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5931 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5943 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.5915 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5958 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.5814 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5921 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5884 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.5901 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5919 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.5844 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6021 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.6064 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5960 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.5965 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5982 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.5791 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5930 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.5994 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5910 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.5806 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5892 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.6052 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5893 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.5942 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5958 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.5820 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5938 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.5825 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7418\n",
      "val Loss: 0.5909 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.5974 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5917 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.5884 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5919 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.5802 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5839 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.5788 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5940 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.5839 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5933 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.6051 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5906 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 372/499\n",
      "----------\n",
      "train Loss: 0.5879 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5942 Acc: 0.7124\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7124\n",
      "\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.6146 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.5942 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.5919 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5882 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.5822 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5941 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.5906 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.6021 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5930 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.5891 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5996 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.6028 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5931 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 380/499\n",
      "----------\n",
      "train Loss: 0.5955 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5895 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.5951 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5875 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.6018 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5844 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.5942 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5934 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.5981 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5879 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.6134 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5996 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.6040 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5848 Acc: 0.7124\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7124\n",
      "\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.5953 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5888 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6006 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.6030 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5904 Acc: 0.7124\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7124\n",
      "\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.6009 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5960 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.6117 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5878 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.6119 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.5872 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.5905 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5913 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.6013 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.5919 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.5988 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5940 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.6003 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5882 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.6065 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5870 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.5898 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5876 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.6009 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5971 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.6009 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6020 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.6164 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5885 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.5870 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5967 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.5909 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5929 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.5981 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6046 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 405/499\n",
      "----------\n",
      "train Loss: 0.5998 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5907 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.5899 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5933 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.6024 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5932 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.5915 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5921 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.5985 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5959 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.5786 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7336\n",
      "val Loss: 0.5870 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.5993 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5882 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.5988 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5866 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.6024 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5914 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.5764 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5954 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.5925 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5894 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.6036 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6028 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.6068 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5994 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.6042 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5848 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 419/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6058 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5946 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.6022 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5935 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.6045 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5929 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.5974 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6036 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5867 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.5982 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5887 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.5983 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6072 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.6156 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6040 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.6071 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5935 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.6018 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5920 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.5944 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5856 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.5940 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6005 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.5914 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5996 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.5974 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5898 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.5956 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5967 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.6028 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5984 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.5914 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5948 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 436/499\n",
      "----------\n",
      "train Loss: 0.6021 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5902 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.5985 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5936 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.6155 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5920 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.5796 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5866 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.6080 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5970 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.5916 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5960 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.5864 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5987 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.6033 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5924 Acc: 0.7124\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7124\n",
      "\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.5962 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5877 Acc: 0.7190\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7190\n",
      "\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5919 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.5841 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5978 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.6109 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5968 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.6039 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5850 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.5996 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5901 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.5879 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5852 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.5979 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5922 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.6027 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5999 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.5915 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5903 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.6027 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5969 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.5965 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5996 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.6028 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5944 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.5975 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5939 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.6139 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6055 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.5999 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5959 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.6103 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.5959 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.6019 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5953 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 462/499\n",
      "----------\n",
      "train Loss: 0.5836 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5854 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.5892 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5874 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.6124 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5930 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 465/499\n",
      "----------\n",
      "train Loss: 0.5815 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5845 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.5787 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6016 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.6079 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5891 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.5872 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5941 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.5941 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5893 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.6092 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5919 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.5805 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6027 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.5931 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5902 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.5878 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5895 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.5850 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5839 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.5811 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5859 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.5757 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5966 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.5822 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5940 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.5966 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5877 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.6120 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5926 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.6138 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5929 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.5909 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5931 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 482/499\n",
      "----------\n",
      "train Loss: 0.5986 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5939 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.5853 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5884 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.6150 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6230\n",
      "val Loss: 0.5881 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.6014 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5924 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.5966 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5892 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.5783 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5937 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.6143 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.5971 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.6075 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5892 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.5957 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5936 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.6036 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5917 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 492/499\n",
      "----------\n",
      "train Loss: 0.5947 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5909 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.5880 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5959 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.6055 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5884 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.5971 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5986 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.6127 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5901 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.5843 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5990 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.5947 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5885 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.5835 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5848 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Training complete in 19m 27s\n",
      "Best val Acc: 0.718954\n"
     ]
    }
   ],
   "source": [
    "model_ft,train_losses,val_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dump_output(model_ft,train_losses[0:50],val_losses[0:50],'without-pretrained-resnet18_lrscheduler_lastlayer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E3)Train wholenetwork, LR scheduler, 1000 epochs, resnet34 \n",
    "----------------------------------\n",
    "\n",
    "Resnet34, train the whole network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.7690 Acc: 0.5369\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.5369\n",
      "val Loss: 0.8167 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.7198 Acc: 0.5574\n",
      "train Rajat Best_Acc: 0.6144 Epoch_Acc: 0.5574\n",
      "val Loss: 0.6808 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6144 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.8003 Acc: 0.5697\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5697\n",
      "val Loss: 1.1544 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.8059 Acc: 0.5820\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5820\n",
      "val Loss: 0.7029 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.6406 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6667 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7319 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6667 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.6609 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6844\n",
      "val Loss: 0.8907 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.6502 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.5714 Acc: 0.7255\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.7255\n",
      "\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.6759 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6025 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.5633 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5812 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.5698 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5916 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.5738 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5961 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.5601 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6147 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.6051 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5927 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.5867 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6296 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.5684 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6265 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.5597 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6139 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.5520 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5878 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.5293 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5839 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.5718 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6145 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.5725 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6122 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.5865 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6010 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.5465 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6158 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.5513 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5999 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.5530 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5994 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.5675 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6165 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.5746 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5999 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.5376 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6059 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.5517 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5924 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.5457 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6115 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.5484 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5976 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.5354 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6022 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.5801 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6083 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.5634 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5936 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.5498 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5992 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.5498 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6069 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.5618 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5914 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.5380 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6004 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.5452 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5932 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.5810 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6207 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.5701 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6035 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.5563 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6064 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.5561 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6152 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.5464 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5958 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.5312 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6072 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.5781 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6021 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.5420 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6004 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.5502 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5961 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.5674 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6209 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.5621 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6018 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.5581 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5911 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.5868 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6039 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.5569 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6096 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.5977 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5860 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.5739 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6034 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.5667 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6238 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.5465 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6021 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.5605 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5937 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.5609 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5969 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.5399 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5879 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.5543 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5975 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.5169 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7500\n",
      "val Loss: 0.5914 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.5786 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5923 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.5334 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.5972 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.5701 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6100 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.5246 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5895 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.5575 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6074 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.5802 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5934 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.5318 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6079 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.5673 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6119 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.5463 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.5994 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.5460 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5962 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.6003 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5970 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.5682 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5941 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.5375 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5911 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.5107 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6049 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.5620 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5927 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.5529 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6036 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.5669 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6141 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.5389 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5966 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.5649 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6065 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.5376 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6004 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.5569 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6051 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.5475 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5885 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.5791 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5948 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.5439 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6239 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.5588 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6125 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.5717 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5944 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.5686 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6195 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.5583 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5927 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.5338 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6131 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.5521 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5833 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.5436 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6129 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.5860 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5968 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 93/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5798 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5933 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.5381 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7418\n",
      "val Loss: 0.5999 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.5692 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6019 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.5400 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5932 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.5604 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6016 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.5519 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6139 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.5851 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6023 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.5406 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6026 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.5600 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6018 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.5770 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5968 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.5556 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5909 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.5485 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6283 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.5671 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5970 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.5482 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6014 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.5622 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6118 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.5484 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6117 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.5880 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6112 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.5580 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5878 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.5640 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5887 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.5724 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5940 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.5538 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6139 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.5448 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6073 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.5321 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6207 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.5562 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5965 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.5654 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5901 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.5587 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5926 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.5729 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6052 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.5689 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6011 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.5835 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6434\n",
      "val Loss: 0.5912 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 122/499\n",
      "----------\n",
      "train Loss: 0.5751 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5972 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.5545 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5972 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.5612 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6053 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.5531 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6041 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.5565 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6020 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.5436 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5968 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.5728 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5952 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.5557 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6151 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.5507 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6064 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.5516 Acc: 0.7541\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7541\n",
      "val Loss: 0.5926 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.5887 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6028 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.6020 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5924 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.5412 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6049 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.5545 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6028 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.5482 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5893 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.5327 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6041 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.5649 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6064 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.5295 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6058 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.5308 Acc: 0.7664\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7664\n",
      "val Loss: 0.5996 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.5744 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6044 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.5489 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5980 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.5729 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6059 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.5318 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5948 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.5624 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5933 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.5372 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6013 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.5802 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5942 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.5591 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6090 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.5702 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5986 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.5690 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6080 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.5652 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6047 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.5370 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6188 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.5743 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6064 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.5356 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5945 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.5749 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6190 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.5409 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6003 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.5632 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6097 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.5341 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6011 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.5981 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6104 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.5593 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5957 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.5248 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5885 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.5446 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6105 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.5304 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7500\n",
      "val Loss: 0.5936 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.5688 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5968 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.5594 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6138 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.5183 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.5966 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.5375 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5812 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.5628 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6041 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.5652 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6025 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.5478 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6057 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.6094 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6051 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.5469 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6097 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.5920 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6037 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.5179 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.5983 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.5682 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6042 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.5711 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5920 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.5750 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6044 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.5490 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5942 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.5902 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6138 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.5712 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5944 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.5488 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5974 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.6076 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5998 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.5612 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.5959 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.5617 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5860 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.5747 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6105 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 186/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5470 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6127 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.5548 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5987 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.5680 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5879 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.5816 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6219 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.5171 Acc: 0.7582\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7582\n",
      "val Loss: 0.6098 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.5634 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5940 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.5666 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6242 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.5523 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6057 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.5665 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5953 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.5729 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6038 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.5536 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6030 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.5733 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5922 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.5584 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5971 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.5604 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5864 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.5769 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6038 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.5531 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5978 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.5756 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6190 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.5524 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5864 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.5356 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5924 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.5457 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6224 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.5447 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6188 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.5466 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5871 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.5484 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6093 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.5423 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5869 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.5379 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5957 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.5442 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6036 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.5676 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6123 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.5454 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5904 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.5216 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5952 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.5494 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6081 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.5373 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6075 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.5934 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6020 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.5615 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6018 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.5507 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6240 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.5566 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6057 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.5543 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5932 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.5842 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6069 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.5433 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5981 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.5647 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6046 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.5506 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6034 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.5658 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6101 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.5606 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6002 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.5863 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5911 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.5577 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6112 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.5600 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6104 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.5923 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5969 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.5523 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5873 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.5502 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5929 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.5476 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5894 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.5776 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6331 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.5505 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6049 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.5334 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6009 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.5733 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5960 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.5603 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5976 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.5275 Acc: 0.7541\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7541\n",
      "val Loss: 0.6134 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.5759 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6211 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.5678 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5916 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.5830 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6009 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.5456 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6038 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.5963 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5980 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.5399 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6057 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.5733 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6015 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.5553 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6136 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.5898 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6037 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.5516 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6260 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.5512 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6043 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.5686 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6184 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.5365 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6092 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.5398 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6078 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.5511 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.5987 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.5900 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6004 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.5414 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6091 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.6150 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6024 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.5648 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6135 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.5208 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6008 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.5531 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6008 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.5583 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6197 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.5525 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6205 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.5596 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5958 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.5560 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5980 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.5366 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6082 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.6042 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6224 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.5632 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6011 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.5421 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5988 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.5597 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6111 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.5781 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5991 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.5615 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6088 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.5885 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6102 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.5677 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6096 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.5355 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5997 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.5344 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.5872 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.5561 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5919 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.5225 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6110 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 279/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5793 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6083 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.5633 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6170 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.5532 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5928 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.5711 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5954 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.5413 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6015 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.5889 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5888 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.5518 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6101 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.5017 Acc: 0.7787\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7787\n",
      "val Loss: 0.5934 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.5525 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5918 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.5844 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6007 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.5474 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6015 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.5811 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5949 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.5650 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6084 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.5729 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6033 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.5922 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.5929 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.5615 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5875 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.5750 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5947 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.5658 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6008 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.5440 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6139 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.5716 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6017 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.5262 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7459\n",
      "val Loss: 0.5948 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.5483 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6343 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.5579 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5987 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.5630 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5937 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.5993 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6311\n",
      "val Loss: 0.5966 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.5435 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6172 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.5834 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5883 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.5504 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5968 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.5332 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6014 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.5363 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5999 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.5395 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6001 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.5543 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5891 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.5575 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5955 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.5546 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5938 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.5477 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6127 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.5314 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6054 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.5301 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7500\n",
      "val Loss: 0.5982 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.5368 Acc: 0.7541\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7541\n",
      "val Loss: 0.6046 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.5902 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6012 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.5552 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6065 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.5299 Acc: 0.7746\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7746\n",
      "val Loss: 0.6071 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.5542 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5875 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.5431 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6032 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.5552 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6040 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.5392 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7459\n",
      "val Loss: 0.5952 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.5629 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5931 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.5624 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6072 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.5556 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6048 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.5969 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6046 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.5543 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6070 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.5245 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5952 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.5885 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5950 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.5571 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5916 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.5458 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6000 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.5387 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6068 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.5702 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5845 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.5483 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6083 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.5663 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.5924 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.5669 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6149 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.5569 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6044 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.5612 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6005 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.5680 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6152 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.5663 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5887 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.5716 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6106 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.5462 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6140 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.6058 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5930 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.5482 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6007 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.5373 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6175 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.5568 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5839 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 348/499\n",
      "----------\n",
      "train Loss: 0.5308 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7418\n",
      "val Loss: 0.5891 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.5843 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5917 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.5373 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6042 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.5737 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6062 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.5821 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5924 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.5624 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5917 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.5554 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6088 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.5994 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5924 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.5491 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6019 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.5662 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5995 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.5449 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6051 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.5646 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5978 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.5755 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6103 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.5574 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6056 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.5723 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6006 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.5483 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6115 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.5492 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6052 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.5427 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5953 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.5561 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7418\n",
      "val Loss: 0.5986 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.5220 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5855 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.5602 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6075 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.5516 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5994 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.5596 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5969 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.5620 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6114 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 372/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5870 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5927 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.5819 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6041 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.5555 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6062 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.5556 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6033 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.5484 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6069 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.5615 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5974 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.5624 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5996 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.5789 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5879 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 380/499\n",
      "----------\n",
      "train Loss: 0.5296 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7500\n",
      "val Loss: 0.6081 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.5526 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5974 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.5742 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5988 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.5634 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5886 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.5550 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6245 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.5593 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5908 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.5473 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5936 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.5603 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5979 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.5651 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6068 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.5703 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6030 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.5495 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6031 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.5515 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5958 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.5499 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5956 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.5404 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6121 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.5683 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5964 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.5718 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6063 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.5530 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6022 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.5353 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7418\n",
      "val Loss: 0.5980 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.5532 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6053 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.5557 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.5939 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.5546 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6110 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.5712 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5983 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.5644 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6012 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.5770 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6029 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.5590 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6012 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 405/499\n",
      "----------\n",
      "train Loss: 0.5299 Acc: 0.7541\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7541\n",
      "val Loss: 0.5989 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.5607 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6104 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.5224 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6026 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.5616 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5981 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.5366 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6033 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.5383 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5978 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.5532 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5897 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.5821 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6124 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.5784 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6071 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.5674 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5936 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.5507 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5922 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.5488 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6011 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.5679 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6021 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.5730 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6107 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 419/499\n",
      "----------\n",
      "train Loss: 0.5462 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5967 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.5709 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6071 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.5346 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7418\n",
      "val Loss: 0.5952 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.5679 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6040 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.5528 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5961 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.5720 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5854 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.5549 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5996 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.5331 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7418\n",
      "val Loss: 0.5939 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.5727 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6020 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.5560 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6092 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.5830 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6129 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.5719 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5998 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.5536 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5960 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.5651 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5950 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.6044 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6311\n",
      "val Loss: 0.5971 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.5789 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5935 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.5456 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5879 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 436/499\n",
      "----------\n",
      "train Loss: 0.5660 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6069 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.5728 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6050 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.5656 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6181 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.5566 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.5860 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.5591 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.5958 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.5831 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6072 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.5805 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6005 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.5640 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6026 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.5504 Acc: 0.7500\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7500\n",
      "val Loss: 0.5980 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.5456 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5971 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.5368 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6107 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.5455 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.5906 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.5781 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6166 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.5747 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5927 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.5751 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6169 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.5581 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6116 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.6030 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5963 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.5705 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6191 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.5900 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5934 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.5632 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5992 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.5587 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5923 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.5570 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7459\n",
      "val Loss: 0.5946 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.5484 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5993 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.5539 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6014 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.5861 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6680\n",
      "val Loss: 0.5896 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.5610 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5930 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 462/499\n",
      "----------\n",
      "train Loss: 0.5758 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5959 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.5478 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5934 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.5161 Acc: 0.7623\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7623\n",
      "val Loss: 0.5848 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 465/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5444 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6026 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.5443 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6016 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.5563 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.5960 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.5500 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6006 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.6330 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6133 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.5632 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5869 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.5392 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5936 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.5577 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6028 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.5760 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6197 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.5195 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.5859 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.5580 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6104 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.5747 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6077 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.5496 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7295\n",
      "val Loss: 0.5969 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.5470 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6039 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.5773 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5945 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.5665 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6101 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.5285 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.5988 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 482/499\n",
      "----------\n",
      "train Loss: 0.5261 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6214 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.5750 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5909 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.5960 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6045 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.5274 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6123 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.5175 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7336\n",
      "val Loss: 0.5903 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.5699 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5966 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.5858 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5935 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.5783 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6061 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.5524 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7008\n",
      "val Loss: 0.5951 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.5572 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6112 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 492/499\n",
      "----------\n",
      "train Loss: 0.5478 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5966 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.5442 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5958 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.5742 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6122 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.5553 Acc: 0.7418\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7418\n",
      "val Loss: 0.6072 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.5638 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6083 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.5461 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5940 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.5382 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5931 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.5485 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7255 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6091 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7255 Epoch_Acc: 0.6732\n",
      "\n",
      "Training complete in 21m 2s\n",
      "Best val Acc: 0.725490\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet34(pretrained=False)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "model_ft,train_losses,val_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dump_output(model_ft,train_losses[0:50],val_losses[0:50],'without-pretrained-resnet34_lrscheduler_wholenetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E4)Train lastlayer, LR scheduler, 100 epochs, resnet34 \n",
    "----------------------------------\n",
    "\n",
    "Resnet18, train the last layer only\n",
    "\n",
    "Best val Acc: 0.954248\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.6079 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6123 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.6128 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6193 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.6308 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6585 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.5879 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6487 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.6014 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6140 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.6805 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6261 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.6244 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6463 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.6173 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6277 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.5988 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5990 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.6034 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6356 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.6436 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6191 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.5925 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6576 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6132 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.6118 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6511 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.5748 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6553 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.6552 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6449 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.5992 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6262 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.6018 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6511 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.5926 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6466 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.6202 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6215 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.6021 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6371 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.6538 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6421 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.6809 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6374 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.6070 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6416 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.5783 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6073 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.6177 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6718 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.5610 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6611 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.6277 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6416 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.5539 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6266 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.5856 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6095 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.5677 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6134 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.6036 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6275 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.5892 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6645 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.5968 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6239 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.6462 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6184 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.5982 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6342 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.6173 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6178 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.5749 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6286 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.6350 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6426 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.5961 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6172 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.5948 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6364 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.6215 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6118 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6543 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.5531 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6124 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.6360 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6127 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.6217 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6078 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.6432 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6432 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.6325 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6281 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.6551 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6415 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.5642 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6665 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.6182 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6305 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.6350 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6301 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.6155 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6221 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.6255 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6151 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.6374 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6391 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.6567 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6407 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.6132 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6350 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.6349 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6143 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.6284 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6142 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.6330 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6267 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.6034 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6784 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.5737 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6420 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.6326 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6274 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.6144 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6624 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.5933 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6025 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.6068 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6297 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.6032 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6268 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.6310 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6372 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.6506 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6359 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.5885 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6535 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.6297 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6236 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.5862 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6225 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.6456 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6145 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.5643 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6646 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.6311 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6247 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.5525 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6162 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.6204 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6107 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.5965 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6243 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.6176 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6218 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.6091 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6432 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.6169 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6208 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.5892 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7172\n",
      "val Loss: 0.5945 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.5897 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6181 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.5775 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6301 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.6087 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6405 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.6212 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6490 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.6618 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6290 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.5964 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6193 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.5962 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6374 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.6250 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6432 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.6128 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6511 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.5866 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6373 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.6126 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6298 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 93/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5959 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6423 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.5913 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6331 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.6069 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6350 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.6373 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6129 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.5918 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6146 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.5815 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6153 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.6511 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6358 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.6028 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6496 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.6080 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.5943 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.5898 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6211 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.6342 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6121 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.5791 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6133 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.5659 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5878 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.6228 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6332 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.6002 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6170 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.6009 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6070 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.6061 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6179 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.6520 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6244 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.6262 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6048 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.6127 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6614 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.6126 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6270 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.5981 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6207 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.6012 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6660 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.6344 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6528 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.5995 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6425 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.6163 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6047 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.6291 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6300 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.6222 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6364 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.5940 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6106 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 122/499\n",
      "----------\n",
      "train Loss: 0.5984 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6262 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.6098 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6063 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.5802 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.5982 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.6717 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6263 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.6325 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6374 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.5905 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6625 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.6198 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6552 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.6006 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6753 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.6121 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6166 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.6362 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6275 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.6082 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6055 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.6624 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6537 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.6453 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6205 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.6299 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6308 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.6576 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6278 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.6444 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6277 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.5488 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6229 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.5950 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6386 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.5776 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6447 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.6091 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6059 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.6054 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6329 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.5953 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6526 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.5816 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6113 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.5762 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6320 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.5656 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6291 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.6322 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6402 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.5877 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6409 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.6042 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6071 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.6376 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6583 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.6164 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6132 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.6133 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6229 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.5849 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6469 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.5734 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6729 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.5991 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6261 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.5990 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6196 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.6115 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6277 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.6037 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6292 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.6322 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6396 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.5986 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6301 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.6148 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6095 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.6443 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6377 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.5997 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6206 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.5987 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6495 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.5877 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6095 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.6592 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6481 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.6127 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6181 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.6270 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6518 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.5985 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6301 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.6022 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6118 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.6210 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6271 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.6201 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6300 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.6276 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6435 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.6543 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6258 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.6668 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6390 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.6283 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6243 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.5866 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6439 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.6397 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6220 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.5944 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6214 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.6163 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6378 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.6269 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6196 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.6161 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6551 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.6214 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6645 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.6152 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6677 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.6191 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6331 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 186/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6594 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6199 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.6303 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5930 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.5911 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6198 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.5787 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6152 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.5771 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6181 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.6097 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6239 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.6050 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6242 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.6143 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6130 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.6024 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6131 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.6157 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6282 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.6271 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6239 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.5574 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6126 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.5792 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6420 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.5732 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6677 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.5947 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6042 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.6281 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6159 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.6108 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6406 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.5848 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6343 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.6026 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6244 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.5623 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6325 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.5722 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6065 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.6083 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6183 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.6131 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6419 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.6453 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6441 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.6328 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6293 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.6694 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6306 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.6073 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6614 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.6280 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6390 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.6156 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6578 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.6113 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5970 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.6179 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6122 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.6238 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6109 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.6143 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6113 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.5858 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6202 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.6511 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6084 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.6711 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6104 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.6131 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6180 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.6392 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6169 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.5835 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6268 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.6214 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6227 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.5975 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6314 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.5765 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6478 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.5677 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6121 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.5739 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6261 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.6114 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6362 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.5862 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6135 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.6060 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6091 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.6297 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6254 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.6058 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6537 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.5852 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6523 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.6013 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6342 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.6708 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6124 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.6194 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6222 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.6188 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6314 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.6598 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6350 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.5682 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6223 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.6248 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6496 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.5991 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6149 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.5735 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6430 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.6422 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6158 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.5882 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6227 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.6195 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6021 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.6312 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6317 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.5426 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6346 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.6192 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6218 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.6249 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6608 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.5905 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6408 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.5890 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6061 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.6155 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6374 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.5854 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6307 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.5914 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5968 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.5821 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6389 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.5956 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6124 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.5743 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6567 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.6378 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6234 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.6268 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6179 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.6471 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6222 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.6160 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6604 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.6367 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6479 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.6182 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6404 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.6440 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6229 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.6064 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6378 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.6522 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6091 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.6266 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6433 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.5743 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6250 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.6588 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6159 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.6526 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6150 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.6363 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6448 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.6488 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6271 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.6073 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6084 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.5924 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6137 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.5660 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6552 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.5827 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6199 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 279/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6393 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6132 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.6201 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6278 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.5779 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6020 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.6272 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6238 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.6132 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6388 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.6008 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6192 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.5973 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6037 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.6161 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6130 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.6752 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5934 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.5826 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6189 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.6019 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6124 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.5717 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6245 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.6004 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6055 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.6760 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6458 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.5950 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6413 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.6351 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6344 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.6196 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6260 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.6413 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6432 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.5926 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6399 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.6036 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6532 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.6107 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6242 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.6254 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6288 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.6124 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6460 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.6411 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6167 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.6290 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6015 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.5732 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6212 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.6138 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6155 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.6099 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6401 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.6328 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6146 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.6240 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6109 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.6249 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6262 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.6487 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6425 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.6192 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6257 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.6161 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.5953 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.6181 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6366 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.5980 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6374 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.6351 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6226 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.6189 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6381 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6186 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.6286 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6472 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.6525 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6230\n",
      "val Loss: 0.5934 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.6004 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6175 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.6229 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6129 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.6166 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6248 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.6202 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6407 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.5836 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6308 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.6005 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6139 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.6283 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6123 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.5953 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6345 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.6048 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6202 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.6096 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6387 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.6155 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6196 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.6531 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6205 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.5818 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6110 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.6360 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6358 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.6305 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6262 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.6105 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6192 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.5620 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6408 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.5911 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6020 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.6126 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6492 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.5995 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6335 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.6090 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6183 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.6350 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6236 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.6210 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6290 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.6212 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6328 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.6181 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6247 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.5822 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6079 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.6539 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6207 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.6206 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5908 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 348/499\n",
      "----------\n",
      "train Loss: 0.5698 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6213 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.6158 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6275 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.5915 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6692 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.6019 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6463 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.5634 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6377 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.6223 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6488 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.6240 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6086 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.6315 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6328 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.6229 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6208 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.6131 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6100 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.6043 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6223 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.6111 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6415 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.6431 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6188 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.6508 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6437 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.6433 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6535 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.5785 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6508 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.6590 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6409 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.5518 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6372 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.6348 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6224 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.5995 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6409 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.6441 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6276 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.5755 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6383 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.5676 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6303 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.5748 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6244 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 372/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5966 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5975 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.6413 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6651 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.5960 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6362 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.6553 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.5984\n",
      "val Loss: 0.6357 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.5939 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6138 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.6063 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6047 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.6602 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6377 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.6103 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6202 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 380/499\n",
      "----------\n",
      "train Loss: 0.6394 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6203 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.6518 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6370 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.6566 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6471 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.6233 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6026 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.6288 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6413 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.5841 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6406 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.6246 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6166 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.6226 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6056 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.5768 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6247 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.6518 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6159 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.5813 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6294 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.5632 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6007 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.5719 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6187 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.6416 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6016 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.6076 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6222 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.6070 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6271 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.5905 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6399 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.5754 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6434 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.6290 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6491 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.6594 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6357 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.6184 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6521 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.5872 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6306 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.5929 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6155 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.5645 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6128 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.5689 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6330 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 405/499\n",
      "----------\n",
      "train Loss: 0.6036 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6395 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.6359 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6277 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.6332 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6129 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.6003 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6198 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.6066 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6484 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.6644 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6235 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.6076 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6192 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.6216 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6448 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.6261 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6103 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.6390 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6182 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.6170 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6208 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.5921 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6093 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.6390 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6660 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.6106 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6284 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 419/499\n",
      "----------\n",
      "train Loss: 0.6522 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6246 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.6872 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6557 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.6336 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6103 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.6077 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6600 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.5572 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6791 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.5687 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6391 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.6210 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6131 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.6362 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6240 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.6259 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6273 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.5871 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6418 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.6099 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6383 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.6220 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5934 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.5611 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6312 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.6136 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6146 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.6053 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6356 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.6257 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6091 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.6317 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6267 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 436/499\n",
      "----------\n",
      "train Loss: 0.6218 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6268 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.5880 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6211 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.5582 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6237 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.6394 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6396 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.6259 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6173 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.6326 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6227 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.6306 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6271 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.6067 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6361 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.6090 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6479 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.5884 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5926 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.6259 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6268 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.5678 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6289 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.5630 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6144 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.5708 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6477 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.5683 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6314 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.6192 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6169 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.6431 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6201 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.6035 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6209 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.6195 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6544 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.5942 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6128 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.5827 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6137 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.6034 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6256 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.6339 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6064 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.6895 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6025\n",
      "val Loss: 0.6557 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.5969 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6410 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.5915 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6553 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 462/499\n",
      "----------\n",
      "train Loss: 0.5609 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.5988 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.6108 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6172 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.6075 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6198 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 465/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6270 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6364 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.5901 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6213 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.6077 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6757 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.6087 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6025 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.6075 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6310 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.6626 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6179 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.6433 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6357 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.6415 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6134 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.5975 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6391 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.6002 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6447 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.6783 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6175 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.6377 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6466 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.6441 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6199 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.6111 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6391 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.5713 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6404 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.6545 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6251 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.6188 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6160 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 482/499\n",
      "----------\n",
      "train Loss: 0.5935 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6222 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.5961 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6300 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.6230 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6456 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.6027 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6231 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.5537 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6264 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.6415 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6286 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.5821 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6026 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.6288 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6050 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.6330 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6594 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.6292 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6296 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 492/499\n",
      "----------\n",
      "train Loss: 0.6852 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6332 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.6050 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6460 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.5899 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6252 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.6345 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6162 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.6338 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6393 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.5980 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6113 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.5920 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6052 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.6049 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6584 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Training complete in 18m 56s\n",
      "Best val Acc: 0.705882\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet34(pretrained=False)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model_ft,train_losses,val_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dump_output(model_ft,train_losses[0:50],val_losses[0:50],'without-pretrained-resnet34_lrscheduler_lastlayer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E5)Train wholenetwork, LR scheduler, 500 epochs, resnet50 \n",
    "----------------------------------\n",
    "\n",
    "Resnet50, train the whole network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 1.2014 Acc: 0.5123\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.5123\n",
      "val Loss: 3.3071 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.9335 Acc: 0.4836\n",
      "train Rajat Best_Acc: 0.5948 Epoch_Acc: 0.4836\n",
      "val Loss: 3.2827 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.5948 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 1.1682 Acc: 0.5041\n",
      "train Rajat Best_Acc: 0.6144 Epoch_Acc: 0.5041\n",
      "val Loss: 3.2708 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6144 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 1.7063 Acc: 0.5369\n",
      "train Rajat Best_Acc: 0.6144 Epoch_Acc: 0.5369\n",
      "val Loss: 1.4025 Acc: 0.5229\n",
      "val Rajat Best_Acc: 0.6144 Epoch_Acc: 0.5229\n",
      "\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.7706 Acc: 0.5738\n",
      "train Rajat Best_Acc: 0.6144 Epoch_Acc: 0.5738\n",
      "val Loss: 3.4352 Acc: 0.4575\n",
      "val Rajat Best_Acc: 0.6144 Epoch_Acc: 0.4575\n",
      "\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 1.0429 Acc: 0.5328\n",
      "train Rajat Best_Acc: 0.6144 Epoch_Acc: 0.5328\n",
      "val Loss: 1.3692 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6144 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.8716 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.9768 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.6286 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7662 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.6455 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6675 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.6337 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7390 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.6532 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6066\n",
      "val Loss: 0.6670 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.6188 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7003 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.6043 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7456 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.6282 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6376 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.6063 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6403 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.5775 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6598 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.6209 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6375 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.6253 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6927 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.6110 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6296 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.5682 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6283 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.6162 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6494 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.6035 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6406 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.6140 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6607 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.5883 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6751 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.6002 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6569 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.5764 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6522 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.5879 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6546 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.6037 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6449 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.5689 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6275 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.5759 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6283 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.5833 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6306 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.5882 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6505 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.5882 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6398 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.5898 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6411 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.5924 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6408 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.5765 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6580 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.5858 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6252 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.5905 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6652 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.5816 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6406 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.6096 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6630 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.5899 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6627 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.6073 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6653 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.5779 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6369 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.5677 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6453 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.5877 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6324 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.6014 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6604 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.5983 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6417 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.5887 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6294 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.5947 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6568 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.5919 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6549 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.6027 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6551 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.5965 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6360 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.5749 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6433 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.5872 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6380 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.6121 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6489 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.6075 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6442 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.5952 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6378 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.5864 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6361 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.5917 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6476 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.6123 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6276 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.5877 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6293 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.6043 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6443 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.5796 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6428 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.6326 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6597 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.5789 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6423 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.6239 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6525 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.5872 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6482 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.5785 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6376 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.6133 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6812 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.6083 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6554 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.6057 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6250 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.5909 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6494 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.5986 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6437 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.6061 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6310 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.5889 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6337 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.6195 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6317 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.6032 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6354 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.5861 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6384 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.5978 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6427 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.6060 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6733 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.5993 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6360 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.5891 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6314 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.5741 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6407 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.6191 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6425 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.6048 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6457 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.5911 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6514 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.6049 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6425 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.6055 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6475 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.6081 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6346 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.6375 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6456 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.5953 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6542 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.5897 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6461 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.5971 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6348 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 93/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6113 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6514 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.6038 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6346 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.5911 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6382 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.5891 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6604 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.5793 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6596 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.5878 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6450 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.5936 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6427 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.5903 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6298 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.5675 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6387 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.6090 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6566 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.6106 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6488 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.6263 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6560 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.5878 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6387 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.5960 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6681 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.6006 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6340 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.5820 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6347 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.5708 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6638 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.6013 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6385 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.5830 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6365 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.6045 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6410 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.5824 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6491 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.5886 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6429 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.5875 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6389 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.6305 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6459 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.5925 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6405 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.6052 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6343 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.5823 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6462 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.5979 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6712 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.6093 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6437 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 122/499\n",
      "----------\n",
      "train Loss: 0.6115 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6450 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.6011 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6300 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.6120 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6368 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.5908 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6271 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.5938 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6323 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.6365 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6437 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.6032 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6279 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.5915 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6529 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.6209 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6523 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.5548 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6292 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.5997 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6405 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.5857 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6328 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.5980 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6424 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.5809 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6438 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.6077 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6359 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.6153 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6460 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.6115 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6393 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.6065 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6305 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.5718 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6236 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.5921 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6311 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.6017 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6479 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.6014 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6537 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.6293 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6449 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.5927 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6391 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.5794 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6321 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.6279 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6425 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.5786 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6501 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.6179 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6574 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.5940 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6373 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.5806 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6449 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.5994 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6546 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.5789 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6336 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.5509 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6508 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.5620 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6323 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.6200 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6583 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.6046 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6538 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.6056 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6558 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.6190 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6473 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.5920 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7003 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.6196 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6505 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.6132 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6590 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.5871 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6608 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.5943 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6309 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.5960 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6421 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.5953 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6768 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.6285 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6562 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.5889 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6295 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.6183 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6263 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.5952 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6487 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.5989 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6345 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.5976 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6336 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.5785 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6603 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.5718 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6448 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.5880 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6419 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.6153 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6316 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.5987 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6627 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.5891 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6307 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.5968 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6421 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.5699 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6339 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.6363 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6426 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.6211 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6403 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.5686 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6362 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.5984 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6521 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.5849 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6384 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 186/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5941 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6379 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.5975 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6377 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.5854 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6585 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.5952 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6432 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.6006 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6376 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.5998 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6372 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.6159 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6487 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.5705 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6424 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.5808 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6322 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.5949 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6540 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.6042 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6340 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.5776 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6334 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.5924 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6412 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.6235 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6211 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.6005 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6413 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.5897 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6398 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.5842 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6504 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.5821 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6239 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.5807 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6392 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.6055 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6716 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.5857 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6484 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.5929 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6274 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.6346 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6520 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.5893 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6354 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.5913 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6283 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.5850 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6551 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.5964 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6301 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.5737 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6764 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.6012 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6323 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.6147 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6475 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.5907 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6519 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.5856 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6513 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.5840 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6234 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.6122 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6535 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.5845 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6229 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.5863 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6395 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.6070 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6432 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.6026 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6390 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6485 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.5637 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6416 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.6025 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6688 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.5791 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6492 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.5897 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6369 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.6148 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6404 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.6103 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6421 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.5913 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6303 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.5791 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6412 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.5663 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6519 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.6026 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6435 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.5841 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6316 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.5958 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6488 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.6026 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6429 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.5928 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6268 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.5988 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6315 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.6111 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6612 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.5790 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6430 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.5741 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6685 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.6072 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6447 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.5881 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6468 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.5967 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6246 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.5860 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6440 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.6024 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6525 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.5899 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6398 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.5907 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6434 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.5903 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6434 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.5980 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6441 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.5939 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6536 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.6199 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6320 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.5754 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6409 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.5894 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6275 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.6040 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6381 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.5755 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6405 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.6274 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6336 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.6020 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6374 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.5886 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6391 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.5767 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6500 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.6010 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6383 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.6091 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6676 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.6138 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6391 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.5940 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6459 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.6205 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6195 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.5913 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6744 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.5895 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6321 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.5992 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6641 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.6028 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6439 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.5754 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6460 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.6120 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6272 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.5915 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6438 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.5739 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6510 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.5828 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6552 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.6145 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6344 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.6200 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6410 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.5953 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6329 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 279/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5848 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6554 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.5865 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6360 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.5888 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6469 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.5823 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6420 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.5928 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6413 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.5908 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6264 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.5957 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6576 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.5825 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6514 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.6144 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6644 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.5973 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6479 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.6010 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6397 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.6042 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6458 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.5986 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6604 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.5938 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6229 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.6076 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6463 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.5760 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6608 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.5946 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6491 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.5781 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6230 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.6030 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6636 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.6025 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6592 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.5685 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6367 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.6056 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6240 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.6098 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6414 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.5877 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6323 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.5796 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6455 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.6035 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6353 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.5984 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6492 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.6110 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6530 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.5987 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6354 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.5943 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6431 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.5856 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6450 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.5905 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6401 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.6182 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6586 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.5879 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6409 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.5777 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6507 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.6020 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6468 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.5728 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6430 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.6283 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6487 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.5945 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6424 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.6144 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6364 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.5811 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6412 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.6103 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6391 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.5784 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6409 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.6261 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6714 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.5963 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6556 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.6218 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6507 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.6236 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6338 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.6172 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6469 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.5939 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6467 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.6003 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6420 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.6043 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6411 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.6101 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6470 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.5675 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6689 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.5853 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6533 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.5947 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6453 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.6156 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6517 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.5845 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6780 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.6152 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6355 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.5886 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6518 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.5939 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6458 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.6141 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6313 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.5906 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6304 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.5848 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6328 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.5962 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6390 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.5901 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6555 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.5912 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6490 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.6013 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6427 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.5939 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6166 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.6172 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6549 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 348/499\n",
      "----------\n",
      "train Loss: 0.6069 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6365 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.5921 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6464 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.5876 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6351 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.5878 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6600 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.5963 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6597 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.5989 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6387 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.5882 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6451 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.6082 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6437 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.6036 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6481 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.5979 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6518 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.5643 Acc: 0.7377\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7377\n",
      "val Loss: 0.6756 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.5917 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6795 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.5889 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6517 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.5994 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6360 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.5856 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6398 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.6049 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6439 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.5818 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6438 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.6127 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6358 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.5999 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6395 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.5861 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6300 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.5981 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6401 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.6199 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6446 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.6063 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6526 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.6307 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6594 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 372/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6339 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6369 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.5985 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6434 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.6020 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6470 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.6109 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6388 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.5826 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6385 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.6032 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6601 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.5906 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6350 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.6015 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6452 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 380/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6603 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.5918 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6616 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.5878 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6403 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.6021 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6377 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.6130 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6535 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.5937 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6577 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.5739 Acc: 0.7459\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7459\n",
      "val Loss: 0.6407 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.6042 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6518 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.5830 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6648 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.5867 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6439 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.5926 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6449 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.5747 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6650 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.5869 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6465 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.6047 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6444 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.5995 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6336 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.6006 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6383 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.5742 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6382 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.5978 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6434 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.6062 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6620 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.6089 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6685 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.6007 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6538 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.6230 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6454 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.5868 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6397 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.6007 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6361 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.6101 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6411 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 405/499\n",
      "----------\n",
      "train Loss: 0.5903 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6398 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.6050 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6310 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.6066 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6351 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.5867 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6202 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.5913 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6414 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.6132 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6656 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.6094 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6383 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.6104 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6708 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.5964 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6441 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.6045 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6384 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.5985 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6260 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.5945 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6447 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.5974 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6572 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.5750 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6612 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 419/499\n",
      "----------\n",
      "train Loss: 0.6129 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6364 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.6035 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6491 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.5842 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6529 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.6283 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6532 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.5886 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6383 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.6176 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6396 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.5950 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6355 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.6165 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6442 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.6022 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6275 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.6152 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6444 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.5831 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6352 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.6012 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6411 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.5801 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6574 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.5981 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6566 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.5810 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6437 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.5843 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6561 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.6098 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6223 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 436/499\n",
      "----------\n",
      "train Loss: 0.5830 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6271 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.5883 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6620 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.5959 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6417 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.5823 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6580 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.5976 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6370 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.5863 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6312 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.5710 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7295\n",
      "val Loss: 0.6264 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.5848 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6636 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.5868 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6651 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.5988 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6553 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.5928 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6481 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.6237 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6527 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.6093 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6387 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.6012 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6260 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.5881 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6385 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.6374 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6330 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.5762 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6256 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.5911 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6471 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.5887 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6370 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.6098 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6743 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.5974 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6490 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.5854 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6352 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.5620 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6311 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.6033 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6686 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.5911 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6644 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.6043 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6531 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 462/499\n",
      "----------\n",
      "train Loss: 0.5864 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6346 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.5910 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6496 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.6209 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6448 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 465/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5961 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6267 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.5915 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6564 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.6100 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6404 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.5844 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6692 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.5987 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6807 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.5990 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6339 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.6139 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6448 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.5866 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6484 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.5802 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6523 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.6300 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6391 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.5782 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7172\n",
      "val Loss: 0.6319 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.5960 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6317 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.6164 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6577 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.5741 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6328 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.5792 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6451 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.6053 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6456 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.5797 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6348 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 482/499\n",
      "----------\n",
      "train Loss: 0.6241 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6615 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.5849 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6394 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.6331 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6641 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.6088 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6456 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.5925 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6388 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.6105 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6506 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.5840 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6419 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.5850 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7213\n",
      "val Loss: 0.6239 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.5859 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6415 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.5911 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6593 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 492/499\n",
      "----------\n",
      "train Loss: 0.5737 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6486 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.5726 Acc: 0.7336\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7336\n",
      "val Loss: 0.6731 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.5720 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6415 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.5990 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6296 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.6085 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6286 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.5995 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6413 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.6321 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6641 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.5825 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.6928 Epoch_Acc: 0.7254\n",
      "val Loss: 0.6335 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6928 Epoch_Acc: 0.6340\n",
      "\n",
      "Training complete in 25m 25s\n",
      "Best val Acc: 0.692810\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet50(pretrained=False)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "model_ft,train_losses,val_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dump_output(model_ft,train_losses[0:50],val_losses[0:50],'without-pretrained-resnet50_lrscheduler_wholenetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E6)Train lastlayer, LR scheduler, 100 epochs, resnet50 \n",
    "----------------------------------\n",
    "\n",
    "Resnet18, train the last layer only\n",
    "\n",
    "Best val Acc: 0.954248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.6397 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7696 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.6021 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7257 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.6305 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7079 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.6219 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6405 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7222 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6405 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.6404 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6471 Epoch_Acc: 0.6721\n",
      "val Loss: 0.8001 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6471 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.6138 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7156 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.5878 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7660 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.5769 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7066 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.6256 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7505 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.6505 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7558 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.6092 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7606 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.6409 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7661 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.6023 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.7090\n",
      "val Loss: 0.7599 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.6106 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.7049\n",
      "val Loss: 0.7571 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.6342 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7850 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.6196 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7388 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.6089 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7226 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.6344 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7205 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.6353 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6905 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.6255 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6919 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.6368 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7668 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.5983 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.7090\n",
      "val Loss: 0.7462 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.6034 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7255 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.5956 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7448 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.6154 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7436 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.6309 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7288 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.6149 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7261 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.5830 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7028 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.6206 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7655 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.6145 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6963 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.5861 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7239 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.6208 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7203 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.6380 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7281 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.6063 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7284 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.5948 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7959 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.6087 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7577 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.5922 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7183 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.5864 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.7049\n",
      "val Loss: 0.7255 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.6185 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7598 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.6368 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7459 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.6136 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7248 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.6494 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.8025 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.5989 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7308 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.6406 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7587 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.6285 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6752 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.6265 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7401 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.5714 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7313 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.5737 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6938 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.6001 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7093 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.5983 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7271 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.6203 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7247 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.6067 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.7090\n",
      "val Loss: 0.7082 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.6029 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7197 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.6294 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7644 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.6279 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7189 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.6402 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7200 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.6266 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6897 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.6117 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7044 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.6019 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6955 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.6151 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7039 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.6121 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6931 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.6022 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7224 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.6157 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7686 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.6252 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7273 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.6117 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7080 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.6284 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7298 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.6262 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7263 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.6348 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7533 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.6350 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7638 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.5922 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.7090\n",
      "val Loss: 0.7274 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7414 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.6178 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7301 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.6285 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7916 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.6150 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7041 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.5797 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7226 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.5890 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.7090\n",
      "val Loss: 0.7255 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.6217 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7135 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.6001 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7901 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.6290 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7105 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.6007 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7209 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.6143 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7096 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.6209 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7074 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.6130 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7365 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.5970 Acc: 0.7254\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.7254\n",
      "val Loss: 0.7069 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.6011 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7155 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.5959 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6896 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.5937 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.7049\n",
      "val Loss: 0.7417 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.5722 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7718 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.6059 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7475 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.6352 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7239 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.6123 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7112 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.6153 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7330 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.6082 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7275 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 93/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6223 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6888 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.6046 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7435 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.6077 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7838 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.6099 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7214 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.6025 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.7172\n",
      "val Loss: 0.7188 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.6021 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7054 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.6204 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7106 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.6149 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6967 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.5694 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7209 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.6018 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7340 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.6210 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7560 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.6227 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6868 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.6172 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7324 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.5890 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7370 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.6265 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7061 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.6117 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6910 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.5853 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7326 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.6508 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7473 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.5980 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7113 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.6078 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7756 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.5898 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7288 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.5939 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7300 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.6142 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7069 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.6416 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7237 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.6151 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6938 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.6207 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7199 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.5919 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7408 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.5964 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7866 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.6046 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7184 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 122/499\n",
      "----------\n",
      "train Loss: 0.6004 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7740 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.6216 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7101 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.6006 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7637 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.6040 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7603 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.6265 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7733 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.6299 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7620 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.6085 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7118 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.6195 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7056 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.6156 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6311\n",
      "val Loss: 0.8222 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.6017 Acc: 0.7295\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.7295\n",
      "val Loss: 0.7526 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.6126 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6723 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.6249 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6797 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.5853 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.7213\n",
      "val Loss: 0.7019 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.6050 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6946 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.6006 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.7131\n",
      "val Loss: 0.6988 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.6183 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6873 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.6305 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7020 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.6223 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7222 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.6017 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7526 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.5914 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7456 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.5955 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7038 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.5872 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.7131\n",
      "val Loss: 0.7210 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.6437 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7754 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.5822 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6987 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.6138 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7822 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.6348 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7312 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.6334 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6943 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.5973 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7389 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.6325 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7536 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.6241 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6876 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.6863 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.6206 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7373 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.5964 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6888 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.6343 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7375 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.6130 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7356 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.6212 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7493 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.5991 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6883 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.6280 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7189 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.6017 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7555 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.6100 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6923 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.6018 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7522 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.6324 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7472 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.6246 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7186 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.5976 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7242 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.6372 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7107 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.5997 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7357 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.6110 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7229 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.5804 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6924 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.6123 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7283 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.6070 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7587 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.5981 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7333 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.6182 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7266 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.6327 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6952 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.5989 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7079 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.6156 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7025 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.6093 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7403 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.6034 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7293 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.6260 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7379 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.5892 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.7008\n",
      "val Loss: 0.7096 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.6155 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6800 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.5899 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6999 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.5726 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.7131\n",
      "val Loss: 0.7463 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.6269 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7340 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.6173 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7879 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.6047 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.7090\n",
      "val Loss: 0.7496 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 186/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6189 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7064 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.5909 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7470 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.6032 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7248 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.6253 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7268 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.6191 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7080 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.6381 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7643 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.6108 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7143 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.6479 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7270 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.6301 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7464 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.6096 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7164 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.6414 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7114 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.6308 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7168 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.6193 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7452 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.6014 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6920 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.6099 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7435 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.6246 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7695 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.5979 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7745 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.6101 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7047 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.6289 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7043 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.6108 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6979 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.6133 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7084 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.5952 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7213 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.6266 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7209 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.6445 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7019 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.5991 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7487 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.6067 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7427 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.5966 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.7008\n",
      "val Loss: 0.6980 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.6185 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6866 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.6089 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7452 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.6074 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7029 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.6018 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7152 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.6179 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6994 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.6015 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7249 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.6161 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6819 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.6050 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7290 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.6229 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7400 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.6182 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7052 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.5998 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7217 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.6401 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7370 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.6375 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6938 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.6174 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7150 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.6160 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7477 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.6035 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.7008\n",
      "val Loss: 0.7152 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.6231 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7056 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.6045 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7046 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.6185 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7234 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.5975 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.7049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7215 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.6122 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7115 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.6158 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7485 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.5929 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6937 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.6417 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7819 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7284 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.6283 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7097 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.6483 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6992 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.6238 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7354 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.6453 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6993 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6759 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.6993 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.6216 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7267 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.6112 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7251 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.6316 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7557 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.6395 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7423 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.6051 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6968 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.5987 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7495 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.6162 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6881 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.6408 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7375 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.6087 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7363 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.6024 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7027 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.6237 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7389 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.6146 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7051 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.6256 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7232 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.6152 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7004 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.6090 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6847 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.6275 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7364 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.6157 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7734 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.6049 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7177 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.6143 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6682 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.6313 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7107 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.6384 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7244 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.6004 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.8194 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.6332 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7125 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.5826 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6709 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.6130 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7618 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.6171 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7658 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.6054 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7101 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.6319 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7130 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.5955 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.7404 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.5966 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7082 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.6104 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.7309 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.6318 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7194 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.6000 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7297 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.5931 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7264 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.5991 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7853 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.6333 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7463 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.5825 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.7147 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 279/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6214 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6945 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.5896 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.8012 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.5934 Acc: 0.7213\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7213\n",
      "val Loss: 0.7589 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.6228 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7331 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.6147 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6804 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.6043 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7102 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.6083 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6891 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.6050 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7789 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.6055 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7306 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.6180 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7157 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.6019 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7220 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.6214 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7062 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.6264 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7249 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.6383 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6952 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.5924 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7114 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.5992 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.7302 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.6197 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7193 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.5959 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7048 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.6155 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7587 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.6470 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7116 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.5861 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.8406 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.6122 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7834 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.6267 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6743 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.6087 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6992 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.6378 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7269 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.6038 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7115 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.5772 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7146 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.5774 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7256 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.6180 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7372 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.5997 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.7335 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.6439 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7551 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.6218 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7890 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.5862 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.7239 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.6119 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7595 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.6301 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7148 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.5947 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7806 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.6133 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7041 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.5855 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.7368 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.6530 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7452 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.6332 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7650 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.5934 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7190 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.6128 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7267 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.6192 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7360 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.6244 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.7147 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.6242 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7948 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.6155 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7050 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.6048 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7346 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.5790 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7119 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.6632 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7646 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.6176 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7080 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.6248 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7310 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.6243 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7047 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.6389 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7345 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.6010 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7469 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.6091 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7205 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.6061 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.7198 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.6086 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7050 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.6278 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.8113 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.6470 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6189\n",
      "val Loss: 0.8429 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.5968 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.7332 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.6263 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7200 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.6242 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7365 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.6179 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.6942 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.6124 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7097 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.6259 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7025 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.6054 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7445 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.6280 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7017 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.6155 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7466 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.6110 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7184 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 348/499\n",
      "----------\n",
      "train Loss: 0.6333 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7426 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.5951 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7268 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.5902 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7216 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.6323 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7111 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.6322 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7600 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.6466 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7345 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.6028 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7478 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.6443 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7824 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.6032 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6856 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.6095 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7299 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.6156 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6968 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.5871 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7052 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.6150 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6844 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.6152 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7180 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.6162 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6958 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6987 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.6242 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6727 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.6201 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6729 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.5875 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7015 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.6280 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7443 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.6570 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7412 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.6039 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.7439 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.6072 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7365 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.5950 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7881 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 372/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6029 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7181 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.6097 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7292 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.6187 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7515 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.6114 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7687 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.6084 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6864 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.6374 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7449 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.6379 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7607 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.6031 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7264 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 380/499\n",
      "----------\n",
      "train Loss: 0.5977 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7090\n",
      "val Loss: 0.7186 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.6248 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7458 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.6118 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7676 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.5949 Acc: 0.7172\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7172\n",
      "val Loss: 0.7327 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.6329 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7315 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.6508 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7721 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.6294 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7377 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.6040 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7071 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.6320 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6921 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.5996 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7707 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.6494 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7346 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.6106 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7323 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.6143 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7846 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.5891 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7054 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.6171 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7326 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.6275 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7004 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.5925 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.7280 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.6145 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7544 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.6169 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7389 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.6238 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7081 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.5948 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7450 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.5954 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7183 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.6115 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7224 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.6329 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6809 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.6084 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6992 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 405/499\n",
      "----------\n",
      "train Loss: 0.6350 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7714 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.6288 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7382 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.5989 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6798 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.6167 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7218 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.6214 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6864 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.6369 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7027 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.6109 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7033 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.6032 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7362 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.6050 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7439 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.6152 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7541 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.6474 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7115 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.6221 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7040 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.5907 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6876 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.6062 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7117 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 419/499\n",
      "----------\n",
      "train Loss: 0.5947 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7026 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.5998 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6822 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.5977 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7114 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.5713 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.7220 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.6017 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7081 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.6264 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7577 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.5992 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7537 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.6159 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6780 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.5833 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7263 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.6071 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7861 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.6216 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7564 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.6536 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7085 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.6073 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7328 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.6366 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7182 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.6201 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7419 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.6090 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6757 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.6337 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7323 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 436/499\n",
      "----------\n",
      "train Loss: 0.6256 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7227 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.6207 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7519 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.6411 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7298 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.6448 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7442 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.6121 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7373 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.6149 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7207 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.6259 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7452 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.6091 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7396 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.6070 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7205 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.6176 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7003 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.6454 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7536 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.6057 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7713 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.5951 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7436 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.6219 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7335 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.6168 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7195 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.5991 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6912 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.6129 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6823 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.6101 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7046 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.5922 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6931 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.6442 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7160 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.6270 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7529 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.6274 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7412 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.6271 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7225 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.6173 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7568 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.5996 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7253 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.5909 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7008\n",
      "val Loss: 0.7300 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 462/499\n",
      "----------\n",
      "train Loss: 0.5902 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7310 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.6265 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7391 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.6120 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7481 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 465/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5873 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7344 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.6030 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7590 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.6151 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7599 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.6570 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7350 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.5955 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7282 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.6167 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7195 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.6130 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7001 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.6077 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7138 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.5891 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7391 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.6329 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7199 Acc: 0.6928\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6928\n",
      "\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.6122 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7543 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.6173 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7465 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.6122 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7434 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.6091 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7359 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.5984 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7788 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.5950 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6975 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.6152 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7071 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 482/499\n",
      "----------\n",
      "train Loss: 0.5972 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7257 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.6379 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7417 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.5702 Acc: 0.7131\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7131\n",
      "val Loss: 0.7640 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.6205 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7563 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.5806 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7049\n",
      "val Loss: 0.7162 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.6151 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6958 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.6265 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6783 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.5761 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7604 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.5990 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7216 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.6124 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7332 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 492/499\n",
      "----------\n",
      "train Loss: 0.5998 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7720 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.6281 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6928 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.6218 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7468 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.6046 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7331 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.6098 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7502 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.6261 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6791 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.6698 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7302 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.5959 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7424 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Training complete in 24m 55s\n",
      "Best val Acc: 0.705882\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=False)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model_ft,train_losses,val_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dump_output(model_ft,train_losses[0:50],val_losses[0:50],'without-pretrained-resnet50_lrscheduler_lastlayer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E7)Train WholeNetwork, LR scheduler, 100 epochs, resnet101\n",
    "----------------------------------\n",
    "\n",
    "Resnet101, train the whole network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.8204 Acc: 0.4877\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.4877\n",
      "val Loss: 1.0312 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.8999 Acc: 0.5738\n",
      "train Rajat Best_Acc: 0.5425 Epoch_Acc: 0.5738\n",
      "val Loss: 4.1860 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.5425 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 1.0343 Acc: 0.5205\n",
      "train Rajat Best_Acc: 0.5556 Epoch_Acc: 0.5205\n",
      "val Loss: 2.9479 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.5556 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.9428 Acc: 0.5820\n",
      "train Rajat Best_Acc: 0.5556 Epoch_Acc: 0.5820\n",
      "val Loss: 0.9940 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.5556 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 1.0665 Acc: 0.5205\n",
      "train Rajat Best_Acc: 0.6013 Epoch_Acc: 0.5205\n",
      "val Loss: 1.5226 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6013 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 1.4038 Acc: 0.4918\n",
      "train Rajat Best_Acc: 0.6013 Epoch_Acc: 0.4918\n",
      "val Loss: 1.6001 Acc: 0.5163\n",
      "val Rajat Best_Acc: 0.6013 Epoch_Acc: 0.5163\n",
      "\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.8233 Acc: 0.5533\n",
      "train Rajat Best_Acc: 0.6013 Epoch_Acc: 0.5533\n",
      "val Loss: 0.7455 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6013 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.7047 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6144 Epoch_Acc: 0.5943\n",
      "val Loss: 1.1716 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6144 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.6819 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6144 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7246 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6144 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.6444 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6434\n",
      "val Loss: 1.1188 Acc: 0.4902\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.4902\n",
      "\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.6429 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.5943\n",
      "val Loss: 0.9271 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.6684 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7753 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.6491 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6963 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.6528 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7338 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.6433 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7123 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.6255 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7236 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.6013 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7268 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.6241 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7422 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.6255 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7077 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.6117 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6948 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.6171 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.8083 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.6612 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7076 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.6310 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7601 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.6117 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7240 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.6130 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6982 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.6159 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7871 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.6275 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7397 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.6306 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7220 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.6208 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7706 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.6296 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7291 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.6211 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7185 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.6186 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7313 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.6084 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7329 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.6312 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.8308 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.6389 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.8154 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.6188 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7650 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.6422 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6025\n",
      "val Loss: 0.8052 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.6336 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.8343 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.6270 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7640 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.6197 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7932 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.6115 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7143 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.6030 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7294 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.6521 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7398 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.6529 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7227 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.6256 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.8022 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.5980 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7513 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.6218 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7270 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.6250 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7340 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.6670 Acc: 0.5697\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5697\n",
      "val Loss: 0.7479 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.5976 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7135 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.6087 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7253 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.6240 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7676 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.6433 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7340 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.6202 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7521 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.6292 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7425 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.6550 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7515 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.6389 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7292 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.6443 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7666 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.6293 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7533 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.6043 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7760 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.6438 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7680 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.6026 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.8374 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.6475 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7266 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.6092 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7231 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.6027 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7490 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.5909 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7624 Acc: 0.5098\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5098\n",
      "\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.6159 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7101 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.6199 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7188 Acc: 0.5229\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5229\n",
      "\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.6431 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7478 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.6051 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7141 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.6020 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7410 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.6145 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7649 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.6093 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7584 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.6070 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7378 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.6215 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.8007 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.6244 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7978 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.6379 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7366 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.6284 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7425 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.5948 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7123 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.6273 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7353 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.6225 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7222 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.6351 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7307 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.6271 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7667 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.6339 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.8131 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.6209 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7141 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.6235 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7114 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.6163 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7175 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.6006 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7175 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.6408 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7414 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.6584 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7496 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.6272 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7845 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.5985 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7793 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.6712 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7724 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 93/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6373 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7546 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7577 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.6255 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7527 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.6273 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7177 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.5971 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7605 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.6180 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6944 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.6320 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7178 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.6576 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7139 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.6414 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7262 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.6236 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7635 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.6257 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7763 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.6292 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7586 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.6319 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7394 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.6382 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7851 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.6158 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7070 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.6361 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7252 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.6199 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7661 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.6171 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7459 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.6058 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7597 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.6428 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7908 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.6291 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7632 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.6329 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7928 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.6281 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7931 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.6041 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7813 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.6101 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7229 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.6407 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7874 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.6425 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5902\n",
      "val Loss: 0.8297 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.6441 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7188 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.6042 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7307 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 122/499\n",
      "----------\n",
      "train Loss: 0.6344 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7373 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.6170 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7205 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.6440 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7466 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.6402 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7309 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.6214 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7330 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.6485 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5861\n",
      "val Loss: 0.7563 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.6205 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6984 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.6180 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7430 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.6365 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7772 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.6179 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7394 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.6176 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7392 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.6105 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7812 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.6081 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7263 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.6249 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7802 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.6378 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7249 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.6384 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7465 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.6022 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7819 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.5961 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7253 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.6119 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7309 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.6440 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7331 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.6282 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7907 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.6315 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7101 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.6007 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7347 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.6353 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7414 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.6238 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7101 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.6357 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7387 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.6190 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7437 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.6389 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7248 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.6183 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7675 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.6126 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7305 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.6173 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7726 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.6554 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7434 Acc: 0.5229\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5229\n",
      "\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.6370 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.8074 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.6101 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7597 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.6374 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5902\n",
      "val Loss: 0.8188 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.6082 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7431 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.6193 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7283 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.6356 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6107\n",
      "val Loss: 0.8238 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.6264 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7756 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.6307 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7441 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.5894 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7739 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.6256 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7469 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.6029 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7701 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.6215 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7802 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.6514 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7479 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.6373 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7908 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.6349 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7506 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.6312 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7862 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.6233 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7739 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.6281 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7308 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.6231 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7458 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.6293 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7430 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.6324 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7408 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.6125 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7419 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.5927 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7217 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.6090 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7505 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.6419 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7752 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.6092 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7754 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.6140 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7154 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.6352 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7166 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.6335 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7182 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.6116 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7500 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.6116 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7166 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.6096 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7912 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 186/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6294 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7282 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.6334 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7470 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.6078 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.8209 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.6047 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7274 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.6246 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7201 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.6063 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7195 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.6215 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7342 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.6217 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7249 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.6104 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7531 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.5947 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.7049\n",
      "val Loss: 0.7123 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.6519 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7804 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.6272 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7202 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.6148 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7170 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.6523 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7634 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.6215 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7709 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.6078 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7387 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.6400 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7181 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.6063 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7794 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.6221 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7633 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.6416 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7248 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.6451 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7639 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.5849 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.7049\n",
      "val Loss: 0.7362 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.6462 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7372 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.6383 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7169 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.6081 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7636 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.6185 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.8088 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.6468 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7585 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.6103 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7119 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.6187 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7440 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.5980 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7351 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.6300 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7445 Acc: 0.5033\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5033\n",
      "\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.6385 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.8318 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.6372 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.8104 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.6070 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7279 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.6441 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7834 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.6613 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7406 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.6254 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7540 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.6109 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7518 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.6180 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7417 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.6197 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7133 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.6297 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.8628 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.6038 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7516 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.5869 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7292 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.6323 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7393 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.6160 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7189 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.6009 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7520 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.6425 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7260 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.6433 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.8218 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.6365 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7295 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.6102 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7172 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.6125 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7513 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.6137 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7690 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.6194 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.8124 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.6428 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7840 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.6269 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7511 Acc: 0.5163\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5163\n",
      "\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.6486 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7326 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.6260 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7042 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.6047 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7426 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.6347 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7232 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.6386 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7282 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.6258 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7583 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.6264 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.8242 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.6264 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7899 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.6391 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7437 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.6091 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7286 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.6278 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7249 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.6103 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7498 Acc: 0.5163\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5163\n",
      "\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.6271 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7394 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.5987 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7415 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.6130 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7691 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.6167 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7445 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.6319 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7859 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.6281 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7299 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.6057 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7537 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.6292 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7319 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.6211 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7188 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.5977 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7724 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.6303 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7388 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.6172 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7272 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.6294 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7343 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.6178 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7335 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.6140 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7387 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.6673 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7299 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.6059 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7895 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.6119 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7198 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.6258 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7524 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.6275 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7500 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.6426 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7271 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.6143 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7622 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.6058 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7211 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.6437 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7814 Acc: 0.5229\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5229\n",
      "\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.6197 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7259 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.6301 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7283 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 279/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6130 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7632 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.6119 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7743 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.6630 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7534 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.6455 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7800 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.6431 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7174 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.6237 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7517 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.6231 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7344 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.6052 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7504 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.6555 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5861\n",
      "val Loss: 0.7567 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.6347 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7438 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.6389 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7178 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.6276 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.8290 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.6264 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7327 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.6226 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7958 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.6191 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7504 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.6394 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7102 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.6270 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7269 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.6507 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7360 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.5998 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7411 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.6403 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6107\n",
      "val Loss: 0.8415 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.6695 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.8367 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.6104 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7392 Acc: 0.5163\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5163\n",
      "\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.6131 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7931 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.5979 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7398 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.6259 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7145 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.6318 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7289 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.6217 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6967\n",
      "val Loss: 0.7342 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.6146 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7227 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.6384 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7069 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.6455 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7434 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.6290 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7169 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.6128 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7496 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.6462 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7383 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.5984 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7629 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.6062 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.8426 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.6345 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7750 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.6365 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7229 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.6124 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7110 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.6474 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7356 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.6153 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7521 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.6248 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7079 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.6128 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.8003 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.6362 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7248 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.6462 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7683 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.6359 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7486 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.6162 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7904 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.6516 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7789 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.5948 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7711 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.6357 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7006 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.6358 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7452 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.6148 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7416 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.6442 Acc: 0.5779\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5779\n",
      "val Loss: 0.7214 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.6243 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7639 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.6157 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7218 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.6349 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7499 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.6326 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7181 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.6441 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7224 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.6296 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7403 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.6000 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7375 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7398 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.6152 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7321 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.6352 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7835 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.6152 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7232 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.5979 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7367 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.6051 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7191 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.6288 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7068 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.6547 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7463 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.6423 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7332 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.6247 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7356 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 348/499\n",
      "----------\n",
      "train Loss: 0.6479 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7406 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.6413 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7674 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.6153 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7671 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.6535 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7065 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.6184 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7431 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.6232 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7306 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.6187 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.8041 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.5981 Acc: 0.6885\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6885\n",
      "val Loss: 0.7196 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.6375 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7120 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.6098 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7294 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.6220 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7186 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.6600 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7939 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.6167 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7337 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.6289 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7202 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.6181 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7776 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.6319 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7496 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.6405 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7141 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.6207 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7269 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.6212 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7208 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.6212 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.8391 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.6267 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7722 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.6061 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7372 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.6390 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7761 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.6216 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7267 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 372/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6303 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7318 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.6519 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7633 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.6066 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7602 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.6221 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7209 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.6472 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7897 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.6733 Acc: 0.5697\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5697\n",
      "val Loss: 0.7970 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.6198 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7252 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.6572 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7385 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 380/499\n",
      "----------\n",
      "train Loss: 0.6485 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7474 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.6254 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7652 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.6436 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7573 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.6261 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7550 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.6556 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7231 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.6211 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.8120 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.6413 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7160 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.6175 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7107 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.6257 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7545 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.6108 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.8621 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.6319 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7843 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.6252 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7765 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.6242 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7730 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.6281 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7491 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.6128 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7409 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.6188 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7334 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.6094 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7443 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7307 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.6297 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7432 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.6149 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7231 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.6343 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7174 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.6283 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7272 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.6085 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7839 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.6364 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7263 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.6237 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7277 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 405/499\n",
      "----------\n",
      "train Loss: 0.6266 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7603 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.5942 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7470 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.6475 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7619 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.6160 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7817 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.6324 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.8646 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.6156 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7949 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.6275 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7206 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.6478 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7183 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.6208 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7337 Acc: 0.5229\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5229\n",
      "\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.6242 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7729 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.6397 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7263 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.6024 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7142 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.6438 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7078 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.6555 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.8097 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 419/499\n",
      "----------\n",
      "train Loss: 0.6169 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7348 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.6349 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7483 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.6589 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7436 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.6368 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7681 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.6188 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7336 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.6045 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7430 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.5887 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6844\n",
      "val Loss: 0.7159 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.6068 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6926\n",
      "val Loss: 0.7581 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.6098 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7231 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.6140 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7263 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.6519 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.8060 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.6067 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7604 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.6056 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7259 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.6276 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7265 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.6179 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7341 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.6210 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7767 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.6505 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7434 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 436/499\n",
      "----------\n",
      "train Loss: 0.6316 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7959 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.6122 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7915 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.6141 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7412 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.6474 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7345 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.6280 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.8589 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.6088 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7477 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.6135 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7422 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.6001 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7608 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.5952 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.7049\n",
      "val Loss: 0.8132 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.6371 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7192 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.6228 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7332 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.6165 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7064 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.6177 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7329 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.6487 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7443 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.6583 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7203 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.5794 Acc: 0.7008\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.7008\n",
      "val Loss: 0.7374 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.6431 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7368 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.6152 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7290 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.6177 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7069 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.5984 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7437 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.6358 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7231 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.6460 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7896 Acc: 0.5229\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5229\n",
      "\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.6573 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7936 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.6217 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7825 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.6278 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7506 Acc: 0.5229\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5229\n",
      "\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.6202 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7647 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 462/499\n",
      "----------\n",
      "train Loss: 0.6299 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7818 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.6371 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7315 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.6329 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7332 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 465/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6497 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7480 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.6345 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7399 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.5914 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7640 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.6174 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.8107 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.6413 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7670 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.6373 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7948 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.6643 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7627 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.6300 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7267 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.6194 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7500 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.6200 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7542 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.6203 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7112 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.6160 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7599 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.6288 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7799 Acc: 0.5294\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5294\n",
      "\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.6025 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7396 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.6114 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7377 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.6537 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7260 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.6378 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7150 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 482/499\n",
      "----------\n",
      "train Loss: 0.6332 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7255 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.6155 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7602 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.6270 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7420 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.6351 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7346 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.6552 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7212 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.6449 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7526 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.6713 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7529 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.6486 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7328 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.6104 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7220 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.5979 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7237 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 492/499\n",
      "----------\n",
      "train Loss: 0.5979 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7160 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.6312 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7676 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.6214 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7282 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.6192 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7428 Acc: 0.5163\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5163\n",
      "\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.6076 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7714 Acc: 0.5556\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5556\n",
      "\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.5996 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7142 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.6122 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7520 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.6578 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6340 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7546 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6340 Epoch_Acc: 0.5882\n",
      "\n",
      "Training complete in 36m 34s\n",
      "Best val Acc: 0.633987\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ft = models.resnet101(pretrained=False)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "model_ft,train_losses,val_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=500)\n",
    "dump_output(model_ft,train_losses[0:50],val_losses[0:50],'without-pretrained-resnet101_lrscheduler_wholenetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.6223 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6930 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.6784 Acc: 0.5738\n",
      "train Rajat Best_Acc: 0.6144 Epoch_Acc: 0.5738\n",
      "val Loss: 0.7018 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6144 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.6640 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6144 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7099 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6144 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.7092 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7566 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.6660 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7230 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.6144 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7120 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.6610 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6958 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.7048 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7062 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.6486 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7412 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.6814 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6405 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7107 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6405 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.6201 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6405 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7175 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6405 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.6747 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6405 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7196 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6405 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.6645 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7065 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.6432 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7029 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.6482 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6925 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.6489 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7305 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.6879 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7424 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.6755 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7201 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.6655 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7128 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.6731 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7169 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.6578 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7633 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.6685 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6912 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.6514 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7111 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.6324 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6921 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.6138 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7406 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.6434 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7424 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.6854 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7350 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.6552 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7212 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.6318 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7050 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.6358 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6940 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.6799 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7045 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.6924 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7138 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.6346 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7466 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.6732 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7481 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.6738 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7278 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.6653 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7158 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.6540 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7584 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.6822 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7234 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.6530 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7159 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.6613 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7167 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.6544 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7170 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.6548 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7212 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.7012 Acc: 0.5615\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.5615\n",
      "val Loss: 0.7724 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.6911 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7049 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.6657 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7407 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.6501 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7032 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.6899 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7284 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.6533 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6961 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.6635 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7118 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.6769 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7260 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.6598 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7239 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.6315 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6918 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.6235 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7118 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.6758 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6953 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.6480 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7069 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.6811 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7133 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.6441 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7030 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.6751 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6601 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7215 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6601 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.6571 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6667 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7108 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6667 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.6337 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6667 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6946 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6667 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.6563 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6667 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7252 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6667 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.6594 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7002 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.6391 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7016 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.6714 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6870 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.6747 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7250 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.6524 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7336 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.6342 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7587 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.6740 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7218 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.6584 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7073 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.6395 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6991 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.7121 Acc: 0.5779\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5779\n",
      "val Loss: 0.7430 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.6527 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7281 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.6298 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7154 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.6863 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7290 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.6811 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7045 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.6488 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7100 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.6738 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7198 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.6668 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6929 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.6954 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7179 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.6558 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7135 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.6566 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7163 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.6231 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7184 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.6350 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7285 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.6659 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6989 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.6648 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7286 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.6557 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7138 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.6578 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7035 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.6567 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7200 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.6944 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7392 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.6517 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7127 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.6634 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7459 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.6681 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5861\n",
      "val Loss: 0.7617 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.6585 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7209 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 93/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6536 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7557 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.6598 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7435 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.6553 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7238 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.6467 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7592 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.6423 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7439 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.6610 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7191 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.6436 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6854 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.6379 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7243 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.6548 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7315 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.7042 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7127 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.6606 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7198 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.7008 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7259 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.6673 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7781 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.6308 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7166 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.6722 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7251 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.6325 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7219 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.6119 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7074 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.6451 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7388 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.6506 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7100 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.6498 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6919 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.6435 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7145 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.6706 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7333 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.6564 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7240 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.6470 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6929 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.6708 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7714 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.6317 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7276 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.6759 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7085 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.6217 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7324 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.6827 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7287 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 122/499\n",
      "----------\n",
      "train Loss: 0.6860 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7120 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.6622 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7065 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.6978 Acc: 0.5697\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5697\n",
      "val Loss: 0.7471 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.6395 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7185 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.6631 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6962 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.6849 Acc: 0.5779\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5779\n",
      "val Loss: 0.7581 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.6550 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7380 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.6498 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5861\n",
      "val Loss: 0.7559 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.6766 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7396 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.6366 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7113 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.6549 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7292 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.5983 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7137 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.6374 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7218 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.6508 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7347 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.6658 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7054 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.6013 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7240 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.6707 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7388 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.6581 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7223 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.6901 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7227 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.6887 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7077 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.6719 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7241 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.6571 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7209 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7177 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.6680 Acc: 0.5779\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5779\n",
      "val Loss: 0.7092 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.6511 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7200 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.6532 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6854 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.6655 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7190 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.6546 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7199 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.6829 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7776 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.6496 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7276 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.6842 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7180 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.6916 Acc: 0.5533\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5533\n",
      "val Loss: 0.7108 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.6682 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7121 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.6614 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7278 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.6557 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7163 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.6785 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7213 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.6668 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7705 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.6257 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7152 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.6585 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7061 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.6340 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7314 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.6493 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7473 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.6569 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7415 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.6196 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6928 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.6476 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7332 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.6435 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7451 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.6826 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7167 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.6307 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7198 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.6637 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6954 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.6652 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6947 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.6563 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6938 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.6470 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7487 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.6484 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7125 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.6528 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7159 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.6502 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7004 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.6346 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7124 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.6632 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7316 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.6803 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7191 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.6373 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7409 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.6447 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7483 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.6346 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7262 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.6776 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7501 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.6387 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7542 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.6472 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7130 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.6490 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7110 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 186/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6128 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7098 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.6470 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7579 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.6692 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7459 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.6243 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7100 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.6602 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7072 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.6571 Acc: 0.5820\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5820\n",
      "val Loss: 0.6993 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.6531 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7139 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.6502 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7231 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.6406 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7012 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.6396 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7021 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.6696 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7433 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.6690 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7206 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.6361 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7313 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.6577 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5861\n",
      "val Loss: 0.7110 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.6565 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7155 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.6687 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7175 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.6167 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7131 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.6505 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7013 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.6843 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.6913 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.6278 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7053 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.6933 Acc: 0.5697\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5697\n",
      "val Loss: 0.7468 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.6584 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7059 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.6474 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7659 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.6607 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7129 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.6266 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7219 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.6416 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7380 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.6729 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7161 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.6790 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7262 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.6025 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7073 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.7165 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7957 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.6690 Acc: 0.5820\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5820\n",
      "val Loss: 0.7267 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.6558 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7563 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.6531 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7544 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.6520 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7191 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.6403 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7217 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.6507 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7168 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.6601 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7192 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.6988 Acc: 0.5779\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5779\n",
      "val Loss: 0.7806 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.6709 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7070 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.6330 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7332 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.6567 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7661 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.6519 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7637 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.6417 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7114 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.6557 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7608 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.6630 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7173 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.6571 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7157 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.6688 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7433 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.6404 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7185 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.6747 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7107 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.6607 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7330 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.6516 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7254 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.7103 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7382 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.6711 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7415 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.6572 Acc: 0.5533\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5533\n",
      "val Loss: 0.7220 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.6537 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7169 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.6356 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7460 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.6559 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7181 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.6331 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7064 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.6351 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7031 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.6558 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7114 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.6481 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7130 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.6662 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7110 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.6865 Acc: 0.5697\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5697\n",
      "val Loss: 0.7187 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.6578 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7189 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.6648 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7327 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.6540 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7500 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.6313 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7017 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.6671 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5861\n",
      "val Loss: 0.7021 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.6210 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7042 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.6353 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7241 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.6638 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7194 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.6418 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7087 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.6647 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7447 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.6524 Acc: 0.5820\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5820\n",
      "val Loss: 0.8019 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.6469 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7009 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.6785 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7354 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.6735 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7165 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.6488 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7705 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.6729 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7099 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.6756 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7436 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.6403 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7096 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.7073 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7049 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.6330 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7164 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.6595 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7151 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.6606 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7067 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.6861 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7154 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.6573 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7183 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.6335 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6803\n",
      "val Loss: 0.7078 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.6681 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7707 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.6407 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7011 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.6696 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7542 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.6825 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7353 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.6644 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7273 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 279/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6549 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7115 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.6913 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7154 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.6461 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6895 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.6299 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7437 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.6879 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7598 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.6917 Acc: 0.5697\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5697\n",
      "val Loss: 0.7541 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.6121 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7557 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.6335 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7054 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.6586 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7192 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.6511 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7459 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.7027 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7268 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.6700 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6980 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.6583 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7162 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.6322 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7237 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.7055 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7280 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.6156 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6855 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.6683 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7527 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.6496 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7066 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.6391 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6983 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.6742 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7771 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.6509 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7141 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.6351 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7086 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.6479 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6025\n",
      "val Loss: 0.6935 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.6442 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7099 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.6917 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7893 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.6513 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7262 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.6874 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7593 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.6759 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7160 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.6754 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7036 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.6275 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6721\n",
      "val Loss: 0.7210 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.6707 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7357 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.6325 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7372 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.6252 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7076 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.6887 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7247 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.6602 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7469 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.6698 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7736 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.6982 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7391 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.6659 Acc: 0.5820\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5820\n",
      "val Loss: 0.7120 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.6261 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7581 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.6474 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7527 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.6302 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7229 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.6392 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7577 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.6534 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7090 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.6640 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7513 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.6506 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7157 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.6292 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7084 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.6524 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7333 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.7005 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7479 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.6357 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7607 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.6325 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7272 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.6956 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7213 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.6447 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7053 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.6570 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7095 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.6514 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7594 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.6715 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7168 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.6482 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7326 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.6327 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7014 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.6583 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7117 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.6486 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7378 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.6344 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7230 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.6846 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7173 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.6760 Acc: 0.5738\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5738\n",
      "val Loss: 0.7330 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.6642 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7171 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.6762 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7693 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.6615 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7505 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.6691 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7535 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.6959 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7667 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.6434 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7314 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.6952 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7058 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 348/499\n",
      "----------\n",
      "train Loss: 0.6516 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6968 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.6528 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7349 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.6342 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7085 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.6664 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7168 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.6561 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7537 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.6389 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7412 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.6557 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7217 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.6655 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7611 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.6573 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7254 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.6537 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7085 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.6406 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7205 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.6310 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6970 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.7333 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7298 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.6635 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7001 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.6720 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6967 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.6788 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7237 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.6169 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6980 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.7080 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5861\n",
      "val Loss: 0.7183 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.6596 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7236 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.6578 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7286 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.6362 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7023 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.6702 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7094 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.6441 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7095 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.6849 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7174 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 372/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6125 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7076 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.6881 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7211 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.6642 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7291 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.6563 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7266 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.6410 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7282 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.6474 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7037 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.6540 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7132 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.6592 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7469 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 380/499\n",
      "----------\n",
      "train Loss: 0.6409 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7397 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.6595 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6946 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.6388 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7017 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7354 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.6821 Acc: 0.5779\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5779\n",
      "val Loss: 0.7405 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.6414 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7209 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.6495 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5984\n",
      "val Loss: 0.8013 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.6318 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7313 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.6370 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7180 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.6559 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7623 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.6349 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7116 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.6636 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7450 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.6622 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7018 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.6592 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7071 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.6853 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7124 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.6468 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7126 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.6405 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7104 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.6433 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7042 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.6285 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7190 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.6474 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7031 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.6900 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7269 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.6717 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7279 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.6719 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7155 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.6456 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6910 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.6289 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7393 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 405/499\n",
      "----------\n",
      "train Loss: 0.7000 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7259 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.6484 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7265 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.6696 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7089 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.6339 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7212 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.6628 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7076 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.6711 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7137 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.6378 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7090 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.6693 Acc: 0.5820\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5820\n",
      "val Loss: 0.7401 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.6455 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5861\n",
      "val Loss: 0.7085 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.6575 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5943\n",
      "val Loss: 0.7354 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.6702 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7163 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.6691 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7491 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.6471 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7049 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.6345 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7374 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 419/499\n",
      "----------\n",
      "train Loss: 0.6955 Acc: 0.5820\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5820\n",
      "val Loss: 0.7294 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.6688 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7137 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.6539 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7383 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.6902 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7459 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.6654 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7300 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.6699 Acc: 0.5820\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5820\n",
      "val Loss: 0.7732 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.6716 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7368 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.6782 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7212 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.6502 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7042 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.6604 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7214 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.6890 Acc: 0.5779\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5779\n",
      "val Loss: 0.7171 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.6297 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6598\n",
      "val Loss: 0.7567 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.6543 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7080 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.6303 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7058 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.6802 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7070 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.7112 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7269 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.6420 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7777 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 436/499\n",
      "----------\n",
      "train Loss: 0.7046 Acc: 0.5779\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5779\n",
      "val Loss: 0.7605 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.6616 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7158 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.7032 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7173 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.6315 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7210 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.6196 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6639\n",
      "val Loss: 0.7396 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.6181 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7216 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.6330 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6516\n",
      "val Loss: 0.7299 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.6641 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7404 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.6491 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7328 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.7039 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7147 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.6422 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7071 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.7233 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7058 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.6155 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7128 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.6289 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7145 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.6540 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7059 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.6867 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6025\n",
      "val Loss: 0.7084 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.6442 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7057 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.6550 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6991 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.6600 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7151 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.6362 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7054 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.6501 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.8258 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.6626 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7244 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.6190 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7193 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.6521 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7452 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.6964 Acc: 0.5697\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5697\n",
      "val Loss: 0.7358 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.6497 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6557\n",
      "val Loss: 0.7907 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 462/499\n",
      "----------\n",
      "train Loss: 0.6829 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7342 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.6385 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7377 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.6687 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7084 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 465/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6794 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6762\n",
      "val Loss: 0.7182 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.6493 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7075 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.6927 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7048 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.6596 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7077 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.6103 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6815 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.6535 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7236 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.6656 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7837 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.6721 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6680\n",
      "val Loss: 0.7215 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.6300 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6869 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.6693 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6900 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.6592 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7121 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.6453 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7402 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.6967 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7222 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.6392 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6066\n",
      "val Loss: 0.7136 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.6859 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5984\n",
      "val Loss: 0.7189 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.6948 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7318 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.6159 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6865 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 482/499\n",
      "----------\n",
      "train Loss: 0.6991 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7610 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.6492 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6909 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.6491 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7004 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.6256 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6270\n",
      "val Loss: 0.7294 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.6490 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7138 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.6529 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6230\n",
      "val Loss: 0.7010 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.6434 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7220 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.6404 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6434\n",
      "val Loss: 0.7450 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.7023 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7350 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.6981 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7086 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 492/499\n",
      "----------\n",
      "train Loss: 0.6570 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6189\n",
      "val Loss: 0.7172 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.6773 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6352\n",
      "val Loss: 0.7391 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.6588 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6475\n",
      "val Loss: 0.7200 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.6832 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7560 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.6408 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7411 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.6665 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6311\n",
      "val Loss: 0.7431 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.6428 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6107\n",
      "val Loss: 0.7195 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.6510 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6148\n",
      "val Loss: 0.7336 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.6797 Epoch_Acc: 0.6405\n",
      "\n",
      "Training complete in 36m 35s\n",
      "Best val Acc: 0.679739\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydd5hb1bW3363pvY89HnvGvWMwGFMMmJKA6RAIweGmkAKBwE1Iucn90tu9KSS5CSVAOknAAdNDC4QSwAZsY9x7H0/x9F6l/f2xdEYazZF01OWZ8z6PH42kI2mP52idtdf6rbWU1hobGxsbm7GLI9ELsLGxsbGJLbaht7GxsRnj2IbexsbGZoxjG3obGxubMY5t6G1sbGzGOLaht7GxsRnjWDL0SqkVSqldSqm9Sqmvmzz/S6XU++5/u5VSbV7P/VQptU0ptUMp9WullIrmL2BjY2NjE5jUYAcopVKAe4APAjXAOqXU01rr7cYxWus7vI6/HVjs/vlMYBmwyP30m8By4LUord/GxsbGJghWPPqlwF6t9X6t9QCwCrgywPErgYfdP2sgE0gHMoA0oCH85drY2NjYhEpQjx6oBI543a8BTjM7UClVDUwDXgHQWq9VSr0K1AEKuFtrvSPQh5WWluqpU6daWJaNjY2NjcGGDRuatNZlZs9ZMfRmMXV/fROuB1ZrrZ0ASqmZwDxgsvv5l5RS52it/z3iA5S6CbgJoKqqivXr11tYlo2NjY2NgVLqkL/nrIRuaoApXvcnA7V+jr0eT9gG4Grgba11l9a6C3geON33RVrrB7TWS7TWS8rKTC9INjY2NjZhYsXQrwNmKaWmKaXSEWP+tO9BSqk5QBGw1uvhw8BypVSqUioNScQGDN3Y2NjY2ESXoIZeaz0E3Aa8iBjpR7TW25RS31dKXeF16EpglR7ZDnM1sA/YAmwCNmmtn4na6m1sbGxsgqKSrU3xkiVLtB2jt7GxsQkNpdQGrfUSs+fsylgbGxubMY5t6G1sbGzGOLaht7GxsRnj2IbexsZmbOJywXt/gaH+RK8k4diG3sbGZmyy/Ql4+jZ485eJXknCsQ29jY3N2GSgW27L5yV2HUmAbehtbGzGJoO9clt1ZmLXkQTYht7GxmZs0lkvt60HE7qMZMA29DY2NmOT7mNyu+FPCV1GMmAbehsbm7HJ5XdBQRX0tiR6JQnHNvQ2NjZjE4cDiqqhxzb0tqG3sbEZmzx0PRx8A3pbE72ShGMbehsbm7GHcwh2vyA/26EbSxOmbGxsbI4vuo8BGqacDqUzE72ahGMbehsbm7GHIa1c9gWYe0li15IE2KEbm+OX9x+CGnt2gY0JXQ1ym10MbUfGfb8bS4ZeKbVCKbVLKbVXKfV1k+d/qZR63/1vt1Kqzeu5KqXUP5VSO5RS25VSU6O3fJtxi9bw7Jdh6+OJXolNMmJ49A1b4f8Wyu04JmjoRimVAtwDfBAZFL5OKfW01nq7cYzW+g6v428HFnu9xYPAj7TWLymlcgFXtBZvM47pa4PBHuhuTPRKbJKROZdAYRWkZsr9nvGtvLHi0S8F9mqt92utB4BVwJUBjl8JPAyglJoPpGqtXwLQWndprXsiXLONDXTUye2WRxK7DpvkJG8CzLwAcsvl/jhX3lgx9JXAEa/7Ne7HRqGUqgamAa+4H5oNtCmlHldKbVRK/cy9Q7CxiYyO2kSvwCaZ2fAn6UWfVSz3x3nRlBVDr0we8zdR/Hpgtdba6b6fCpwNfAU4FZgOfHLUByh1k1JqvVJqfWOjvRW3sUCnl6F32dFAGx82/Bm2PQ5ZhYCyPXoLx9QAU7zuTwb8uVPX4w7beL12ozvsMwQ8CZzs+yKt9QNa6yVa6yVlZWXWVm4zvime7vl5oDNx67AR1t4D3y1IHnVLVwPkTgRHCpTOAkdaoleUUKwY+nXALKXUNKVUOmLMn/Y9SCk1BygC1vq8tkgpZVjv84Htvq+1sQmZqWfBlffIz71tgY+1iT3v/lZu+7sSuw6QHV5Xg8TpAW5bB8u/mtg1JZight7tid8GvAjsAB7RWm9TSn1fKXWF16ErgVVaa+31WicStvmXUmoLEgb6bTR/gZhQ+z48fTu4nMGPtUkMx3aASpHKR23/nRLOhAVQPh9yShK9EgnTuIbEo7cBLFbGaq2fA57zeezbPve/6+e1LwGLwlxfYtjxDLz3IJz5Bbt8Oll57DMin/v0i4leiQ1A+xHILITu5sQbe0NDbyhunv5PaDsMH38ycWtKMHZlrBmzPii3rQcSuw4b/3TUQl6FFE7ZO6/E014Dh9fAa/+T6JVAdgl84Lsw6SS5P9QHLfsSuaKEYxt6M5r3jry1SS4Ge2V77kiBH5TCe39O9IrGNy4nzLpIfm7Zn9i1AORXwFl3eBL2WcXjvmDKbmpmxpq75NY29MlJp7tYqmyuxGLtfuOJxZECV/8GnP1wdEOiVwMN26SgbuYFoJT0uxnohKEBSE1P9OoSgu3Rm2FUXdqGPjkxiqWKp0uJu23oE0tfu/xNiqZJLHxoILHree9BePSTYuQBsorkdhyfJ7ah96W/C/rboXwBnHRDoldjY4qCipMkGZtVZMsrE832p+AX8yAlHbRLjH0i6az3SCtBPHoY10VTtqH3xQgLLPsCLLousWuxMWfqMrj5dSiZ4Tb049dTSwraawAF086BwmppOJdIjGIpg1kXwZd2QunsxK0pwdiG3hcjLJCeLTLLrmOJXY/NaJyDnp+ziqDfroxNKO01ooCqPgO+uBkmL0nsenw9+oxcSdA6xm+bLdvQ+5JVCIuul2Kcv/8HHF4b/DU28WX1jfDAefLzx56Ejz+V2PWMd9qPQMFkz33trxVWnOg6NtKj722Tuos9LyduTQnGNvS+VJwIH7pftqEATXsSux6b0XTUQmaB/Jya7km62SSG9hqPoX/4o/BQAkOezkGYfRFUerXUSkmDLY9Cw5bErSvB2Ibel7bDorrJyJXtaPP4LrRISjrqIH+S/Lzhz/DHSxO7nuMJ5xAcfAsad4v+ffOjcGhNZO+ZWQhlc+Tn1Axo2h35OsMlJQ0+/Ec44VrPY2nZkJIxrlsV2zp6X57/ulTE3roWSmbaEstkwzkEXfUeQ999DA69KV0TUzMSu7ZkxeWEbU/Arudh70sih1x6M1z8E3jx/8H0c6H6zPDf/6ZXPT+XzIDtTyZOsz7QLf+yS8Hh9mMNLf04TtrbHr0vne7SepCT1jb0yUX3MZHwGX+jzEK5tSWW/tn1PDz2adj/Gsy9DK57EM7/phjAqcvg0Fvhx9V9X1c8PbESy13Pw52zRu8qxrk6yzb0vniHBaafC/MuH6nysEks3Y3gSIV895AzuxgmOMd2yO0XN8NV98L8KyEzXx6rXgYdR6H1YHjvveMZ+Ol0CQUBFM+Q20S1QuhqkFtv1Q242yDYoRsbEIPe1eAx9Auuln82yUPFifDNRvEawWPoE63dTmZO/TRMOxvSc0Y/V71Mbg+9BcXTQn/v9hroaYacUrlv9JdpT5BH31kv8Xhjp2dw7tcwH5Y3PrANvTddDYD2GHqtZQuamgF5dm/rpMHhYHgzmmWEbmyP3i/ZxVB1uvlzZXPF2z34Fiz+j9Dfu70G0nI8F9ycUvjaIc/fJd4YA0d8lViGim6cYoduvBnokdYHRVPlvnMQfr0Y1v0uocuy8WLNXfDgVZ77pXPgxuf9G7LxjtYyRGevHw25wwEXfBsWfii89zc09IZhVSpxRh7EozcbOFL7Pqy5O/Ea/wRhydArpVYopXYppfYqpb5u8vwvlVLvu//tVkq1+Tyfr5Q6qpS6O1oLjwlls+HWNRKbB1ENFFXbCdlkom7zyN7iGbmiGDE8yljickrRTY27Q+NQf/L3wu86Jk2+AsmEl9zomcEQKt4aeoO198BDHwnv/SIlNVNEFL4ceB3++Q0YSIJRhwkgqKFXSqUA9wAXA/OBlUqp+d7HaK3v0FqfpLU+CbgLeNznbX4AvB6dJccQs6t9vCWWnfXw7ztl7uXxymCfJOnuO0t6x0eTjlpPItbg1f+F3TGcNNWwHf75LfjFfPjbNfD2vRIqeuBcMWrJTLO74K8kwKQ05yBsWQ1H1oX+/h21ow19TwvseSkxIoYbHoGr7xv9eJa7sdk4Tcha8eiXAnu11vu11gPAKuDKAMevBB427iilTgEmAP+MZKFx4aVvy5fZ2+CXzBRvKF6G99kvwys/SGzRSaS8+iNpH1G/RbbM0cRb/mrw7v1iWKKN1vDnK+A3Z4hxn7QYrvuLKFcyCyXx+MoPoXFX9D87WjRZMPQqRc67cAa43LEVLvzhyMdKZsgc30R3sfTGagfLv14Dj94Y+/XEGSuGvhI44nW/xv3YKJRS1cA04BX3fQfwc+D4GMHeUSuVdd6JnJIZMNjj6WoZa3LKxPsonxufz4s2g72w8S8eNUfNu9F7b63dHv2kkY9nFsZGdTPYK+GZBVdL98OProL5V0hyXim47JeiZHnyFinkSkaa90o4o2CK/2McDgl/HXor9PdPSfNINQ0M5U28JZZth+HOObDjH6Ofs+rR730ZtvkGJI5/rBh6M02Sv4zG9cBqrbURuLwVeE5rfcTP8fIBSt2klFqvlFrf2NhoYUkxwiwsUD5fPLn+jvisoWWf2yM6TpNGWx+XsMa5X5cv/JEoGvreVpn/6WvoY1UMk54tw8c//CfILRv9fG45XHqnTFVae1f0Pz8aNO0RbbsjyFe9epkY5o4QHJqa9fDglR4NvYGhpY93+5DOeqmaTjGpyB326AOcJ4N9np+P1++fH6wY+hrA2x2YDNT6OfZ6vMI2wBnAbUqpg8CdwMeVUj/2fZHW+gGt9RKt9ZKyMpMvVLwwCwtUnwk3vQbl8+KzhpYDULMOVn8qPp8Xbdb9ViR7U8+GyUvF0EfrS5ORD7esgYXXjnw8VobeORR87Qs+JAVIr/4PtB6K/hoiZdkXRFUTjKleenqrNO6SatsUH5V2Timk58Xfo++sl1vfYimA3Amw9CaZguWPtEy46H/l5zHWntyKjn4dMEspNQ04ihjzj/oepJSaAxQBw319tdY3eD3/SWCJ1nqUaicp0Fq8mXkV5s8N9cuJEEsG+9xDHIBj22P7WbGgZgPUboRL7pTQxpRTYfMq2VIXVUf+/impMGHB6MezCqU/UbTZvAr+cQfc/h4U+gl9KAWX/kJaCxRWRX8NkWIY8GBMXCQX0oNvjmwIFgjjXPXdBSsFn3g6cLgoFhhVsbkmhj6rEC75WfD3mHkBpNw55mbLBvXotdZDwG3Ai8AO4BGt9Tal1PeVUld4HboSWKX1cbrn6W0F58Dokxbgj5fAIx+P/RqUguv/BnMukW3v8dZ6ob9dwlyL3NK6hdfAl3ZEx8gD7H9dms71+0jkTrgOzrw9Op/hTUednBM5QXaZOaUyjUwpaAsYpYwvHbWw9l7PMJ1AOFLE8597WQjvXyNG1ayZXOXJ5uGuWNLVAMrh/+/VuCtwOGnj3+D5r8Hij8VHrhtHLFXGaq2fA57zeezbPve/G+Q9/gT8KaTVxZPsYvhWI7hMkmq55aIgiTWpGTD3UpmYtOs5CeOUHUfjz2acL/8Msoqi+4U5vBbeuQ8u/MHIx+esiN5neNNZJ0k8qzu59x4U9cpNr5nvPOLN0ffgxf+GqtNG5zXMWPrZ0N7fTENvcOAN2PQwXP4rSdjGg856MfL+Jkk9dB1MPhWu8VMAedS9I933irzPlFNjt9Y4Y1fGepOSBmlZox8vmSlNn2LtYR94A979LZTOkvuNO2P7edFk/+vm3tLb98l0n2jQcVQuur6Go/0o7Hx2ZDItGnTWj87ZBGLOpTIQ5enbkyOZZ0VD783QgOjpj75n7fhAhr7tMLz/t/hKLC/+KXz2Vf/PB2ts1npA+v089xVY/4fory+B2IbeYPtToiAwS+oZuuBYJ9u2PiZJvTK3tDLcjoLxxuWEp26DZ74w+rnuRlHiDHRH/jnenUW92f8arPpo9CWwnXWh9TjKKYHlXxPPsH5zdNcSDk17IafcM40rGErB0/8pBtoK1/4BlvtJuSVCYpmeDQWmym8huziwjr5lv6y7aOrx892ziG3oDRq2iVeanjv6OcMjinWFrHGipefA1w/Dsv+M7edFiz3/lG6Fp5p47lNOk4tk7cbIP6ejFvJMDH2sOlj2tYXm0YPkJRxpsPmR6K4lHJr3eHaHVkhJkzDPQYvKm4knwIT55s+VJKBd8WOfhfcf8v98II/eOSj5laJptqEf03TUmocFQAy9I02801hiGHqw7oUlA+/+VgziXJORfpOXyG009PSdJsVSELsOlre/J0VRoZBdLDNLtzya+D44TXush20Mpp4FjTuguznwcS0HpC1Eix+1U06ZOE3x0tK7nLB1deALS6ApU22HxSExPPrO2uiHAhOIbegNOuv8e2/ZxfCNejj5Y7H7fENaaXhCm1ZJL5VEG4tgNO+Dff+CU240v0hmF0PJLKkNiAStReN8wodHPzc8fCTKHr1S4cnsTv0MnPpZkeQmCpcTTr9VBueEQvVZchtMT39sO6z5tf9dlFJiNOPl0Xe7ZxSYSSsNSmZKPYxZO5OCKXDzv+UibXSvTaYWDhFi96M36KgNXEzhWxQSbdoOAdrj0TsHJNzRdji8gRDxYt3vZeLTKZ/wf8yUpbD7BTHWvn3CraIUnLTS/LnMGHj09Vsl53DxT2HyKaG9dsZ58i+ROFJgeRidRyYthtQs0dPPv8L/ccMaej/JWJBxhWnZoa8hHIaLpQLkVJZ+1r+yKDVdhtqAhKQWfyz23/k4Ynv0Bh21kB8gHvvmL+GBGH5503MlsVXpNipGQjaZG2YBnHYzXPWbwF+wc74q3lK4Rh4kEf7+Q+Yx1qwimHVh6PH0QLQdhqPrw19zR510IfXV/MeLpj1irEPtwZOaLnr62RcGPq79iExyMiZLmTH7IplsFQ+Gi6WCJM+1NldEbfybdEEF8fqvvNvjdI0BbENv8LEnZKvrD5cTat+L3Re3oBLO+29P6KbUrZ9PdollUbUUCwWieJp/GZ5VDr8tzcN6TGLHaZlww6Mw95LIPsMbQ8ET7sWj9YB0Id31XPBjY8HGv44c0BIKZ9wKMz8Q+BhDWhnoQtheA2//Jj7tBAK1PzA48Ab8oEzOJV92PA07vZqhtR1J3NzbGGAbeoPKk80HFhgYSa1Y/fGPrBupX84qFCOTzB5922HxgqzITl/6juyKwqXTXd3pz/AO9kY3Rt9ZF7jKMhhTToeCKtj89+itKRSa98oFNpzww2CfSH0DFQm2Hw1+8W47Ai98XYbFxJoZ58OH/xzYo8/IBdegucSy5YAnNg9SXPXiN6O+zERhG3qAYzulF337Uf/HxFpi+coPpFDDm7I5ye3R122G139s7mWPOnYTbHks/M/qqIWMAvmymvHb8+Gpz4f//r501kliL9w4rcMBiz4sVZadDdFbl1Wa9kgSPFyevFV2Bf44/RZpEhaIYS19HJQ3hVNgwVWBk+f+WhW7nO5iKa9QTSgSy6GBpFfo2IYepLjlrV9J33l/GCdBrORiLQdGxwQv+TmsfNj8+GTA+CJ4e0L+mLIUjm2T9g7hECyHklUUZY++PvKB8Is+IkqQrRFc4MLBOSQ7z9IQpZUGaZmS89j8iH8DtvBDMC9IX5zccsk9xSMEsvlRqeoNhL9WxR21In7wFj0Yhj5YhfMTn4MflsX/bxwitqEHKa2HwPHY9GxRGMTCox/ql+RWsU/oqHRm5MYmlrQdEi/bSj+byUvF6Fktr/fFbOCIN9FuVXzpL+DKCMcEls2BipPiH75pOyQhikg8+lM/LSGO7U+Ofq63VQxrsJ2KUmI846Glf/d+6TUUiPRcUYj5hm6Mzqe+Hv1gN3Q3BX7Pw+5mvdGYCDc0EPl7+ME29CAKiUBhAYMbnw29gMYKrQcZIa00aD8KT9wS3izPeNB6EIqqrClTjMKpcCdOzV5hXpBlkFUYXUNfVB2dxmQrfhz5BSNUtIZ5V0DFovDfY9pyuVCsM2kA1rAdHv+MtVba8dLSdzYEd4qUkvCN786vZBZccTdMOMHzmLFLDRS+6evwPB+pA+hywd2nwGs/iex9/DB2hKKR0HE0cFjAwEqIIhyML4JvMjg1AzY9BBMXJmcnvdZD1rtrZhWKZDTcCtlzvxb4+WiOExzql3a1i66TwTORUH1GdNYUCqUz4SN/iew9lBKv/oWvS37F0JiDR0Nvpd/8gqtHvjYWaC2TpQIVSxl8cbOMVvQmv2J0MWTxDFm3K0AjQ6OfUWpW5B794TUibggkCIkA29CDJN6stHE9+BasuUuGQxvxvmiQkSc96H09+pxSyC5JXuXNGZ+XOKxVrrpXmmyFykCPVx8gPwU4OWXSI2hoIPKhEZ11sOGPUtMQqaEH2PmchBWufyj4SL9o0HZEWmj4znINlRNXyvei3KefTbu7536gBmIGC66ObA1WMGZJWAlzmnWn3faknDuzPuh5rHSm1H4Ewhh8f8I1sOnv0i8n3JbMWx6FtByYc3F4rw+CHboB0c+bNeTyZaAbdj8f/d70U8+SpKvZxaNsbvIa+lM+EdqJWXmK/0lNgTi2He5bBgcCfPHO+iJ8dW90JgMZmmwruzwrDHTJeWPEc2PNE5+Dv14T+ftkFcqYxFFtoWvEATEzmmbUb4lt219Dp2/Fo//3z6T5mTdv3Cn9mswIpKap2yRN9qqXyf9VV5jqqqEBudjMvVQuODHAkqFXSq1QSu1SSu1VSo3qS6qU+qVS6n33v91KqTb34ycppdYqpbYppTYrpT4S7V8gKpxwbeD4r0H1GTJubcOfovv5rQf9t/E1JJbJ0N/cm45a2PEPiVNapb8Lnv0K7HohxM9yJ8ut7LqiQaTFUr7MvVS8tXglZUPtWhmIwT549MaRsfpAfejNeOd++btHo4OpGRl5cPaXZRxiMFoPwcE3PPe1diveTNqM/PWawBfMC74NH/4jLLpenIxwiwJ7mqXL66LYmceghl4plQLcA1wMzAdWKqVG7OW01ndorU/SWp8E3AU87n6qB/i41noBsAL4P6VUYTR/gYjpaxdZVoeFXuYZeXDKJ0WJEM3e9H+63LyXO4hH39eWfMOKD7wBf78hNC8mLRu2PBJ6tajxtwlk6I9ugLuXytzaSOmIsqFPz5HmYtuejL3euq9D/iahdq30R1qmxI7fud/jbFSfKcleq1z4AwmtPfn52ChL8irE6FrJF2W7WxUbv0t3o+y4zNod5JQFTsYWToGq0yMPx+VXwA2PwKwg1cgRYGWFS4G9Wuv9WusBYBVwZYDjVwIPA2itd2ut97h/rgWOAXEeJBmEpj3w2KdlG2aF0z4nFZNv/yY6nz8srfTTV2P2CvjoI3KRSSZaDwIqtAHQDoeMcgu1k2XHUUhJl3CBP5QDmnaFv332prNO+rhEcwziCdfKTN1A4adoYEyVipZHDxLWbNrt8YTP/hKc85XAr/Emqwgu/z+po3jj59FbF0iPmgevsH4BzSoGZ7+nZsYQQpg1NCyaKueeWRfS+i2ySzGKLB+8Uu6HSn+XDM6JcZdaK4a+EvCeeFzjfmwUSqlqYBrwislzS4F0IE4Nqi1iDE62Go8tqISF10obV7N2p6HSanSt9JNtL6qW5lD+kpD+aDsCr/1YpjvFgrZD4klZnadqUL1MYu6HQohXGy2kA8k4o9nBct4VcMnPImvC5su0c0THfeD16L2nGU1umV8kGnpfFlwtxnrd78Qjb9wV+s5kzsUyxP2NO6OT43K5pJr9qVvlIu+02BI626c6tsVEQ29QNBXQ5gPfD74F637rmU87NBDe77XreblIRGNeQwCsGHqzs91fwPh6YLXWesTlSSlVAfwFuFFrPco6KqVuUkqtV0qtb2yM8XAPX4bjsSHEfy/+iQyAjoaCwigPD9Qp770Hg1f9GTTvg4dXwq8WwWv/C89+KTZ90VsPhic3XfpZKKyGJz9nvUFcdqnsBAIx3JM+CoZ+yqmB2y6HQ2oG3PIWXPjD6L6vL9ol51I0W1unZUrb3h3/gIP/hnuWSnI5VC7+ibviNMKwZ38X/P0/pJp9yafgPx6zPqjHaINgFE2VzJDZAYVVo48NpKWve1+Sv4bSp3SWZzcVClselULMKaeF/toQsGKpagDv/flkoNbPsdfjDtsYKKXygWeBb2qtTdrGgdb6Aa31Eq31krKyOEd2rIQFfMkqlCt5y4HIjag/Db037/0lcAK4uxkOvyM/p+dKGOqsO+CKu8TwxaKDYriGPiMPrr5PKg6tJucu/jFc+/sg75sPqOho6besjk3isGiq7BJimVg/aSX850a5sESTJTeKymaHu8NjKCE7g+xiuPWd4K0TAtHTAn9YIReai38mFcyhSBqrzoBPPufZQU9ZCpfeaa7WKpoqlbRmk+V8awtKZ0lSNdDwcbPfZd+/RJ4ZY9mtFR39OmCWUmoacBQx5h/1PUgpNQcoAtZ6PZYOPAE8qLV+NCorjjYd7gHQof5H12+B+8+RirrFN0SwAAVl8wLHg8vmyOAOM9pr4P7lcrG6Y6u0af3iVvl9XE5JmoajXQ+E1tLGNlyNefWZslYrMXCt5fcI1lzM4YhedewzX4CTPy5DOKKJyynb9KrTZShHLHAOxWZgRvF0+MoeKeCD8BUmKanikb/yAxE2lM8bfYxzSDzu3lb3vza5nb5culNWnAgf/G7wVspm5JRAzjLP/UNrJRxr5tHnToBvHvOEZwwGekQJN9frgmW0FW/aI3N3rbD9SXANmU9NizJBrZvWegi4DXgR2AE8orXeppT6vlLKO/W+Elil9Qh35TrgHOCTXvLLk6K4/sipPEUGOofKhIVSSLLmrsg8tDNuhc+/HTgeXDZXvArfOZ4uJzx+Ewz1wfV/85yQxkXLkQLX/BamLiOqKAVX/BpOvD7898gqkjjvq/8bYGDzELzw3/CDEmuhq0/9E879f+GvCaTp2kBXbHoMGX+fHc9E/71B4tY/qYbXfxab90/PFkMGkTkPQ33y93zyVnGYtj4m54ExJOVv18KdsyRE9IeL4OGPSKiv9n05t6+6JzwjD9LO+tX/gUNr5P6qj/pvn63UaCMP0LBVQmTeHv8lW/4AACAASURBVP1wd9sQwjdbVkPpHLElMcbSpV9r/RzwnM9j3/a5/12T1/0VCNDrNAk4/XPhvU4pOPN2eOJm2Puv8KVRLqf5yeRN2Ry5bdoFOV5e9Bs/l6Tw1fdLP31/HPi36PSjVXXX0yLGMH9yZFvO5j2SnGveOzo009MCqz8F+1+VdrjzAwm93FhtxxCI4QEWUZxW5c3cS6WtQPO+6Je7d9TI3yUnhDBkqLxzn9xG8nfPKZVk9+ob4T73jFrlkLBT0VTJ48y9VJyB4X+FoeXR/KFS4PWfSEimfJ7sHAKNEP3X9+VidINXQKJkJlzzewkDGRRNhS/ttO4gaC26+bSs6Cb9/TC+WyBoLR5KweTQVS0ACz4EL39PhiSHY+iH+uHHVXDBd8Sz94dh6Bt3ecIlh98WVc2ijwT3rF/7iQzumL0iOifVtsfh2S/LiR1J9ejEE2D51+DVH8kXe+GH5PGG7bBqpSiirrjb+lD2db8XeeV5EXj1w8n5GHUNnXOJGPpdz4mjEE0Mb7s0Chc8f3zimei0g15wtXjFSolXWzLTo+CyUrwYLqnpkJ4njkQgxY3BQLeEd7znHWcXi1zWG0dKaN8FpaKf8A/A+G6B0NcG95wafnl2aroMYDjwunUdvjeth2QbG6xvTv5kOO+bI7323jbpTnjJncE/Z/ENkvSNVgl+60FpDGWl5DwYZ31JwmfPfkm8aeeQGPnBXvjks9aNPMh2fEuEqaBoF0v5UlQtW/VdYahWgmG0A46mtNKXaecEHhpuFaXEWC68Rpr2hSrTjYTsIvHkh9sTB/Doi6bCQOfI8OKbv4T9JjLZd38rVcRWeOPnMoA+ToxvQ2+l4jIYp3wCzrgtvJFzhuLGn4bewOGA5V8dGROcswI++6q1xlXzrxQ1zsa/hb5GM1oPSfIqGkqBlFS46j4x7E/dJp7RtX8Q+eqUpaG9VzR60hdMlpL2WBl6EK++9WB0q2S1llh33qTQGs2NR7Lc1bHDxVJT/R/rK7Ec7INXfmheD9FRK7NnnQE6XoLszP/1fRneHifGuaE3iqUiMPSZBXDRj8J7DysaeoOmPbBpFWz4s5SSD/ZZD8Ok58hWedsT0RluHq600h9ls+ED35M4rXaJhx/O/2dWobS0iKSQbeoy+ND9wWcTRMJZd4gyKppebE+z7FCXfzUuMd/jmuxi8ehzyuWiG6iRWGG13Bre/7FtopQxa71cOlueCzaCcMtqOdfj0dnTzfiO0XdGwdAbrPudeFVLPxv8WIOW/XKhsNLyeMcz8K/vSe/rqtNEThkKiz8GG/8ixj6UcIgZrYdEIhhNTrsZZl8oX4BwySqSC0V/hxj9cGjeJ7ufvCiEpfxh5IMGe613gAxGTincsib5mt8lI4s/Jv/3i28IHicvMgz9Qbk1WhNXmIgHjbYTTbv9t6DQGraulsEusTzHfBjnHr07dBNocrxVtj4B6/8Y2ms668Wbt+KBlc2V2/RsUdmEGjaZslReN+/y0F7ny2AvFE811z9HglLW/y/8Eaw6tutY8AK3J2+R3kexZu29cOds+f+MlINvQuNuCXvFQkM/1lj4ITHynfXBd3/pOXDr25KLA8nFZRaa6+4NiWVTAIllw1Zx8BZcFd7aw2R8G/rUdLkyR6OH+dxLZFtndXI8wEf+KglHK0xeIrH8qx8ITxGilKhzwvV0DdKyZCDDkk9F9j6xoOp0GdvnrxDrua/Cb4IUeVkdQhMpZXNk57H/tcjeZ6hfLk5Pfs725q3SdljCJz+fA2/+Ivjx5fM84Z2692HSSeYOSVahhIMCGfod/wAUzImhssiE8W3oz7oDbh6dVNl7rJMfP78THcoXx9Coh6KmUMr6oIHccvjP9yJrZaq1FCD5KxCxQoy77EVE8XRY/B/mFzOtxfNtPQTvPzz6eeOYzvr4DGSfera0bdhp8ULvjw1/EsN13jfs2LxVdj7n2bVZ6Qm0+VF42i2FXfYF6WDrj4/8Fc4dNbLDwymfkB72ufFt9TJ+Df3mR6UK0oSnN9Vx3+v7qO8IQRVRPF1aGVj94jbvk9YFB9+y/hmRopR87jv3h2+w19wl2v+BnuiuLRr0d4kRbzIZ1Ny4C3qaZAaoPy+up8U9ki6GihuD1HSp7tz9Qvh/i/5OeP2nInmccX501zeW8c6JBSqWMmjaDRv/Kh0qF14TuPCw6rTAU9TyJ8U1CWswPg39gX/LFHs/48Pq2yVuWtsWovxt7iWi5bbS2Kh5r2wDw50xGS6Lb5DwxL5RnaSt0XpQqgrDKTCLNQPdEsI48Nro54xe6ks+LV/cDpO+fNGeLBWMuZdKa4ua9eG9fu29cvG64Lu2Nx8KWV6G3opHXzRVkvzbnxQH0WjVYEbdJikmNLMBG/8mrZUTsCsef4Z+aED+EIXVngSLD3XtYuBr20JMlJ3ySfjMS57e6IEwiluCaeijzeyL5UTf+JfwXt92yCM5SzaMkI1ZMvbgm1J4ZqgszAaAOAegfEF4c23DYeYHRPduXGBCYahflF7zLofJp0R/bWMZ7xyOlcZ6hpT41R+J7QjUsqSzQf4uTbtHP7fhT5KTCdbyJAaMP0O/9m75I1xyp19pm2Ho69pDNPSFVaIBt6KIadgKGRalldEkNV28+u1PwXP/FXoCL9oa+miSmiHjCs1K9NNzZcs94QS50JlVNlaeDLeukb9hPMgqhC9tD0+BkZoh+aWL/if66xrrZIc4Ocy7aKpiUeDd07DE0ich21EHNe/C3AhVb2EyvrRYbYclpjn3MtFs+6F+2KMPo3Jx/2vw1q/h+of8F8R01qO3rKZ//rVkJmLLff63ASVVtaF8vssp03bmx1caFhKZheaG/qp7PD9PO0cqG737l8Do+/FAKSny0tq6IqqnRZyUeA1LH2tkl8puaunN1o7Pq/AU85kVSnlTWCVjKH09+l3u3F2k8uYwGV8e/Y5n5Iu14sd+D+nsG6SrX2JwIYduQEJD+/41ctK8L/texeUa4rP7zw79/aNBaroMbD7nq3L/7d9Iv59g3n3XMUB7ikiSEbM2CF3HRg6lPv1WuPTno3/fZ78MdweZZBVtelpETx+KEuqf3xKZaLBSextzMvNlKlUAZ28EDgec81/ys1mh1IhjU0SY4evR73hGdPZGg8I4M74M/Rmfh9vWBYzBGt68UlAbaugGxFtMywmsvjlpJV+o+CtrWgtwuRKsfdZadiH/uAMe+VjgRHJ+hQxiOHFl3JYXMguugqlnjXzsua/Cb7xayladJmEc3xBbR23oFceRkl0sXUXX/9GvCmwErYdg08Mw66L4J/LHM4bNmGRhnIbvWMG+DskRzbs8YUnz8WHoB3pEO6t10Mk4tW5DP6s8l7pwQjdpmTDzAtHTm1XdNe0Bl4udXTk4XZr23gR7ZUrB9Q/LLNNdL8jUrK4Ac3sdKdEfUxdNlv/XyJbPhn6+csnI43Y8A2/fN/IxYwh5vDnzduhvl5GRwVhzl4QRot3i2CYwZXNlJ2hFPHHqZ+ACr3Edmflw+waZq5AgLBl6pdQKpdQupdRepdSoagCl1C+9JkjtVkq1eT33CaXUHve/+DVg9uaNn0vrWwtT2g1p5SnVRTR3D9A3GIYUau6l0FU/eu5ofyf87gPoF75GnTss1NQVg8HdoeJwG45PPiszdNfebX7c2nvhwauSuwKzt3XkttnQz/t6+btegNd/PPJiHK9iKV8mL4GqM+HtewNL9zobZFD8idfL+Dub+DF5Caz4X2tCi+nLRw/KKZqa0JxK0FUrpVKAe4CLgfnASqXUfO9jtNZ3aK1P0lqfBNwFPO5+bTHwHeA0YCnwHaVUiCnvCGk9KINBTrhOMuZBqGvvQylYPEWWGVacftaFMslm78sjH1/3e+hro3vutXQPyAWkMRkMvUHVaZJo3fNP891I7XvSpyOZNdtv/Bzu88p9GLkSX0M/fblcFOo3y33nEHQfS4xHD3KhbT8iWm1/vH2vFHyddUf81nUc090/xJcf2cTRcL7DkdDfJRfkhu1S23H/OeJYJBArqpulwF6t9X4ApdQq4Epgu5/jVyLGHeAi4CWtdYv7tS8BKwA/Negx4KVvS4HPB79n6fD69j5KczOoKpGCoLr2PqaXhdiyNrsYPvempxEZSPOqtffA9POoyZ4HiAFq7howf49EccmdstU081xaDyV3IhYkGTvUK22c0zI9+nlfSei0c+T2wOsSd+1pElVFJBOzImH2Cjj98zKH2B/Tzhb5aLRHEI5RNh1p47H3atBofnFdHEdVa5e0TPjAdyXUU7cpel1Kw8RK6KYSOOJ1v8b92CiUUtXANMAou7T0WqXUTUqp9Uqp9Y2NAeLDoXLwLdGLn3WH5W1TbXsfFQWZTCqQP0zY3sCE+SON5Xt/EY/xnK+M2CUkRejGm5wSSfK1HRnduz6ZNfQGRrFanzt6mJ4jiVffXUjeRLkQG3r6vInwzUY48aPxW6s3Dges+B85b/wx8wNw7tfit6bjHKMe5qn3aznY1B2/D87Ml464TXth5z/E+aheFr/PN8GKoTfbp/sL0l4PrNZaG4FtS6/VWj+gtV6itV5SVhbFZj/v/Ea8uRASV/XtvUzMz2RCQQZKEV5CFsSDf/BKeOcBkfa99SuYcjpULxuhz086Qw9Sb/DrxbDea2D3QLdcqJLd0Pu2Kr7qXrjUz7jFactlvKIhvUxNj+9IOzN2vSC7UG8GuuGxz0oowMYyRq+qFIfinldN+h/FktJZ0s121wsy3CTB7aOtGPoawFuPOBkwaRQCiKH3DsuE8troc83vRS8bwraprr2PSYVZZKSmUJqbEV6MHuQzu47JjkIp6Wh3wbdAKerae0lxKEpy0pMvdANS9DF1Gay529Mvve2w+7njIHQDYui7Gkfq5305+eNwze/k562PSaK5ryP2awxE3SZxChp3eR5770HY8og1+aXNMPXtfeRnpnLDaVU8vvEoh5vj2IivZKaIMfrbE1Yk5Y0VQ78OmKWUmqaUSkeM+dO+Byml5gBFgPcE6heBC5VSRe4k7IXux2JLf6d7gHUGlM8NerhBV/8QnX1DTCwQr25SYVZ4WnqDOZeIx9jfKVOd3AnBurY+JuZnUp6fmZwePUgxVfcxj+SveAZ87q3k75KYUyZfMq3hua8E7j8/caEopFLTxVs+8G/rbaNjxamfkcHrhvJpaEAqravPkmS5jWXqO/qoKMjic8tnkOJQ3PtaHL360tmen6efF7/P9UNQQ6+1HgJuQwz0DuARrfU2pdT3lVLe4+BXAqu0VxN3dxL2B8jFYh3wfSMxG1Pe/CXcc5rI5ULAkFZWGIa+IDN8jx6km6V2wi/mjfDGatt7qSjIpDQ3ncZk9OhBYopTThfvcmhAjOHEhfHvzRMqExeKZrn6TLd+Pkjfmj0vSVuMzjrInZCQhlMjyCmBk26Q+cCdDbB5lYy8PPtLiV3XcUh9ex8TCjKZkJ/JylOnsHpDDUda4uTVV58pucFb1iY+HIhFHb3W+jmt9Wyt9Qyt9Y/cj31ba/201zHf1VqP0thrrf+gtZ7p/hfirL0waD0kIYf5V4asiTaSNxXuROykwizq2vtCG0DiTcViuR3qk2pZr8+pKMyiNDeD5mT16JWCc74CHTWw+e/i2b/yw0Svyjr+9PO+HHoLXv+JtI1OlOLGlzM+L+0N3rlPnJaKk5J/J5WE1Hf0UZEvRvZz587AoRT3vrYvPh8+6SRR3QRKrseRsVcZ+/J3pHLwgu8EP9YHj6HPHL7tGXCGX73qcMDNb8Dt7w0rcLTWkgdwe/RNXf3hX0hizcwPwLn/Ld7JzmdDm56VKLSGXyyAB92bzWCGftpycA3BkXcSp6H3pWSGhJTW3CV5hrO/lNy1C0nIoNNFU1c/E4a/y1l85NQprN5wJP66+iRgbBn6Q2th2xNw1hfDqhw0FDbl+VLiX1konn1YXSwNKhaN0D03dw8wMORyh24y6Bt0DRdPJR1GErlkRnL3ofdGKRjsga4Gc/28L1WnS7dBSExVrD8u/xV8ox7u2Jqw1rbHM8c6+9Ha47SBePUAv4lnrD5JGFuG/uXvyiCHMPuA1Hf0UpqbTkaqxGkrhg199DwA470qCrMoyRUD09SZpOEbg53PwbHtosY5HjCUN3NWBPeE07JgylL5+fRbAx8bT3JKRZKXVWit7N5mBEa+bWK+x9BXFmbx4SVTeGRdTeizJo5zxtYZdPV9cM1vw1ZO1LX3DcfnASYVykkSkfLGB2N3MKkgi9Jc6ZTY3J3khr7LndROxvGBZmQVSUz70p9bO376crnNLIjdmuLA67sb+eaTwfs5jQfq2+U7NbFgZCL0luUzcGnNffGK1ScJY2vwSPE0azMg/VDf3seUYo8xK83JIC1FRRa68cHwJCoKM4edzcbOJFXeGCz+uIyuO+G6RK/EGq6hwB04fVl4jQx2T7S0MkJ+9uJOth7t4FuXzR/elY5X6kw8eoApxdlce8pkHl53hFvPm8mE/MQrYuLB2PLoI6S2rXdETM/hUFQUZEU1dFPX3kd6qoOSnHTK8tyhm2RV3hikpMp83ZySRK/EIhoaQvBsi6fDvMsS3o8kErbUtLP1qBR7tXQnueMQBxo6+shIdVCYPbpn/+fPm8mQ08XD7x5OwMoSg23o3XT3D9HhVSxlUFGQGdV4Xm1bL5MKMlFKUZzjDt0kq5b+eOUT/4AvmwxnHsM8vM5jtOzzSRyqie7vmS9TirOZPSGP94+YjJwMkYaOPr7/zHYGhky6vSYRtqF3Y/TFmFQw0qurLMyKcujGkwdISxGPI+k9+uONzHzIm5DoVcSN7v4hntp4lBllEnqyzycxwL5hG28WTCpg69H2iKXNL26r5w9vHWBbbXtE7xNrbEPvxhgh6OvRTyrMor6jD2eURv7VtfVSUej5jNLcjDH5xRwYctGbrLLRMcYzm2rpHnBy2/kzAdujB8Oh8m/oF1bm09Q1wLEIFW81rbLbPxyvitswsQ29m2HZo2/opjATp0tzrDNyr37I6aKhs3/EriFpG5tFyNcf28xHHlibvMVgY4iH3z3M7Am5fGCe7GKSXsUVY1wuzbEOT7GUGSdUisJq69HIPPGaVjHwcW2YFga2oXdjePS+WfhJ0SiacnOssx+nS4/06PPGpke/rbaDzTXtvHe4NdFLGdNsPdrOppp2Vi6tIjcjlYxUx5h0HEKhpWeAAadruP2BGfMq8lEKtkRs6MVBPGR79McHdR19lOSkk5k2UpZmeN/RUN4YSV1vj74sNyO5xglGAa01R9yezt/eGT/KhkSwat1hMlIdXL24EqUUpWPwfAoVf2FYb3IyUplemjOsVAqX4dCN7dEfH9S7s/S+GEVT0VDeGLsCb4++JCedzr4h+ofGTjy7tWeQngEnuRmpPLu5jvaeMHsFHYf87o39XPObNXH5rJ6BIZ7aWMulJ1RQmC0KrpLcsRkKDAWPoQ8sl11YWRBRErW7f2hYynqoJY4TrMLANvRufDX0BnmZaeRlpEYldDPs0Rd6TsBSt5Z+LH05jVawnzl7Gv1DLh57rybBK4of7x5oYcOhVjr6Yn9x+8fmOjr7h1h5mqc1RUlO+riP0RsKukCqG5A4fV17X9ihU6M52qzyXBo6+ukbTF5nzTb0bowhBWZMKsyKSse72rY+cjNSyc/0FHGU5h4nRVMhYIRtLlowkROnFPLQu4fHTVLWOE/2HusKcmTkPPzuYWaW57Kkumj4sZLcjDHlNIRDfXsfKQ41XJDojwWTIkvIGonYZTNLAeLX6z4MbEMP9A44aesZ9BvTqyiMTtFUXfvoXYPR72ZMGfoW+b+aUpzNR5dOYe+xLtYdHB9JWSOXs6chtmP/dtR1sPFwGyuXVo0oCjJCN8l0YY13MVF9Rx9luRmkOAI3tJs/KR8Q4UA4GPH5M2dIxfihJI7TWzL0SqkVSqldSqm9SqlRw0Xcx1ynlNqulNqmlHrI6/Gfuh/boZT6tTIrVUswde3m0kqDSVEqmjIGjnjj8ejHjhdW09pDUXYauRmpXH7iJPIyUnnonUOJXlbM6RkYotWdj9jTEFuPftW7h0lPdfChxSPbcZfmZDDgdNHZPxTTz7fKttp2FnznBXbVx2/erb98my8FWWlUl2SH7dEfaekhI9XBye4dVTIrb4IaeqVUCnAPcDEwH1iplJrvc8ws4L+BZVrrBcAX3Y+fCSwDFgELgVOB5dH8BaJBsCz9pIJMWroHIo7B1bbJwBFvrIZu6tv7WPTdF9kUhbLtWHOktZfJRdIcLjs9lasWV/Lc1npax3gPFm9l1p4Yhm56B5w8vvEoFy+cSJG7jYZBSW5ytdVYd6CFQadm/aHYTxA1qA9SFevNwsqCsCWWNa29TC7KoiQnndyMVA43J29C1opHvxTYq7Xer7UeAFYBV/oc81ngHq11K4DW+pj7cQ1kAulABpAGNERj4dHEmCzl2/7AYFIU+tL3Dzlp6uoflQfISk8hJz2FpiAdLDfVtNHRN8SOusjkYPGgpqWHKcWe3/Ojp1UxMA6Sskfdu77KwqyYhm6e21JHZ98QK5eOng9gzDhIlhGVu9z/D7uT0KMHWDipgJrWXtp6Qr8w1rgdGqUUVcXZx7dHD1QCR7zu17gf82Y2MFsp9ZZS6m2l1AoArfVa4FWgzv3vRa31Dt8PUErdpJRar5Ra39gYQnvZKDGcpfcXo3cbZ+OCENZntI+WVhqU5GYEVUrsbxRvoTnJvWKXS1PT2suUIk+753kV+SyuGvtJ2aPumO25c8qobe+jM0bKm2c21zK1JJvTpo0e1F6SY+R8kuM82ek28LtjHMoy6OwbpKt/dHNCfyysDD9OX9Paw+QisQ3VJdlJraW3YujNYuq+39ZUYBZwLrAS+J1SqlApNROYB0xGLg7nK6XOGfVmWj+gtV6itV5SVlYWyvotcaCpO2Dr1tq2Xoqy00YVSxkYIwUjUd54DxzxxZgdG4j9jfJFSfYWtI1d/Qw4XUwuHjmk5KNLq9jf2M3b++O3hY83tW29pDgUZ7lVGPsaY7OVP9zcw4LKAtPOjIbSJBkkli6XHo7N745xctqgoWPk3OdgLAxTedPVL/kYI0RZVZJNTWtv1HpiRRsrhr4GmOJ1fzJQa3LMU1rrQa31AWAXYvivBt7WWndprbuA54HTI192aHzqT+v4r9Wb/D4vWz3/xRUTCuTLUxdBQtajoR99ApbmZgQN3exvEqOR7HFuQ2JmeDoGly2aRF5mKg+N4R7gtW29TMzPZG6FeImxMG5aa2rbe0flegyKspMnRl/T2kvPgJOZ5bk0dw/ERVlmTJayOlCkKCedysKskOP0xu5t2KMvzmHA6RqODiQbVgz9OmCWUmqaUioduB542ueYJ4HzAJRSpUgoZz9wGFiulEpVSqUhidhRoZtYorXmaGsvr+1q9Gsk69pHJ0m9yUhNoSwvI6IYvRH2MdPqWwndHGg6PkI3hobeO3QDkou45uTJvLC1Lmnix9Gmpq2XysIsqoqzSU91xERL39YzSN+gy69jkp7qoCArOVpf76yXcMgVJ04C4uPVB1PQmbGwMj/k0I2hoTcMfZV7B3sogoTs4+/V8Oj6I8EPDIOghl5rPQTcBryIGOlHtNbblFLfV0pd4T7sRaBZKbUdicl/VWvdDKwG9gFbgE3AJq31MzH4PfzS0TfEgNPFkEvzwrZ602PqO4InbyYVZEY0O9YID2Wljw4PleWm09I94Hfb19YzMByySfbQjaGh9/XoQZKyg07N6g1jMylb29bLpMJMUhyKGWW5MTFstcP9kvyfr8nSBsEI21y2qAKIT0LWCN2EMiJw4aQCDjR1h5RTqRn26MXAV5fIbbhxeq01//fyHp7YeDSs1wfDko5ea/2c1nq21nqG1vpH7se+rbV+2v2z1lp/SWs9X2t9gtZ6lftxp9b6Zq31PPdzX4rJbxEAb8/m6fd9I07QN+ikpXsgqAcgWvrIPHp/lbeleRm4tH8jbsR6i7LTkt7Q17T2UJ6XYZrvmD0hjyXVRTz87mFcSRrLDBenS1Pf3jes0JpVnhsTLX3dcL8k/6HG0pzk6Ii6s6GTquJsppXmUJidxq44JGTr2vsC5tvMWDhZ4vTbQ/Dqa1pFQ28UPFYUZJLqUGErb9YdbOVwSw/XnDw5rNcHY8xXxhqezSnVRbx9oHn4im9QHyCk4o3Mju0LWzVieHtmBNPSG4nYJVOLQ0qy/eHNAzzw7/hOuz/S0mvqzRt84sypHGzu4fmt5rur45VjnX0MuTSV7t999oRcjrb10h3lwqU6qx59EjgEu+o7mTMxD6UUs8vzYl4tDOLRhzrw20jIhhKnNzT0RkI8NcXB5KKssD36xzbUkJ2ewoqFE8N6fTDGvKE3jOeNy6aitTSC8sYTOw/m0WfSO+ikvTc8yZw0TTM3gCVBZsfub+omLUVx4uQC+gatT256dEMNP3txV1T69FjlSGsPU3wUN95cckIFM8py+NW/do8pr97Y7Rke/czyPCD6PW9q2/tIS1HDzoEZErqx7hBorWnu6uf9I208s6mWe1/byzee2MJbe5vCXmffoJMDTd3MnSj/D7Mn5rKroTPm8tpgk6XMKMvLYEJ+RkhxerPzvKokJ6xJU70DTp7dUsfFCyvIyUgN+fVWGDeG/rRpJcyvyOfpTSPDN/Ud8gUNFqOPRGJpDB4309CDp4NlII++qjg7ZOlcY2c/g07N/a/Hx6sfcrqoa+8blYj1JsWh+M8LZrG7ocuSV9874OTLj2xiS01yz+QcjtkaoZsJuUD0K2Tr2nqZkJ+JI0Afl5KcDFp7BhlyBu4x0zfo5Lr71rLgOy9yyg9f5qp73uL2hzfy0xd28cj6I3zx7+/TFeaOZO+xLpwuzRy3oZ8zIY/OvqGYq1IaLOTbzFjoniFrFcOj96a6ODusZOw/t9fT1T/ENaf4lidFj7Fv6Dv7UQqKc9K54qRJbDrSNuKPMdwjPljorcuUhQAAIABJREFUxv0FDkdiaTZwxJvgoZtuppflUpwjx1mJ0ztdmpbuftJSFKvWHeFYHGRfde0yW9e7KtaMyxZNsuzV//Ll3Tz2Xg1/eOtANJcadWp9YufVxdmkpzjYcyy64Yra9j6/55GBETduCVLtua+xi3cPtnDOrDK+fdl8fvvxJbzwxbPZ9r2LeOTmM2js7OeuV/aEtU4jEWt49LMm5I14PBZI9fkAE/MD//+YsbCygH2NXfQMBL+wdfYN0ualoTeoKs6mo28o5Crb1RtqqCzM4vRpJSG9LhTGvqHvHqA4O50Uh+Jyt8zrGS+vvr69j0I/ahhvjPh6OMobz8XE3NPIz0wlPcVhWs3odGkONfcwvTSHYiPEY8HQt/YM4NLwsdOn4nRpHvj3/pDXHSpHhiVn/j16GOnV+1NCAbx/pI3fvbGfzDQHL29vSOrhLLVtvRRkSSM3kJjt9LKcqCdk69p7/e4MDTxtEAKfJ0bNw+fPm8mnzprGB+dPYO7EfHIyUllcVcS1p0zmD28eGJb2hsKuhk7SUx1MLckBJBEPsW32dqxDHKWJBYHbE5uxsLIAl8ZSixFjV+/r0VeVGBJL6+Gb+vY+3trbxDUnVwbcpUXK2Df0nf3DHnNlYRZLqot4ZpMnTl/Xbq0BUmlOBmkpKqwulmYDR7xRSlHipzq2prWHAaeL6WU5w7H8FgvSuUb3dPtTqou48sRJ/O2dwzHXr9cY7YmDGHrw8upf3mPq1fcPOfmv1ZuYkJ/JT65ZRGf/EGv2Nkd9zdHiqFtD783M8tyoevQut7In2O4zWM7HwNNO2vz9/mvFHDJSU/jhP7aHvNad9Z3MKs8lNUVMTHFOOmV5GcO9b2KBp5VJOB69FLlZGS1Y0zJSWmlgSCxDUd48sfEoLg0fipHaxmDsG/qufkrzPB3+rjhpErsaOoe3kPUd5pOlfHE4lFt5E55Hr1TgPEBprrkkzuhxM70sl2L3lrzVwtbQeK/S3HRuPW8GfUPOmIc/jrT24FDm/Xx8Mbz6XQ2dpl79Pa/sZXdDF/9z9QmsWDiRvIxUnttSZ/JOyYGoqkYamNkT8tzVodFR3jR1S87Fn3rLoNRiLudIaw95makUZKWZPl+el8nt58/kXzuP8dquY6bH+GNnXcdwfN5gzoS8mBZNDXehDVF1Y7ymJCfdUpzet1jKwCiastrFUmvN6g1HWFJdxNTSnBBXHBrjwNAPjFAoXHJCBQ4FT2+SwoS6tsDtD7ypKAhvAEldey9luRmkpfj/7/ZX5GK0PphemkNeRippKcpS6MYw9GV5Gcwsz+OShRX8ec2hmM5vPdLSQ0VBVsDf05vLFk1iuolXv722g3tf28eHFldy3txyMlJT+MD8Cby0o4HBIAnGRHG0tZdKHwM8qzwXrWHfsej0vKmzmE8qzbE24+BISw9T3N0X/XHjsmlMK83h+//YbnmASGv3AMc6+4fj8wazJkhtQazUVlaGgvtDKcXCygK2WlDe1LT2kpnmGN45GWSnp1KWl2FZebOppp19jd1cc0psvXkYB4a+uaufkhyPoS/NzWDZzFKe2VRH36CT5u6BgJpkbyrDHEBS2zZ64Igv/j36Lgqy0ijOSUcpRVF2ekihG8O7u+38mXT1D/GnNQdDXr9VzJQIgUhxKL7g9upfdHv1g04XX129icLsdL59uWfswYqFE2nrGeTt/cHDN794aTe3/HVD6L9AmHT0DdLZPzSsoTfwKG+i48VaLe/Pz0ol1aGCFk0dae0NmjhPT3Xwrcvmsb+xmwfXHrS0zp3Didj8EY/PmZBH76BzWKEUbeo7+shKSyE/MzyJ4sLKfPY0dAadO+HdntgXUd5YM/SPbaghI9XBpe7K4Vgypg1974CT7gHniNANwOUnTuJwS8+wcbHqAVQUZlLf0Rdyh7pATagMSt2zPn11xqK4yRk+qYpzrBXDNHUNkJ7qIM+dHJxXkc8H5k3gD28dCFsyF4xgGnozhr36f4lX/8C/97OttoMfXrWAwmzP32357DKy01OCSjIbO/u5//V9PL+1flRxXKzw1dAbVJfkkJaioiaxHO6AGsRpMHI+gXIyWmtqWnss5VPOnzuBc+eU8auX9ww7EIHY5e5x4+vRz3bfj1Wcvt6toQ93iN3CSQUMeXXc9EdNW49fh6aqONuSR98/5OTpTbVcuGDiiBnSsWJMG3pPnHpkFv6iBRNJT3EMK1GCbYUNJhVm4XRpjnVaNyBaa+ragifQSnPTGXC66OgbaYT3N3UxvTR3+H5JbjotFnT0TZ39lOVmjDjpbz9/Ju29g/xlbfTH+vUNOmno6LdkOLwxvPqd9Z3c8+pefvXyHi45YSIrFo70cjLTUjhvbjn/3FYf8EL7pzUH6HeHGEKNK4eL0cnQNxmbluJgWmlO1CpC69p7yUh1UJQd3DCU5AQeEt7Y1U/foMvyhflbl82nb8jJz17cGfTYXQ2dFGWnjRrOPatczuNYxenrw6iK9WZhpbtlcW3gOH2gnWtVSTb1HX1BdwWv7DhGe+8g15wcO+28N2Pa0DcacWofQ1+Qlca5c8qGK+GsJA/Bo4MPJSHb3jtI76AzeALNREvf1T9EQ0c/08s8iZrinAxLOvrGrv7hsI3BiVMKOWd2Gb97Y7/l6lqrGJKzYKEAMwyv/ucv7SY7I4XvXbHQ9LiLF06kqWuAdQfNe9p39g3y4NpDXLxwIhUFmbyyMz6G3jgffA09wKzyvOh59O5eOlY81pLcdJoCnCfBFDe+zCjL5cZl03h0Qw2bawKPs9xR52l94E1eZhqVhVmxM/RhVMV6M7koi4KstIAJWX8aeoPqkmy09iRs/fHYezWU52Vw9qzoz98wY0wb+qZOc48eRH1jYDVLb3gLDR3WZYpWC7KGDb3X1viAW3Ezw8vQl+SkWzP0nf2U5aaPevz282fS3D0Q9b7wvt38QiHFobjjA7MB+O7lC0Z5ggbnzSknI9XB837UNw+9c5jOviFuOXcG580t5809TXHR3te09ZKe4jA9z2ZNyOVwS0/E84ZBqmKtGjIJBfo/T2v8tJMOxO3nz6QkJ4Mf/sN/p3GXS7O7oXNUfN5g9oTcmBRNuVxa+txEYOiVUiyuKmTNvma/rRqM89zf/1tVsXxXA8Xpm7r6eW1XI1cvriQlhtp5b8a0oTdi2SUmBu+CuRPISZfEjdX+EuX58kUOpco00MARb4w8grdSYn+TeILTvEI3RdnpdPQNBVWfNHUNmBrMU6cWc/r0Yn73xv6o9h0xim/C8ehB8ibvfuMCrlrsfyubk5HK8tllvLCtfpRyo2/Qye/ePMBZM0tZNLmQ8+eU0z3gZP3B1rDWEwqSbDdvSzCrPA+to9PzJlAHVF9KcgK3KvYMiLFu6PMy0/j8eTN492ALGw+b/78aw0Z84/MGsyfmsb+xO2h7hlBp6u5nyKUj8ugBLpw/kUPNPX7zCDWt/ttwg1e74gBx+qfer2XIpeOitjEY04be8I7NDH1WegrXnDKZE6cUWn4/o8K2MYTCo9p2awk0QxnkrX3e19iNUp6TB/Bo6QN49Ub7A3+Nr65eXElde9+wdDMaHGntIT3FwYS88L9o5RZee8kJFTR09LPxyMjwwRMbj9LY2c8t584A4MyZJaSnOuISvqlt6/XblsBQ3kRq6IecLho6+oI6DAYluRn0Djr9avgPt/RQmpsRtCLclw8vmUJeRiq/f9O8JsMYNuKroTeYXZ7HgNPFwSjPV20IcbKUPz44fwJKwQt+kv7+NPQGJTnp5KSn+PXoRTtfwwmVBcPVwvFgbBv6rn7yM1PJSDU/mb93xQL+8unTLL+fw6EozU0fLrW2Ql1bL6mOwN0GQdQ0DjUydLO/sYvJRVkjemuXWGiD0NIt7Q/8feaSqTJUer2fWHc41LT0UlmUFdMyboDz55WTlqJGhG+cLmnctmhyAWfOkH4h2empnD69hFfjYOiPtvaOklYaTC3JIdWhIpZYHuvsx6WtCwcM58afV3+kpZeqMHZfuRmpXL90Cs9vrTdt8GeEZfwZMeMCEO04fTiTpcwoy8vg1OriAIa+l6y0lOF2JL4opQJ2sfz3niZ21HXw0dOqIlpnqFgy9EqpFUqpXUqpvUqpr/s55jql1Hal1Dal1ENej1cppf6plNrhfn5qdJYenKaugVEJSW/CkWGV52VyzILEzKDW3W0wWCwuxaEozkmn0Tt009g9QnEDDJ9ggeL03sVSZhh9c9ZFMaxR0+pfchZN8jPTOHtWGc9vrR8OPb2wtZ6DzT3csnzGiL/p+XPK2N/UzcEo7lx8GXS6aOjs87tjS091MLU0h90R9ngZNmQWPXqjsZk/LX04UliDT5w5FYAHTWoydtbLsBF/4dCZ5bkoFf3mZoaUNpyqWF8uWjiRnfWdpueNcZ4Hsh1VxVmmXSy11tz1rz1UFGTyoTipbQyCGnqlVApwD3AxMB9YqZSa73PMLOC/gWVa6wXAF72efhD4mdZ6HrAUiI8UAnf7g5zQGxwFojwvIzRD3x7CdjvHk0BzuTQHmrpHKG7kmOCGvjFAEhrkAndKdVFUPfoj7iKSeLBi4USOtvWy5Wg7Wmt+8/peppXmcOGCkUMbzp87AYBXYyizrG/vQ2tGVcV6M6s8N+LQzbCG3nKM3n9jMyvtpAMxuSibFQsn8tC7h0cNVtlZ3+E3Pg8ik60uzva7w3G6NJ/847t844ktIeWQ6tr7SHWo4YZukXDRAjlvXjRpzWGlKLC6JIcjrb2j8khv729h/aFWbj5nut8oQ6yw4tEvBfZqrfdrrQeAVcCVPsd8FrhHa90KoLU+BuC+IKRqrV9yP96ltY5ucC4Avn1uokF5foalohGD+nbrLRZK8zyNzRo6++gddDK9bKRHXxSCR19qkpswOHVqEQebe0KqCfBHd/8QLd0DYSdiQ+WD8yaQ4lA8v7WeN/c2sfVoBzefM33UrqmqJJvpZTkxjdMfHZZW+jeasybkcai5OyLlTcgefYAZB1bbSQfi02dNo7NvaMQw675BJwebewIaepCwjj+P/qF3D/Parkb+9s5hHl1vfbZwfUcf5XkZUVGxTC7K5oTKAtMeTDUWHJqq4mwGhlyjeu/f/eoeSnMzuH5pfMM2YM3QVwLeo8lr3I95MxuYrZR6Syn1tlJqhdfjbUqpx5VSG5VSP3PvEEaglLpJKbVeKbW+sbExnN/DFN8+N9GgLC+T5u5+S6oBrTX1Hda1vdIGQQy40cxshk+zo6LsdJQKHKMPFroBT5x+QxTCN0fCkOpFQlFOOmfOKOH5LXXc++o+yvMyuNrPVvj8OeW8s78l6iP9DDxVsYE9epf2/E3D+5w+cjNSLVdRBsrlDCukIvh7nVxVxMlVhfxxzcHhAjbPsBFzaaXBnIl5HGweLTlt6urnZy/s5IzpJZw5o4TvPL2NvRZzG+JQRR62MVixcCIbD7cN988BaXXR3jtowaMfrbzZcKiVt/Y2c/M500OaZxstrBh6s0uk754qFZgFnAusBH6nlCp0P3428BXgVGA68MlRb6b1A1rrJVrrJWVl0SkgGBhy0d47GANDn4HWVnvCDzIw5LIcN/TWPhtzYn09+hSHojArLWB1bGNnPxmpjuHe6GYsnFRARqojKnF6T9vW+Hj0IF/Eg809rN3fzGfOnuZ3K3z+3HIGnC7W7ItNi2OjKjaQqioaPW/q2q1r6EFCJLkZqaYe/fCFOcwYvcGnz5rOoeYeXt7RAHji7v4UNwazJ+ThdOlRF74fP7+T3kEnP7hqAb/8yElkpadw20MbLe2E6sOcLOWPi9xhwH9u93j1Ry3WilS7tfTe82PvemUPRdlp3HB6/L15sGboa4ApXvcnA7UmxzyltR7UWh8AdiGGvwbY6A77DAFPAidHvuzgGDLFaBv68jxDSx88fBNqN72S3HS6B5z8//bOPTyq8tr/n5WZzOROkhmSQAIkkATkIkEDomC9VYuXij+tiDfEX61Hra3ao9Wetp7K0ac9z89an55afdQqrdUq1ar0aOvdWu9ARbkI4W5CQu73ZHJ9f3/s2ZMhTCZ7roGd/XmePMns2TPzvrBn7fWud63v6u4dYE99JykOG7kZR44/e5SiKX0lE2zDyGFPoGxKJpsORB6nj5bhCIVzZueRIFrTlsuDLIXLC7NJc9pjFr6pbu3GneYI6qUVuVOxJUhETTdqWkcXxhvOSIqolU3d2BIk4gyVb8zJJT8z2ZdqueNQm7fZSPDrwNeExO/Gt2F/E89vquLbS6dTnJNObkYSv7x0PjsOtfPzV0cu0ALvyrnVE1ZnqZEozkmjOCftsOybodqD4J8zOVNLvjjQpN3ItlS18u7Oeq47dTopjtj0hB0NI4Z+A1AiIkUi4gBWAuuHnfMScAaAiLjRQjZ7va/NEhHdTT8TCL2LQRjoF3igHPpI0A19fcfosW29H63R3F5/GYS9DZ0UuVMDGmvXKDIIDR09QcM2OgsLs9la3RaxXnplk5ZyNly2NZZMTHfyb6fN4MfnH0d6kHCGw57A0mI37+6si0lj6oMtI2fc6DjtNqa5Rt6ANEJ1i8ewyqqOK9URUJO+srmLSROSfE1BwsVuS+DaJYV8uq+JLVWt7DjUTmlu2qjvW+TWUk71FUD/wCA/fWkrkyck8f2zin3nnTErh28vLeL3Hx3g9SCdyNp7+unqHQirs1Qwls3J45N9Tb6aldGKpXTstgTyM5N9ufT/8/YuMpLsrDp5WlTHFwqj/k97PfGbgdeAL4F1SqltIrJGRC70nvYa0Cgi24F3gDuUUo1KqQG0sM1bIrIFLQz0WCwmMpz6EQTNIiXHa7SNefTaOUY9J12Tp76jh731HUeEbXSyUhNHzboxMu/ywiwGBhWbvwquXTIaWqqeMQ2WaHLnsllctnD0pfCZs3KoafXwZU30S+8PNncF1LgZTklOWtiaN1ov1B7DOfQ6rrTAwma6Dn00WLFwCqkOG797fy87D7UzMzd4fB60m+/0iam+XPq1H+5nx6F27v7mnCM83h8um8nc/Ax++MIXI/aCGFo5Rzd0uGxuHgODyheaqmruJsUxcg69P9NcmorljkNtvL69lmuXFAV1SGKNoVu6UupVpVSpUmqGUuo+77G7lVLrvX8rpdQPlFKzlVLzlFLP+r32DaXU8d7jq72ZOzFHLzwaLmgWKXomi5EUy0NtWmcpI941DK0+DjZ3c7Clm+kjdJ0ZTdhM8+hHvxhPmJaFCBHH6SubuuKWWhkOp8/UFpTRTrNUSmmetiFDn86Bxq6wtHf0qk+jGTc67jRHwOYjRnTojZKRlMhlC6fy1y9qAjYbGYmS3HQqajuobfPw4Ju7OH3mRF9aoz9Ou43/ufwE+voHueXZzQGVSyPpLBWMOZMzyM9M9qVZGsmh15nq1aX/zdu7SXPauXZJYVTHFipjEzCKA/oFHu30SqfdRmZKoqG0xEMGOkv5o3vhmw40oxRH5NDruFIdNHf1MTiojqhE1eQPeg3d4DKSEpmVl8HGCOL0SikONndzUlF22O8Ra3Iykpibn8E7O+r47hnFo7/AIM1dmjKpEY9+Zp62AbmrtsMnh2sUvSG90Rx6HS3E13PYddLdO0B9e4+v7V00uHZJIWs/1OL0o23E6szMTeeVL2r48Ytb6R0Y5J4L54xoQIvcqfzXRXP5wbrPufmZf+FOc9La3UdLt5YFU+PNfIq2oRcRls3N46mPD9DR028otVJnmiuF1u4+XtlSww2nzTist8JYYFoJhMaOHpITbTHZ/MhJN5ZLf6itJ6RMAN2j/3SfZnhnjBC6yU51MDCoaO0+si1gY6dWKh+sItifhYVZ/OtAc9giU63dWneleG7EhsOZM3P411fNQTWCQmWkhiOBmF+gaSptrgw9TBZqDr2OK83BoIIWv+ukKgYb51OyU3xZKkY9en1D9s0va7nxtBlMcwXvmXrxCQWsOnkar2+v5a9fVPNFVQut3X1MSE5k8XQXt5xVEpM6jmVz8+jtH+TdnXUhVX/rKpZJdhvXLS2K+rhCxcQeffSLpXSMyiAcau0e9QL2x2m3kZ5k50uvMFTRCKEb/YbQ1NXrK6DSaWj3rmQMhqzKC7P5w0cH2HGoPWRPE4Z0zeOZWhkOZ8zK4ddv7+a9XfUsL4tO+fnBIDr0w5mSnUx2qoPPK1u4anFom3KhVsXq6FWijR09vrhyZXPoqpVG+MkFszmtdKJvD2s0dM9/mivFJ0Q3GmuWzw3q+ceCE6Zm4U5zsm5jFW2efsPXuf7dvfKkqVGp1o0U03r0sSiW0pmY7jScXhlqCtvENC1PPy8jaUS9kKyUkatjjRRL+VM+LQtgxGYeoxErwxFt5hdk4kp1RFXkzNdZysCXX0SYXzAhbI8+MyUxZKVJd+qR0tehNhwxSn5mckgVn9OyU7isfAq/vHR+SAVE8d7wtyUI58zJ5b0KrZDT6HVempvGg5eVcevZpbEcnmFMbOiNZZ6Egx66CZau19XbT5unP2TZVH3MI8XnYUjYLFBGxWg6N8OZnJlMfmZy2LrtsQgFxIKEBOG00om8W1Efcs/fkahu6SYp0VhrP4CyKVnsru+g3XNkyC0YRlpRBiKQDEJlUxdJiQlRT1IIlYQE4b+/dbyvQvtoZpmfhpJRj15EuGhBftCixXhiYkMfW4++d2CQtu6R88/1TIBQPXo93DRS2Ab8QjdBPPpgOjfDKS/MYsP+prDyzCubuslIsjMheexSx4xy2syJtHT1BW0VFwoHW7rJN9jaD2D+lAkopRXQhEJ1a+g59OAng+Bv6Ju1DKl4e8bHMounu8hI0gz20b5yHQlTGvqhxhsxitHrufRBMm8OhSmbqqsOjpRDD/5SxUeGjxo6ekhKDC5/MJzywmzq2nt8y/pQ0A3HscApM9wAfLCnISrvV93SbWgjVqfM2+RmeNOU0ahp7Q55IxYgM0XrceAv11HZ1M2Uo3w/5WjDYU/g7Nl5TEhONLx6O9owpaFv7greeCNS9GVvsA3ZUOUPdIyEbpx2TcekqfPIEIBeLBWKx7aw0Hicvn9gkAONnbyzs44nP9jH1oNtcVOtjJSJ6U5m5aXzwe7oGPqDLR5DG7E6mSkOCl0pfB6Coe/uHaClqy+s0I3e4+CwGH0EOvTjmZ9ecBzP/dviY3YldHQEkKJMQ4yqYnX03rHBUix9Hn2Ihj4/KxmRkTv06GjVsYE8+tBDVqU56aQn2dl4oClgH8umzl5+tn4bW6tbqWzqom9gKMST5rRz1qwjC12OVpYUu3nq4wN4+gYiUhH09GnVqqEYetC8er35tBGjUW2w5/BI+Pc4aO3qo93THzeVUTORmeIY81z4SDCnofelGMYqvVL36IOEblo9pCfZQ87jv3D+ZEpz00Y1INmpzoAKmg0dPSF7bAkJQvm0rIAVsk2dvVzx2Mfsa+jkzFk5LJuTR6E7lSJ3KoWuVNxpjmPKy1lS7OJ37+9j04FmlhS7w36fGoO9gIdTNiWTlzZXe+WrR39tTYu+1xPeqsmV5vBdJ2MhPmdxdGBKQ68LOcUqfzXNaSc50RY0xTKc1ErQ4oHHF4zesNyV6vC1T/Onvr2HBVOzQv7c8sJs3tm5k+bOodz85s5ernz8E/Y1dPL4NeWcWhIdCemxZFGRC3uC8MHuhogMfSjFUv7ozeg3f9XCpHmjvzbcqlgdV5qTLVVaqEjXRz9WQm0W0cOUMfr6GOnc6IiIlksfJHRT2+aJuCN9MAJJFfcPDNLU1cvEMFYyC/VGJAc0r765s5crHv+EPfUdPLbKHEYetJt02ZRMPohQn/6gQSXD4cyenEGiTdhcZSxOr3v0uWEqM7pSh6SKfQ1HLI9+3GFKQ9/Q0YvDlkBGcuwWLKPJINSE6dEbxeU19P4pkU1dvShlvFjKn+MLJuCwJbDhQJPPk99T38Hjq8r5Wqk5jLzOKcVutnhL6MPlYEs3IsYlqHWcdhuzJ2UYVgytae3GneYMu8eoO81Be08/nr4BKpu7mJCcaLhLlYV5MKmh78EV49hxToZzxBh938Ag9R09URdZ8ic71UFP/yBdvUNqiKEWS/mTlGhjXsEE/rGznqt+9wm7vZ682Yw8wNJiN4MKPt4bvld/sKWb3PQkHPbQv0JlUzLZcrDVUOFWKM3lA6GHL5s6e7XUSitsMy4xpaFvjGFVrE4wvRutahZyY+jRB2oSPqTYGd7cywuz2HGonV11HTx69YmcZkIjD5qhTU60RZRmqeXQh/f/O39KJl29A4YakdS0hNZCcDguvyrqyubo6dBbHFuY0tA3dPRGvbPUcCamO2n39AfsZ6mnVsY6dAOHF8NEqsH/jTl55GY4efTqEzl9Zk7kgzxKcdgTOGl6dkSGfl9DZ0iCdf7ohVNG8um1EGD4Xrju0dd3eKhq7rbi8+MUQ4ZeRJaJyE4R2S0id41wzgoR2S4i20TkmWHPZYjIQRH5TTQGPRqx1LnR0ePggeL0td7Uu1hvxsLh1bG+rlphevQnTM3ik//4uqmNvM6SGW721Hf6CttCoc3TR02rx9f0O1QKXalkJNlHFThr8/TR0dMfUehGv+l/WdNOb/+gVRU7ThnV0IuIDXgIOBeYDVwuIrOHnVMC/AhYopSaA9w67G3+C/hHVEY8CkopGmOoc6MTLJe+pjWy3Gcj6FIJ/tWxDe2a/EFqiCqH45FTil0AYXn1epPv0hxj2uvDSUgQ5k/JZHNlcM2bSHPoYUgX6TPv5m+B5dGPS4x49IuA3Uqpvd42gM8Cy4ed8x3gIaVUM4BSyqcFKyInArnA69EZcnDauvvpHRiMWbGUTk76yL1ja9s8OOzGVQ3DITstsEc/MT00+YPxynF5GWSnOsLSvdnl7XU6WvVyMMqmZLLzUPDG7JFWxQKkOGwkJSb4Vg/R7CxlcexgxNDnA5V+j6u8x/wpBUpF5AMR+VhElgGISALwS+COYB8gIteLyEYR2VhfX2989AFo6Iyt/IEUtHAkAAAUmklEQVSOLoMQaEO2ptVDbkZsDW6qw4bDlnB4jD4OISuzkJAgnDzDxQe7G0JW7ayo7SA50RZRs5WyKZkMjqJkGQ2PXkRwpTp9siChSjZYmAMjhj6QtRr+zbADJcDpwOXA4yKSCdwEvKqUqiQISqlHlVLlSqnyiRMjy/RoiCDFMBSyUxzYEiRgjP5Qm4dJGbH9QologlVNHf6bsbEPWZmJpcVuatt62FPfGdLrKmrbKc5JO6JfbyjoFbKfBymcqmntJkGGwoThoq9uczOcEen7WBy7GDH0VcAUv8cFQHWAc15WSvUppfYBO9EM/8nAzSKyH7gfWCUiv4h41EGIVVPw4SQkCO40R8AY/aFWT0xTK3WGV8fqoRsLYyzRZYtDjNNX1LZHFLYBzREpyEoOuiFb3aJVV9sNNpcfCT3zxkqtHL8YKR3dAJSISBFwEFgJXDHsnJfQPPm1IuJGC+XsVUpdqZ8gIquBcqVUwKydaBFr5Up/AuXSK6U41OZhWRwMvb9gVf/AIM1dlkcfClNdKUzJTuaD3Q1cc0qhode0dvVR195DaZgZN/6UTcn0bZIGoqY1shx6HT0Vd6xTK/v6+qiqqsLjCT3TyWKIpKQkCgoKSEw0vgc4qqFXSvWLyM3Aa4ANeEIptU1E1gAblVLrvc+dIyLbgQHgDqVUZGIiYdLY0UOCDPVVjSU56U5fho1OS1cfvf2DMU2t1MlOdfiEqjQ5BMLSuRnPLJnh5pUtNfQPDBrynCvqIt+I1Smbksn/flFDXbvHt7nvT02rh9mTMyL+nCGPfmzj81VVVaSnp1NYWGglDISJUorGxkaqqqooKioy/DpDa0Kl1KtKqVKl1Ayl1H3eY3d7jTxK4wdKqdlKqXlKqWcDvMdapdTNhkcWJvUdvWSnavHzWDMx3enLXdfRDX8s5Q90slKGYvT6ysIK3YTGKcVu2j39bK1uM3R+hTfjJtwcen+GCqcO35AdHFT8+q1d7GvopDhIpzGj6DH6sU6t9Hg8uFwuy8hHgIjgcrlCXhWZrjI2npknOelaUwd/zZLaMBuOhIMrVROs6ukfiGvIykycMiO0fPpdtR2kOmxRyV6ZM3kCtgRhc+VQH4COnn5ufHoTD7xRwf9ZkM+Np8+I+HP0XPqjIUZvGfnICeff0DL0ETAxI4lBdXjz5ZowWwiGg55L39zZN7QJbRn6kHCnhdZesKK2neLc9KgYrGSHjVl56T6Pfn9DJxf/9gPe2F7LT84/jgdWzI9KlsxppTmsPqWQBVNH73NgYU5MaujjE6ceqo4dMvSH2jxIFFLijDCkd9MzpMFvhW5CZmmxm40HmgPqFg2noraD0pzIwyk686dk8nllC+/srOPC37xPXXsPf/i/J3HdqdOj5v1mpzr42YVzxn1qZUtLC7/97W9Dft15551HS0toDd0BVq9ezfPPPx/y62KB6Qx9Y0dvzDpLDSeQ3s0hr354YoQpcUbI9sogaB59D8mJNlKdpmwaFlOWFLvp7R9kY4BWiv40dfbS0NETlY1YnbIpmbT39HPtkxuYnJnMX29eytKS8DtfWYzMSIZ+YCD4Df7VV18lM/PYXg2Zyip09fbT1TsQ1xg9HK53c6itJ6aqlf5kp2rpVY2dPdpKJsa1A2ZlUVE2iTbh/d0NQY1sNDdidU7yfvY5s/P4f5ceH3KP4WOVe/66je0GN8CNMntyBv/5zTkjPn/XXXexZ88eysrKSExMJC0tjUmTJrF582a2b9/ORRddRGVlJR6Ph1tuuYXrr78egMLCQjZu3EhHRwfnnnsuS5cu5cMPPyQ/P5+XX36Z5OTR92veeustbr/9dvr7+1m4cCEPP/wwTqeTu+66i/Xr12O32znnnHO4//77+fOf/8w999yDzWZjwoQJvPfeexH/25jqqop1U/Dh6B69v95NbauHqa74bHpl+4TNeqlv74lZ60Szk+q0s2Bq1qhxel3jZmZe9Dz6aa5UNv74bDKS7dZGZYz5xS9+wdatW9m8eTPvvvsu559/Plu3bvWlKT7xxBNkZ2fT3d3NwoULueSSS3C5XIe9x65du/jTn/7EY489xooVK3jhhRe46qqrgn6ux+Nh9erVvPXWW5SWlrJq1SoefvhhVq1axYsvvsiOHTsQEV94aM2aNbz22mvk5+eHFTIKhKkMfaQyvaHitNvITEk8LEZf09rNoqLsuHx+ZnIiCTIUUigMUx/dQovT/+rNisOaow+noraDdKc96qmzE2Iofne0EszzjheLFi06LBf917/+NS+++CIAlZWV7Nq16whDX1RURFlZGQAnnngi+/fvH/Vzdu7cSVFREaWlpQBcc801PPTQQ9x8880kJSVx3XXXcf7553PBBRcAsGTJElavXs2KFSu4+OKLozFVc8Xo9eyXeHq2E9OGesd29fbT5umPS8YNaDIMWSladWxDR2/cbnBmZEmxG6XgwyBNwytq2ynJTbM8b5OQmjrkGL377ru8+eabfPTRR3z++ecsWLAgYK660zn0HbPZbPT3j6w+qjOSaJ7dbufTTz/lkksu4aWXXmLZsmUAPPLII9x7771UVlZSVlZGY2PktaemMvR6imGsu0v549879lAci6V0slMd1LX10NTZa4VuImB+wQTSnXbe3z2yeuquuo6obsRaxJf09HTa2wO3b2xtbSUrK4uUlBR27NjBxx9/HLXPnTVrFvv372f37t0APPXUU5x22ml0dHTQ2trKeeedx4MPPsjmzZsB2LNnDyeddBJr1qzB7XZTWRlUE9IQpgrd6EVDelOOeJCTnsSG/U1AfFoIDicr1cGeeq0RhuXRh4/dlsDiGS7eHyFO39Ch3UxLLEN/zOJyuViyZAlz584lOTmZ3Nxc33PLli3jkUce4fjjj2fmzJksXrw4ap+blJTEk08+yaWXXurbjL3hhhtoampi+fLleDwelFL86le/AuCOO+5g165dKKU466yzmD9/fsRjMJ2hn5CciMMev4VKTrqTuvYelFK+qth4KFfquFIdvhuNpXMTGUuL3byxvZavGruO2FCv8DUbiV7GjUX8eeaZZwIedzqd/O1vfwv4nB6Hd7vdbN261Xf89ttvD/pZa9eu9f191lln8dlnnx32/KRJk/j000+PeN1f/vKXoO8bDqYK3WgtBONr7CamO+ntH6TN0x9XnRud7FQHegjQKpaKDD218p8Bwje+9oGWR29xDGIqQ1/f0RO3YimdoaIpD7WtHtKT7HEtWnL5ZYhY8geRMd2dyqQJSQHTLHfWtpORZI9LxbPFscV3v/tdysrKDvt58sknx3pYh2G60M1xeZHLuoaCf+/YmlZPXL150Dx6HcvQR4aIsLTYzevbaxkYVIcpoO7yNhuxMm4shvPQQw+N9RBGxVQefUN7/HRudPTesfUdPdS2eeKWWqmT7TXuKQ5L/iAaLC1x09rdx7bqIelgpZSmcRPFQikLi3hiGkPf0z9Am6c/7l6tf3XsmHj03gYrljcfHU7xthf0z76pb++htbsvqmJmFhbxxJChF5FlIrJTRHaLSMBWgCKyQkS2i8g2EXnGe6xMRD7yHvtCRC6L5uD9aenqI9EmcU8xTHfaSUpMoLq1m4aOnvh79Km6obcybqLBxHRNtvj9XUOGvsLaiLU4xhl1rS8iNuAh4Gy0JuAbRGS9Umq73zklwI+AJUqpZhHJ8T7VBaxSSu0SkcnAJhF5TSkVHQEHP3Izkqi499zDmoDEAxEhJz2JbdVtDKr46ND7oxeHWRk30WNpsZs/fHSA7t4Bkh02PzEzy9BbHJsY8egXAbuVUnuVUr3As8DyYed8B3hIKdUMoJSq8/6uUErt8v5dDdQBE6M1+OGIiKG+n9EmJ93JtoNaTDfeoZssK3QTdZaWuOkdGGTjAa0+YVddO1kpidaqaZyRljZyqG7//v3MnTs3jqOJDCNWMR/wr8Gt8h7zpxQoFZEPRORjEVk2/E1EZBHgAPYEeO56EdkoIhvr60cuQT9ayclw0tmraVrH26N32BO4fNFUzp6dO/rJFobwyRZ7wzcVtR2UWBk3FscwRtI0Al3dw+MjdqAEOB0oAP4pInP1EI2ITAKeAq5RSg0e8WZKPQo8ClBeXh7f2EsU8NeYibdHD/Dzi+fF/TPNTIrDzglTs3h/d4M346ad5WWTx3pY5uPJ8wMfv/YV7fff7oJDW458ftnPYdLx8NnTsPmZI183AnfeeSfTpk3jpptuAuBnP/sZIsJ7771Hc3MzfX193HvvvSxfPjxgERyPx8ONN97Ixo0bsdvtPPDAA5xxxhls27aNa6+9lt7eXgYHB3nhhReYPHkyK1asoKqqioGBAX76059y2WUx27r0YcTQVwFT/B4XANUBzvlYKdUH7BORnWiGf4OIZACvAD9RSkVPKegoIsdr3B22hMPy2i2OXU4tcXP/6xVsr2mj3dNvbcSagJUrV3Lrrbf6DP26dev4+9//zm233UZGRgYNDQ0sXryYCy+8MKTVm55Hv2XLFnbs2ME555xDRUUFjzzyCLfccgtXXnklvb29DAwM8OqrrzJ58mReeUW7KbW2tgZ766hhxNBvAEpEpAg4CKwErhh2zkvA5cBaEXGjhXL2iogDeBH4g1Lqz9Eb9tGFvhGaO8FpLe9NwpJizdCv/WA/ACU5lqGPOqN44Jz7i+DPL7hS+zHIggULqKuro7q6mvr6erKyspg0aRK33XYb7733HgkJCRw8eJDa2lry8vIMv+/777/P9773PUBTqpw2bRoVFRWcfPLJ3HfffVRVVXHxxRdTUlLCvHnzuP3227nzzju54IILOPXUUw1/TiSMGqNXSvUDNwOvAV8C65RS20RkjYhc6D3tNaBRRLYD7wB3KKUagRXA14DVIrLZ+1MWk5mMIXpZ/FiEbSxiw7z8CaQn2Xn5c23xaomZmYNvfetbPP/88zz33HOsXLmSp59+mvr6ejZt2sTmzZvJzc0NqEMfjJH05q+44grWr19PcnIy3/jGN3j77bcpLS1l06ZNzJs3jx/96EesWbMmGtMaFUOllEqpV4FXhx272+9vBfzA++N/zh+BP0Y+zKMb3aPPmzB670iLYwO7LYFTZrh4bVst7jRH3DWULGLDypUr+c53vkNDQwP/+Mc/WLduHTk5OSQmJvLOO+9w4MCBkN/za1/7Gk8//TRnnnkmFRUVfPXVV8ycOZO9e/cyffp0vv/977N3716++OILZs2aRXZ2NldddRVpaWmHKVzGEqtmPgroejd5GZYxMBNLi928tq3WCtuYiDlz5tDe3k5+fj6TJk3iyiuv5Jvf/Cbl5eWUlZUxa9askN/zpptu4oYbbmDevHnY7XbWrl2L0+nkueee449//COJiYnk5eVx9913s2HDBu644w4SEhJITEzk4YcfjsEsj0RGWnaMFeXl5Wrjxo1jPYyQGBxU3PT0v1h18jROKXaP9XAsosS+hk7OuP9drjl5GvcsP3Zypo9WvvzyS4477rixHoYpCPRvKSKblFLlgc63PPookJAgPHL1iWM9DIsoU+hK4d/PLuXrVo2CxTGOZegtLEZARPjeWSVjPQyLMWTLli1cffXVhx1zOp188sknYzSi8LAMvYWFhcUIzJs3z9e0+1jGNDLFFhYWRz9H257gsUg4/4aWobewsIgLSUlJNDY2WsY+ApRSNDY2kpQUWs2OFbqxsLCICwUFBVRVVXEsChceTSQlJVFQUBDSayxDb2FhERcSExMpKioa62GMS6zQjYWFhYXJsQy9hYWFhcmxDL2FhYWFyTnqJBBEpB4IXVloCDfQMOpZ5sOa9/jCmvf4wsi8pymlArZqPeoMfaSIyMaR9B7MjDXv8YU17/FFpPO2QjcWFhYWJscy9BYWFhYmx4yG/tGxHsAYYc17fGHNe3wR0bxNF6O3sLCwsDgcM3r0FhYWFhZ+mMbQi8gyEdkpIrtF5K6xHk8sEZEnRKRORLb6HcsWkTdEZJf3d9ZYjjHaiMgUEXlHRL4UkW0icov3uNnnnSQin4rI59553+M9XiQin3jn/ZyIOMZ6rLFARGwi8pmI/K/38XiZ934R2SIim0Vko/dY2Ne6KQy9iNiAh4BzgdnA5SIye2xHFVPWAsuGHbsLeEspVQK85X1sJvqBf1dKHQcsBr7r/T82+7x7gDOVUvOBMmCZiCwG/hv4lXfezcC3x3CMseQW4Eu/x+Nl3gBnKKXK/NIqw77WTWHogUXAbqXUXqVUL/AssHyMxxQzlFLvAU3DDi8Hfu/9+/fARXEdVIxRStUopf7l/bsd7cufj/nnrZRSHd6Hid4fBZwJPO89brp5A4hIAXA+8Lj3sTAO5h2EsK91sxj6fKDS73GV99h4IlcpVQOaUQRyxng8MUNECoEFwCeMg3l7wxebgTrgDWAP0KKU6veeYtbr/UHgh8Cg97GL8TFv0G7mr4vIJhG53nss7GvdLDLFEuCYlU5kQkQkDXgBuFUp1aY5eeZGKTUAlIlIJvAicFyg0+I7qtgiIhcAdUqpTSJyun44wKmmmrcfS5RS1SKSA7whIjsieTOzePRVwBS/xwVA9RiNZayoFZFJAN7fdWM8nqgjIoloRv5ppdRfvIdNP28dpVQL8C7aHkWmiOiOmhmv9yXAhSKyHy0Ueyaah2/2eQOglKr2/q5Du7kvIoJr3SyGfgNQ4t2RdwArgfVjPKZ4sx64xvv3NcDLYziWqOONz/4O+FIp9YDfU2af90SvJ4+IJANfR9ufeAf4lvc0081bKfUjpVSBUqoQ7fv8tlLqSkw+bwARSRWRdP1v4BxgKxFc66YpmBKR89Du+DbgCaXUfWM8pJghIn8CTkdTtKsF/hN4CVgHTAW+Ai5VSg3fsD1mEZGlwD+BLQzFbP8DLU5v5nkfj7bxZkNzzNYppdaIyHQ0Tzcb+Ay4SinVM3YjjR3e0M3tSqkLxsO8vXN80fvQDjyjlLpPRFyEea2bxtBbWFhYWATGLKEbCwsLC4sRsAy9hYWFhcmxDL2FhYWFybEMvYWFhYXJsQy9hYWFhcmxDL2FhYWFybEMvYWFhYXJsQy9hYWFhcn5/9HV+6XweKegAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ft = models.resnet101(pretrained=False)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "model_ft,train_losses,val_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=500)\n",
    "dump_output(model_ft,train_losses[0:50],val_losses[0:50],'without-pretrained-resnet101_lrscheduler_wholenetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.9353 Acc: 0.5369\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.5369\n",
      "val Loss: 22.8180 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.9177 Acc: 0.5328\n",
      "train Rajat Best_Acc: 0.5686 Epoch_Acc: 0.5328\n",
      "val Loss: 2.2513 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.5686 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.9070 Acc: 0.5246\n",
      "train Rajat Best_Acc: 0.5686 Epoch_Acc: 0.5246\n",
      "val Loss: 4.6482 Acc: 0.4575\n",
      "val Rajat Best_Acc: 0.5686 Epoch_Acc: 0.4575\n",
      "\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.8547 Acc: 0.5615\n",
      "train Rajat Best_Acc: 0.5686 Epoch_Acc: 0.5615\n",
      "val Loss: 1.2681 Acc: 0.4575\n",
      "val Rajat Best_Acc: 0.5686 Epoch_Acc: 0.4575\n",
      "\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 1.0515 Acc: 0.5205\n",
      "train Rajat Best_Acc: 0.5686 Epoch_Acc: 0.5205\n",
      "val Loss: 0.8328 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.5686 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.8311 Acc: 0.5533\n",
      "train Rajat Best_Acc: 0.6209 Epoch_Acc: 0.5533\n",
      "val Loss: 1.6964 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.6209 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 1.1042 Acc: 0.5451\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.5451\n",
      "val Loss: 0.8220 Acc: 0.5359\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.5359\n",
      "\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.7557 Acc: 0.5451\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.5451\n",
      "val Loss: 0.7644 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.6515 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6393\n",
      "val Loss: 0.7806 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.6275 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.6910 Acc: 0.5656\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5656\n",
      "val Loss: 0.6408 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.6457 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6528 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.6719 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5902\n",
      "val Loss: 0.7105 Acc: 0.5425\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5425\n",
      "\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.6734 Acc: 0.5656\n",
      "train Rajat Best_Acc: 0.6732 Epoch_Acc: 0.5656\n",
      "val Loss: 0.7610 Acc: 0.7059\n",
      "val Rajat Best_Acc: 0.6732 Epoch_Acc: 0.7059\n",
      "\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.6832 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.5984\n",
      "val Loss: 0.6129 Acc: 0.5686\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.5686\n",
      "\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.6353 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5930 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.6389 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6215 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.6512 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6004 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.6482 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6154 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.6427 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6525 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.6637 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6268 Acc: 0.5490\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.5490\n",
      "\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.6479 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6132 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.6463 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6082 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.6275 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6095 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.6577 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6179 Acc: 0.5817\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.5817\n",
      "\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.6478 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.5980 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.6353 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6150 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.6526 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6042 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.6245 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5921 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.6301 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6066 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.6286 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5903 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.6596 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6025\n",
      "val Loss: 0.6212 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.6428 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6063 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.6393 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6093 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.6506 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6069 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.6201 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5948 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.6421 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6229 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.6294 Acc: 0.6926\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6926\n",
      "val Loss: 0.6078 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.6413 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6066 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.6209 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6036 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.6503 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6014 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.6495 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6043 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.6386 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6000 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.6540 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5987 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.6415 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6274 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.6524 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.5902\n",
      "val Loss: 0.6079 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.6638 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6147 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.6444 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.5984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6050 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.6500 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6189\n",
      "val Loss: 0.5885 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.6278 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6010 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.6373 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6049 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.6547 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6210 Acc: 0.5752\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.5752\n",
      "\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.6642 Acc: 0.5697\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.5697\n",
      "val Loss: 0.5962 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.6292 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6009 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.6575 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6148\n",
      "val Loss: 0.5971 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.6250 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6041 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.6280 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6083 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.6455 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6020 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.6386 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6075 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.6464 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7059 Epoch_Acc: 0.6230\n",
      "val Loss: 0.5910 Acc: 0.7190\n",
      "val Rajat Best_Acc: 0.7059 Epoch_Acc: 0.7190\n",
      "\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.6375 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6045 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.6353 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6004 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.6306 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6099 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.6423 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6040 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.6475 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6067 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.6142 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.5978 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.6449 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6225 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.6355 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6081 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.6424 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6040 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.6518 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6188 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.6531 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6025\n",
      "val Loss: 0.6153 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.6522 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6027 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.6398 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6022 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.6376 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6070 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.6460 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6076 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.6391 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6167 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.6405 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6041 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.6438 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5978 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.6722 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6066\n",
      "val Loss: 0.6128 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.6429 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6081 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.6562 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6145 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.6640 Acc: 0.5574\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5574\n",
      "val Loss: 0.6045 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.6324 Acc: 0.6967\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6967\n",
      "val Loss: 0.6046 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.6458 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6138 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.6353 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6148 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.6227 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6046 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.6188 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6093 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.6204 Acc: 0.7090\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7090\n",
      "val Loss: 0.6030 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.6529 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6003 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.6464 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6032 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.6218 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6140 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.6464 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6101 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.6379 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6023 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.6456 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6113 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 93/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6406 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6000 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.6313 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6040 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.6444 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6226 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.6403 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6159 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.6300 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6023 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.6484 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.5990 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.6303 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6077 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.6689 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6066 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.6414 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6113 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.6559 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6050 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.6305 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6050 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.6504 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6073 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.6234 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5997 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.6251 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5911 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.6371 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6089 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.6397 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5884 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.6462 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5994 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.6309 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6052 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.6314 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6032 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.6448 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6110 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.6147 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6055 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.6254 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6163 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.6567 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6094 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.6491 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6030 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.6689 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6060 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.6271 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6075 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.6487 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6066\n",
      "val Loss: 0.6110 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.6628 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.5980 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.6466 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.5967 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 122/499\n",
      "----------\n",
      "train Loss: 0.6411 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5952 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.6377 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6083 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.6505 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6084 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.6276 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5959 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.6285 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6019 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.6507 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6128 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.6449 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6004 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.6357 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6082 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.6205 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6233 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.6691 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6087 Acc: 0.5948\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5948\n",
      "\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.6413 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6066\n",
      "val Loss: 0.6075 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.6495 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5835 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.6598 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6066\n",
      "val Loss: 0.5895 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.6598 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5861\n",
      "val Loss: 0.6026 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.6385 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6021 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.6341 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5935 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.6544 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6106 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.6464 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6030 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.6596 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6048 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.6434 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6253 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.6445 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6150 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.6434 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5861\n",
      "val Loss: 0.6059 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.6487 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.5965 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.6109 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6007 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.6362 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6058 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.6485 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6025 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.6455 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6051 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.6342 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6103 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.6629 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6003 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.6276 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6096 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.6469 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.5983 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.6517 Acc: 0.5820\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5820\n",
      "val Loss: 0.6063 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.6313 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6137 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.6220 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6067 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.6532 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5792 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.6455 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6015 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.6377 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6090 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.6295 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5985 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.6284 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6083 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.6410 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6011 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.6565 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5861\n",
      "val Loss: 0.5941 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.6498 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6031 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.6350 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6072 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.6502 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6066\n",
      "val Loss: 0.6130 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.6389 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6137 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.6431 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6088 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.6506 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6005 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.6427 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6062 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.6363 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5940 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.6556 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6038 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.6720 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6044 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.6553 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6088 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.6519 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6126 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.6570 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5861\n",
      "val Loss: 0.6032 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.6501 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6058 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.6559 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6025\n",
      "val Loss: 0.6193 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.6367 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5835 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.6365 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6007 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.6549 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5861\n",
      "val Loss: 0.6142 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.6369 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6134 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.6406 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6068 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.6262 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6082 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.6745 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6066\n",
      "val Loss: 0.6095 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.6421 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6110 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 186/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6508 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6181 Acc: 0.5621\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5621\n",
      "\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.6340 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6095 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.6323 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6063 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.6343 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6057 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.6397 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6120 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.6485 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6022 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.6372 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.5992 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.6477 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6025\n",
      "val Loss: 0.6025 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.6315 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5917 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.6408 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6057 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.6312 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6034 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.6605 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5984\n",
      "val Loss: 0.6113 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.6763 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6127 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.6406 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6184 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.6425 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6111 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.6593 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6025\n",
      "val Loss: 0.5976 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.6479 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5975 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.6358 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6063 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.6447 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5984\n",
      "val Loss: 0.5980 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.6175 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6084 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.6474 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.5947 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.6312 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6086 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.6499 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.5897 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.6250 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6040 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.6443 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6230\n",
      "val Loss: 0.5944 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.6333 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6103 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.6412 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6032 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.6502 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6169 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.6302 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6162 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.6410 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6054 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.6375 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6045 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.6328 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6085 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.6287 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6098 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.6373 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6066 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.6353 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6063 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.6130 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.5930 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.6492 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5943\n",
      "val Loss: 0.6081 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.6288 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.5958 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.6242 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.5945 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.6476 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6036 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.6430 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6079 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.6405 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6069 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.6284 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6163 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.6226 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5997 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.6463 Acc: 0.5820\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5820\n",
      "val Loss: 0.6163 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.6269 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6123 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.6438 Acc: 0.5738\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.5877 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.6571 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5902\n",
      "val Loss: 0.5963 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.6481 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6253 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.6482 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.5941 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.6506 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6171 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.6382 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6070 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.6393 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6162 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.6384 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6221 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.6324 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5889 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.6572 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6025 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.6145 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6058 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.6302 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6037 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.6322 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6087 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.6458 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5937 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.6344 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6119 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.6323 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6221 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.6307 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6091 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.6206 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6027 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.6280 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6098 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.6522 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6066\n",
      "val Loss: 0.6099 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.6237 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.5955 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.6432 Acc: 0.6721\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6721\n",
      "val Loss: 0.6054 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.6461 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6070 Acc: 0.5882\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5882\n",
      "\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.6321 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7049\n",
      "val Loss: 0.6136 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.6414 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6085 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.6488 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6072 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.6444 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6229 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.6298 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6061 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.6352 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.5972 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.6394 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6057 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.6451 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6200 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.6489 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6074 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.6420 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6125 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.6359 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6077 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.6364 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6115 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.6672 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6035 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.6331 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.5999 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.6467 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6027 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.6183 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6088 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.6791 Acc: 0.5656\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5656\n",
      "val Loss: 0.6081 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.6301 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5873 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.6361 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5917 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.6419 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5943\n",
      "val Loss: 0.5976 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.6343 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6096 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.6422 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6087 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.6393 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6009 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.6369 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6029 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 279/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6417 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6055 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.6388 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.6071 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.6439 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6311\n",
      "val Loss: 0.6088 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.6498 Acc: 0.5984\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5984\n",
      "val Loss: 0.5945 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.6424 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6121 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.6390 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.5967 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.6330 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6066\n",
      "val Loss: 0.6001 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.6630 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6148\n",
      "val Loss: 0.6006 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.6333 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5994 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.6511 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6174 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.6327 Acc: 0.6311\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6311\n",
      "val Loss: 0.5937 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.6508 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6375 Acc: 0.6144\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6144\n",
      "\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.6371 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6285 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.6338 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6230\n",
      "val Loss: 0.5950 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.6237 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6049 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.6460 Acc: 0.5943\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5943\n",
      "val Loss: 0.6018 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.6640 Acc: 0.5820\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5820\n",
      "val Loss: 0.5990 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.6458 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6077 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.6353 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.6084 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.6275 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6639\n",
      "val Loss: 0.5984 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.6464 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6052 Acc: 0.6601\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6601\n",
      "\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.6273 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6030 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.6480 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.5983 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.6334 Acc: 0.6844\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6844\n",
      "val Loss: 0.6001 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.6599 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6046 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.6419 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6007 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.6420 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6076 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.6440 Acc: 0.6475\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6475\n",
      "val Loss: 0.6012 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.6274 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.5999 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.6597 Acc: 0.6189\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6189\n",
      "val Loss: 0.6206 Acc: 0.6993\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6993\n",
      "\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.6388 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6107\n",
      "val Loss: 0.6086 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.6308 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.5981 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.6521 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.5961 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.6621 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5861\n",
      "val Loss: 0.6110 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.6226 Acc: 0.6680\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6680\n",
      "val Loss: 0.6040 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.6450 Acc: 0.6270\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6270\n",
      "val Loss: 0.6097 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.6303 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.5994 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.6343 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.6082 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.6395 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6014 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.6397 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6674 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.6484 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6098 Acc: 0.6078\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6078\n",
      "\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.6307 Acc: 0.6516\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6516\n",
      "val Loss: 0.5985 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.6524 Acc: 0.5902\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5902\n",
      "val Loss: 0.6219 Acc: 0.6340\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6340\n",
      "\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.6248 Acc: 0.6803\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6803\n",
      "val Loss: 0.6042 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.6472 Acc: 0.6598\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6598\n",
      "val Loss: 0.6032 Acc: 0.6536\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6536\n",
      "\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.6495 Acc: 0.6352\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6352\n",
      "val Loss: 0.6156 Acc: 0.6209\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6209\n",
      "\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.6463 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6146 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.6434 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6107\n",
      "val Loss: 0.5992 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.6401 Acc: 0.6025\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6025\n",
      "val Loss: 0.6098 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.6403 Acc: 0.6066\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6066\n",
      "val Loss: 0.6059 Acc: 0.6275\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6275\n",
      "\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.6460 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6164 Acc: 0.6013\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6013\n",
      "\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.6283 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.5961 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.6507 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6230\n",
      "val Loss: 0.5917 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.6672 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6107\n",
      "val Loss: 0.5814 Acc: 0.7190\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.7190\n",
      "\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.6486 Acc: 0.6434\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6434\n",
      "val Loss: 0.6053 Acc: 0.6797\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6797\n",
      "\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.6701 Acc: 0.5861\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.5861\n",
      "val Loss: 0.6020 Acc: 0.6863\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6863\n",
      "\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.6586 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6230\n",
      "val Loss: 0.6099 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.6610 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6148\n",
      "val Loss: 0.5992 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.6313 Acc: 0.6762\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6762\n",
      "val Loss: 0.6040 Acc: 0.6471\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6471\n",
      "\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.6501 Acc: 0.6557\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6557\n",
      "val Loss: 0.6029 Acc: 0.6405\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6405\n",
      "\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.6347 Acc: 0.6393\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6393\n",
      "val Loss: 0.5871 Acc: 0.6732\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6732\n",
      "\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.6485 Acc: 0.6107\n",
      "train Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6107\n",
      "val Loss: 0.5989 Acc: 0.6667\n",
      "val Rajat Best_Acc: 0.7190 Epoch_Acc: 0.6667\n",
      "\n",
      "Epoch 341/499\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f202b554aec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m model_ft,train_losses,val_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m---> 15\u001b[0;31m                        num_epochs=500)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdump_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'without-pretrained-resnet152_lrscheduler_wholenetwork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c907b7687a7c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet152(pretrained=False)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "model_ft,train_losses,val_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=500)\n",
    "dump_output(model_ft,train_losses[0:50],val_losses[0:50],'without-pretrained-resnet152_lrscheduler_wholenetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
