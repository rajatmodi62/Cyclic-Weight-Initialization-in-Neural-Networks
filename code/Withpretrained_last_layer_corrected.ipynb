{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=8,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                #print(inputs.size())\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            print('{} Rajat Best_Acc: {:.4f} Epoch_Acc: {:.4f}'.format(\n",
    "                phase, best_acc, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                \n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            #collect losses\n",
    "            if phase=='train':\n",
    "                train_losses.append(epoch_loss)\n",
    "            if phase=='val':\n",
    "                val_losses.append(epoch_loss)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model,train_losses,val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            print(\"modi\",inputs.size())\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def plot_losses(train_loss,val_loss):\n",
    "    df = pd.DataFrame(list(zip([i for i in range(0,len(train_losses))],train_loss,val_loss)), \n",
    "               columns =['epoch', 'train_loss','val_loss']) \n",
    "    list_data=[df.train_loss,df.val_loss]\n",
    "    plots=sns.lineplot(data=list_data)\n",
    "    return plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_output(model,train_loss,val_loss,experiment_name):\n",
    "    model_dump_path=experiment_name+'.pt'\n",
    "    torch.save(model.state_dict(),model_dump_path)\n",
    "    csv_dump_path=experiment_name+'.csv'\n",
    "    df = pd.DataFrame(list(zip(train_loss,val_loss)), \n",
    "           columns =[ 'train_loss','val_loss'])\n",
    "    df.to_csv(csv_dump_path)\n",
    "    list_data=[df.train_loss,df.val_loss]\n",
    "    plot_dump_path=experiment_name+'.png'\n",
    "    plots=sns.lineplot(data=list_data)\n",
    "    fig=plots.get_figure()\n",
    "    fig.savefig(plot_dump_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.5518 Acc: 0.7049\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.7049\n",
      "val Loss: 0.3631 Acc: 0.8366\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.8366\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3339 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.8366 Epoch_Acc: 0.8730\n",
      "val Loss: 0.2109 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.8366 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.3653 Acc: 0.8402\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8402\n",
      "val Loss: 0.1984 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.3148 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1999 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2830 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1745 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2697 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1658 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.2359 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1556 Acc: 0.9739\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9739\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.2398 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1540 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.2282 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1582 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.2498 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1503 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.2389 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1571 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.2213 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1503 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.3178 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1551 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.2247 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1546 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.2725 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1531 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1991 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1538 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.2079 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1646 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.2383 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1599 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.2366 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1602 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.2423 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1530 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.2659 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1621 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.2768 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1586 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.2477 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1664 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.2545 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1608 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1718 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1534 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.2071 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1582 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.2082 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1602 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.3001 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1629 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.2679 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1534 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.2447 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1581 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.2791 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1509 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.2231 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1540 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.2565 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1643 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.2843 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1655 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.2262 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1637 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.2238 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1605 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.2340 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1588 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1946 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1623 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.2286 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1561 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.2117 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1530 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.2219 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1589 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.2345 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1635 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.2458 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1610 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.2026 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1526 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.2984 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1601 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.2296 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1561 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.2194 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1799 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.2503 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1570 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.2215 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1529 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.2230 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1674 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.2439 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1621 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.2834 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1569 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.2911 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1571 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.2074 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1587 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.2702 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1589 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.2280 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1564 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.2822 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1538 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.3224 Acc: 0.8607\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8607\n",
      "val Loss: 0.1599 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.2868 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1582 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.2251 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1614 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.2457 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1575 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.2972 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1692 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.2326 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1528 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.2346 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1614 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.2011 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1580 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.3214 Acc: 0.8525\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8525\n",
      "val Loss: 0.1560 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.2135 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1588 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.2273 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1495 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.2343 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1643 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.2519 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1564 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.2536 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1518 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.2398 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1561 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.2384 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1571 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.2504 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1837 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.3159 Acc: 0.8648\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8648\n",
      "val Loss: 0.1556 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.2381 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1534 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.3292 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1543 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.2790 Acc: 0.8648\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8648\n",
      "val Loss: 0.1552 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.2493 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1560 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.2075 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1601 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.2863 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1585 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.2424 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1554 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.2203 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1531 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.2359 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1659 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.2574 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1555 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.2045 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1538 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.2505 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1550 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.2095 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1529 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.2204 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1689 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.2129 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1557 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.2616 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1534 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.1902 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1531 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.2317 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1597 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2590 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1570 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.2798 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1605 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.2570 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1561 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.2622 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1499 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.2580 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1714 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.2460 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1630 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.2127 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1590 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.2538 Acc: 0.8607\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8607\n",
      "val Loss: 0.1661 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.2239 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1572 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.2754 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1605 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.2189 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1579 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.2426 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1567 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.2106 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1600 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.2655 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1510 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.2397 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1704 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.2473 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1645 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.2376 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1569 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.3160 Acc: 0.8607\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8607\n",
      "val Loss: 0.1577 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.1994 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1520 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.2380 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1559 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.2116 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1587 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.1764 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1726 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.2346 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1583 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.2053 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1559 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.2275 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1673 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.2377 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1823 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.2448 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1670 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.2345 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1683 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.2862 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1565 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.2170 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1569 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.2214 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1551 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.2044 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1545 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.2441 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1495 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.2509 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1577 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.2175 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1586 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.2106 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1681 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.2610 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1555 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.2236 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1620 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.2562 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1585 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.2028 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1535 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.2855 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1500 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.2781 Acc: 0.8648\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8648\n",
      "val Loss: 0.1608 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.3054 Acc: 0.8566\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8566\n",
      "val Loss: 0.1680 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.2417 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1565 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.2593 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1584 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.2658 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1608 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.2439 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1696 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.3038 Acc: 0.8607\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8607\n",
      "val Loss: 0.1554 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.1852 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1603 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.2012 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1524 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.2041 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1520 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.2295 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1533 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.2487 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1630 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.2816 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1603 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.2830 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1520 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.1968 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1595 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.1801 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1505 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.2599 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1548 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.2134 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1525 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.2128 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1639 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.2481 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1552 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.1941 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1446 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.2049 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1633 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.2506 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1565 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.2867 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1533 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.2383 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1628 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.2038 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1593 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.2294 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1688 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.2266 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1586 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.2095 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1678 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.2507 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1627 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.2002 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1502 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.2031 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1580 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.2402 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1561 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.2303 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1684 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.2721 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1557 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.2338 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1568 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.1989 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1589 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.2270 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1568 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.2443 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1534 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.2078 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1591 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.2243 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1593 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.2542 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1542 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.2051 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1568 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.2369 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1842 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.2188 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1591 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.2532 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1614 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.2600 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1599 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.1988 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1594 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.2324 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1518 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.2851 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1582 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.2087 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1624 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.1853 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1581 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2977 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1645 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.1736 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1616 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.3339 Acc: 0.8279\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8279\n",
      "val Loss: 0.1593 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.1908 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1585 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.2500 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1542 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.2017 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1549 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.2457 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1560 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.2684 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1629 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.2459 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1562 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.2492 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1573 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.2349 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1628 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.2481 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1598 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.2246 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1578 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.2527 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1596 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.2495 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1549 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.2310 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1604 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.1949 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1759 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.2113 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1623 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.1692 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1625 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.2420 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1567 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.2262 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1577 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.2709 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1575 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.2094 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1585 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.2401 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1573 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.2470 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1618 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.2228 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1573 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.2054 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1619 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.1959 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1684 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.2205 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1623 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.2468 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1627 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.2540 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1583 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.2353 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1497 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.2704 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1664 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.2188 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1560 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.2522 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1608 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.2822 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1628 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.2658 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1689 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.2497 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1560 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.3253 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1514 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.2343 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1589 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.1720 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1567 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.2440 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1703 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.2842 Acc: 0.8607\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8607\n",
      "val Loss: 0.1513 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.2469 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1620 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.1999 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1572 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.1817 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1624 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.2478 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1488 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.2296 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1541 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.1962 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1552 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.2512 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1611 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.1622 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1561 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.2428 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1727 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.2599 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1618 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.2710 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1632 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.2272 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1613 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.2059 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1641 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.2237 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1583 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.2217 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1609 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.2303 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1586 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.2350 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1587 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.2177 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1502 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.1840 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1603 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.2080 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1605 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.2330 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1536 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.2681 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1563 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.2043 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1612 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.2050 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1580 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.2079 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1512 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.2386 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1558 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.1799 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1528 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.2551 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1546 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.2474 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1613 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.2803 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1540 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.2541 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1755 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.2777 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1584 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.2484 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1515 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.3147 Acc: 0.8648\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8648\n",
      "val Loss: 0.1716 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.1790 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1570 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.2465 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1859 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.1982 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1718 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.2931 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1504 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.1944 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1533 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.2546 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1532 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.2828 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1485 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.2298 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1533 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.2721 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1548 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.2022 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1522 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.2598 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1542 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.2008 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1561 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.2338 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1549 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.2908 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1634 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.2514 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1566 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.2442 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1638 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2685 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1685 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.2625 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1583 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.2171 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1547 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.2498 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1543 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.2413 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1607 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.2608 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1740 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.1826 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1652 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.2157 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1540 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.2207 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1698 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.2818 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1616 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.1987 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1517 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.2470 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1567 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.2595 Acc: 0.8607\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8607\n",
      "val Loss: 0.1540 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.2488 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1537 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.2356 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1561 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.3018 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1573 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.2310 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1580 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.1838 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1590 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.2865 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1606 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.2116 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1703 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.2366 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1601 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9739 Epoch_Acc: 0.9542\n",
      "\n",
      "Training complete in 14m 37s\n",
      "Best val Acc: 0.973856\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model_conv,train_losses,val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n",
    "                       num_epochs=300)\n",
    "dump_output(model_conv,train_losses[0:50],val_losses[0:50],'with-pretrained-resnet18_lrscheduler_lastlayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.6089 Acc: 0.6639\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6639\n",
      "val Loss: 0.2550 Acc: 0.9085\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.9085\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3671 Acc: 0.8484\n",
      "train Rajat Best_Acc: 0.9085 Epoch_Acc: 0.8484\n",
      "val Loss: 0.1565 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9085 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.2703 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1481 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2969 Acc: 0.8648\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8648\n",
      "val Loss: 0.1289 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.3637 Acc: 0.8361\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8361\n",
      "val Loss: 0.1270 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2048 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1431 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.4371 Acc: 0.8320\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8320\n",
      "val Loss: 0.1310 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.2193 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1348 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.2522 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1213 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.2315 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1384 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.2194 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1282 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.2748 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1258 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.2680 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1315 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.1978 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1390 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.1979 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1695 Acc: 0.9150\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9150\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.2517 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1331 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.2131 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1316 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.2389 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1301 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.2183 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1339 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.1740 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1259 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.1888 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1295 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.1697 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1307 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.2278 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1274 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.1512 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1229 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1943 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1257 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.2060 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1312 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.1644 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1322 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.2707 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1488 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.2305 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1354 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.1789 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1291 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.2250 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1352 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.2311 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1200 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.2194 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1336 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.1700 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1285 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.2104 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1325 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.2488 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1201 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.2214 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1304 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.2181 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1367 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.2102 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1374 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.2183 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1282 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.1895 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1289 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.2298 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1261 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1986 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1233 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.2433 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1220 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.2573 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1320 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.2208 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1313 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.1578 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1345 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.2180 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1310 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.2075 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1293 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.1693 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1221 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.2733 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1218 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.2529 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1327 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.1929 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1383 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.2269 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1301 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.1695 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1315 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.2570 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1430 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.1656 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1422 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.2286 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1294 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.2273 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1226 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.2518 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1268 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.2058 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1275 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.1695 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1221 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.2252 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1353 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.1992 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1274 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.1872 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1271 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.1646 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1341 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.1922 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1352 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.2072 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1342 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.1918 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1333 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.1626 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1238 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.2300 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1301 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.2369 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1239 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.2092 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1191 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.1788 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1274 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.2157 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1267 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.2411 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1360 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.1647 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1302 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.2106 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1280 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.1547 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1259 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.1817 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1378 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.2120 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1305 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.2191 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1422 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.1624 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1460 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.1944 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1243 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.1980 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1346 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.2268 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1240 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.2483 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1271 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.2002 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1301 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.2226 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1316 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.1610 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1444 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.2372 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1264 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.1715 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1291 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.2214 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1379 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2023 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1227 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.2416 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1188 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.2351 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1224 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.2597 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1256 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.2678 Acc: 0.8607\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8607\n",
      "val Loss: 0.1284 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.1969 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1360 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.1822 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1284 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.1475 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1264 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.1865 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1476 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.2325 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1317 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.1721 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1240 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.1822 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1340 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.2178 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1245 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.2208 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1237 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.2349 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1293 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.2411 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1390 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.2020 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1266 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.2091 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1219 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.2069 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1344 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.1665 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1291 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.1821 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1257 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.2338 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1451 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.1836 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1225 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.1784 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1388 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.1855 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1241 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.1500 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1269 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.1964 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1369 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.2424 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1220 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.2620 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1292 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.1829 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1262 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.2131 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1255 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.2348 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1185 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.2582 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1230 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.1517 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1339 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.2123 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1345 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.2252 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1313 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.2562 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1274 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.1836 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1288 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.1726 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1181 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.2091 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1277 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.2253 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1286 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.2107 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1422 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.2000 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1224 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.1841 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1292 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.1690 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1222 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.2025 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1188 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.1807 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1203 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.2285 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1286 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.1595 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1445 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.1558 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1282 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.2006 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1248 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.1815 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1277 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.2193 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1198 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.2112 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1242 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.1655 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1279 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.1527 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1297 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.2279 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1221 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.1839 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1223 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.2329 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1335 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.1868 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1334 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.1975 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1400 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.2955 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1218 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.2020 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1234 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.1807 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1505 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.2038 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1265 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.2736 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1261 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.1841 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1270 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.1988 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1347 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.1606 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1274 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.2559 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1263 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.1841 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1238 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.1974 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1325 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.2577 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1240 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.1910 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1236 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.2452 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1191 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.2297 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1412 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.2017 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1352 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.2012 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1301 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.2226 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1268 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.2012 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1200 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.1808 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1329 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.2508 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1300 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.2183 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1346 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.1646 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1271 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.2097 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1617 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.1970 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1255 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.2038 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1297 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.2068 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1301 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.2394 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1257 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.2196 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1366 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.2601 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1318 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.1771 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1313 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.2449 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1307 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2220 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1322 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.2361 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1254 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.2330 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1187 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.2109 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1205 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.2120 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1344 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.1714 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1462 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.1565 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1258 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.2033 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1309 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.1965 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1274 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.2352 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1409 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.2359 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1211 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.2257 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1306 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.2262 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1302 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.1591 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1359 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.2576 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1227 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.2198 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1300 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.2633 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1297 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.1735 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1272 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.2188 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1322 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.2125 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1285 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.2535 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1308 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.1771 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1262 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.1955 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1344 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.1665 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1275 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.2059 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1353 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.2389 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1234 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.2625 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1271 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.2249 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1344 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.1727 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1322 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.1928 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1267 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.1734 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1364 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.2020 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1272 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.1750 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1239 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.2727 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1324 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.2360 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1291 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.2033 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1214 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.1850 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1262 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.1601 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1401 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.2712 Acc: 0.8648\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8648\n",
      "val Loss: 0.1223 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.2251 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1281 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.2157 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1278 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.2310 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1200 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.1909 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1246 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.1533 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1337 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.2146 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1369 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.2493 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1132 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.2214 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1198 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.2612 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1248 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.2445 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1294 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.1861 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1242 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.2577 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1234 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.1844 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1390 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.1701 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1401 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.3030 Acc: 0.8443\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8443\n",
      "val Loss: 0.1223 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.2038 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1243 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.2012 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1412 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.2203 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1216 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.1996 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1346 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.2104 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1359 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.2175 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1219 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.1559 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1337 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.2451 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1292 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.1900 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1628 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.1706 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1287 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.1920 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1316 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.2694 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1294 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.1616 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1226 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.1856 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1246 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.2155 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1322 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.1806 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1238 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.1906 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1205 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.2444 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1562 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.2283 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1343 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.2503 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1284 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.2069 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1225 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.1359 Acc: 0.9631\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9631\n",
      "val Loss: 0.1337 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.2150 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1297 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.1708 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1295 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.2265 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1306 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.1999 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1383 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.1597 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1268 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.2448 Acc: 0.8648\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8648\n",
      "val Loss: 0.1247 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.2028 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1466 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.2070 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1287 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.2451 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1415 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.2661 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1230 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.1830 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1343 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.1621 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1347 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.2354 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1305 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.1561 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1276 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.2779 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1272 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.2455 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1323 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.2179 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1260 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2145 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1323 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.1970 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1335 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.1810 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1210 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.2063 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1364 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.1286 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1200 Acc: 0.9673\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9673\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.2271 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1268 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.1634 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1331 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.1667 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1298 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.1702 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1300 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.2215 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1493 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.1980 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1208 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.2602 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1298 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.1331 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1247 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.2265 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1317 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.2260 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1362 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.2314 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1338 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.2227 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1305 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.1894 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1159 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.2348 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1298 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.2089 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1266 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.1821 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1230 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9673 Epoch_Acc: 0.9477\n",
      "\n",
      "Training complete in 14m 33s\n",
      "Best val Acc: 0.967320\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet34(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model_conv,train_losses,val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n",
    "                       num_epochs=300)\n",
    "dump_output(model_conv,train_losses[0:50],val_losses[0:50],'with-pretrained-resnet34_lrscheduler_lastlayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.5868 Acc: 0.6148\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6148\n",
      "val Loss: 0.2907 Acc: 0.9085\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.9085\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3385 Acc: 0.8648\n",
      "train Rajat Best_Acc: 0.9085 Epoch_Acc: 0.8648\n",
      "val Loss: 0.2155 Acc: 0.9150\n",
      "val Rajat Best_Acc: 0.9085 Epoch_Acc: 0.9150\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.3011 Acc: 0.8607\n",
      "train Rajat Best_Acc: 0.9150 Epoch_Acc: 0.8607\n",
      "val Loss: 0.1784 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9150 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.3240 Acc: 0.8484\n",
      "train Rajat Best_Acc: 0.9216 Epoch_Acc: 0.8484\n",
      "val Loss: 0.1686 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9216 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2617 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9281 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1529 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9281 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2949 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1513 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.3029 Acc: 0.8566\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8566\n",
      "val Loss: 0.1543 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.2305 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1570 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.1969 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1379 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.1908 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1653 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.2070 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1492 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.1933 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1416 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.1659 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1666 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.2148 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1521 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.2025 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1449 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1982 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1415 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.2063 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1514 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.1965 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1525 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.2292 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1460 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.2020 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1447 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.2322 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1518 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.1828 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1390 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.1557 Acc: 0.9672\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9672\n",
      "val Loss: 0.1582 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.1991 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1439 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.2302 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1420 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.1654 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1531 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.1531 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1496 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.2206 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1547 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.2024 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1444 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.1898 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1425 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.1795 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1565 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.1809 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1392 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.2535 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1518 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.1489 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1425 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.2028 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1493 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.2277 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1408 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.1944 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1507 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.2283 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1519 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.1943 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1432 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.2254 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1541 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.2430 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1448 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.2031 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1551 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1952 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1489 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.2583 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1515 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.2082 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1380 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.2132 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1374 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.2250 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1383 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.2134 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1619 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.2536 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1393 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.2067 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1534 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.1882 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1428 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.1845 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1572 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.2116 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1451 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.2192 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1463 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.1830 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1494 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.2444 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1519 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.1805 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1640 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.1708 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1425 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.1512 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1481 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.1601 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1428 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.2148 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1427 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.2150 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1518 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.2061 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1504 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.1625 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1517 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.2021 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1529 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.2138 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1387 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.2294 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1445 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.2228 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1489 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.2328 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1391 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.1978 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1426 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.2134 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1414 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.1627 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1452 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.2052 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1410 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.1837 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1520 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.2470 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1510 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.1732 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1471 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.2235 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1467 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.1791 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1481 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.2032 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1512 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.1610 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1451 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.1959 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1524 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.1963 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1479 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.2071 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1490 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.2355 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1422 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.2156 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1528 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.1883 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1363 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.1687 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1401 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.1717 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1569 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.2178 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1519 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.2134 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1377 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.1858 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1422 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.2225 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1398 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.2518 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1405 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1635 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1421 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.2020 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1560 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.1536 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1654 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.2401 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1415 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.1913 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1551 Acc: 0.9608\n",
      "val Rajat Best_Acc: 0.9542 Epoch_Acc: 0.9608\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.2705 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1541 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.2140 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1443 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.2347 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1449 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.1794 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1381 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.1902 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1544 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.1428 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1499 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.2203 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1425 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.2591 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1491 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.1831 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1570 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.2457 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1599 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.1882 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1356 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.2660 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1461 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.1767 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1320 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.2288 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1457 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.1941 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1508 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.2592 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1401 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.1609 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1419 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.2117 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1498 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.1950 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1447 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.2319 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1451 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.2088 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1422 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.2390 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1413 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.1535 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1513 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.2920 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1429 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.1742 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1538 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.2068 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1441 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.2084 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1451 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.2087 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1616 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.1655 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1533 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.2339 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1433 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.1796 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1478 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.1971 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1410 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.1830 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1485 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.2255 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1465 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.1605 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1439 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.1901 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1381 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.2403 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1513 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.1861 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1419 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.2468 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1355 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.1959 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1550 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.2435 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1542 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.3033 Acc: 0.8566\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1544 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.1957 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1508 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.2152 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1378 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.2131 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1458 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.2271 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1348 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.2225 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1489 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.1705 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1510 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.2361 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1464 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.2385 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1390 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.2466 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1451 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.1995 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1440 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.2354 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1509 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.2246 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1424 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.2478 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1414 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.1824 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1459 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.2011 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1354 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.1910 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1420 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.1972 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1480 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.2177 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1383 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.1925 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1447 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.2209 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1409 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.2030 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1426 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.2306 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1465 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.2056 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1408 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.2207 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1529 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.2017 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1473 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.1896 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1400 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.2101 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1448 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.2260 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1360 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.2680 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1431 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.1740 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1536 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.2044 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1490 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.1952 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1517 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.2103 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1421 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.1836 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1505 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.1656 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1410 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.1988 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1561 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.2048 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1440 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.1804 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1469 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.2333 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1390 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.1956 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1480 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.1678 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1490 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.1877 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1502 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.1978 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1467 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.1802 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1404 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.2137 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1360 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.2270 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1637 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1720 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1437 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.1836 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1417 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.2258 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1422 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.2334 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1507 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.1380 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1460 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.2275 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1542 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.2068 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1504 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.1928 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1486 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.2047 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1343 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.2058 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1504 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.2143 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1547 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.2352 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1539 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.1782 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1501 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.2046 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1431 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.2070 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1445 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.2066 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1570 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.1967 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1426 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.2109 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1696 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.2620 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1530 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.2322 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1439 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.2757 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1402 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.1994 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1446 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.2044 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1402 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.2133 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1458 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.2195 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1621 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.1968 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1415 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.2142 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1322 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.1644 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1425 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.2126 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1424 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.1846 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1404 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.1599 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1511 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.2603 Acc: 0.8730\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8730\n",
      "val Loss: 0.1412 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.2337 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1447 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.2241 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1405 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.1933 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1501 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.2039 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1443 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.1889 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1475 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.1699 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1437 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.1759 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1420 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.2430 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1368 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.2444 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1479 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.2326 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1463 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.2301 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1411 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.2489 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1471 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.1498 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1529 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.2190 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1433 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.2386 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1379 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.2535 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1476 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.1567 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1376 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.2078 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1586 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.2402 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1410 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.1632 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1575 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.2169 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1531 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.2621 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1448 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.1974 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1417 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.1730 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1434 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.1737 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1579 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.2061 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1473 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.2126 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1389 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.1931 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1381 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.2114 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1430 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.1625 Acc: 0.9672\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9672\n",
      "val Loss: 0.1605 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.2184 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1909 Acc: 0.9150\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9150\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.1713 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1418 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.1896 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1389 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.2110 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1475 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.1954 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1517 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.2387 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1469 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.1954 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1445 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.2012 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1478 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.2144 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1412 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.2523 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1625 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.2052 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1540 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.1946 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1454 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.1964 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1411 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.2086 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1503 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.2008 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1495 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.2369 Acc: 0.8811\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8811\n",
      "val Loss: 0.1585 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.1893 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1413 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.1815 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1459 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.2138 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1396 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.1845 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1426 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.1789 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1558 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.1969 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1491 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.1845 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1544 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.1772 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1470 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.2153 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1360 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.2089 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1476 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.1860 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1432 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.1995 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1427 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.2158 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1524 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.1805 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1407 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.1920 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1419 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2552 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1428 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.2034 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1631 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.2069 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1484 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.1975 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1385 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.2052 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1493 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.2016 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1552 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.1829 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1473 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.1711 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1400 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.2408 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1535 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.2005 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1461 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.2210 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1386 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.2214 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1437 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.2462 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1416 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.1792 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1488 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.2252 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1418 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.1845 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1725 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.2638 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1355 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.1994 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1457 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.2126 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1611 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.1590 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1398 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.2282 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1452 Acc: 0.9542\n",
      "val Rajat Best_Acc: 0.9608 Epoch_Acc: 0.9542\n",
      "\n",
      "Training complete in 14m 45s\n",
      "Best val Acc: 0.960784\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3hUVfrA8e/JpPdCEtJDCaEECL1ZUCwgCnax467r2l1d19XfFl3XXbeq64pYdu0dFEVFWUWRjrQACTWEhBQghfSezPn9cZOQMkkmdZjJ+3keHjL33rlzbnLnnTPvaUprjRBCCPvnZOsCCCGE6B0S0IUQwkFIQBdCCAchAV0IIRyEBHQhhHAQzrZ64UGDBunY2FhbvbwQQtilHTt25Gutgy3tsyqgK6XmAv8CTMB/tNZ/sXDMtcATgAZ2a61v6OicsbGxbN++3ZqXF0II0UApldHevk4DulLKBCwBLgSygG1KqZVa633NjokDHgNmaa0LlVIhPS+2EEKIrrAmhz4VSNVap2mta4APgIWtjvkZsERrXQigtc7t3WIKIYTojDUBPQLIbPY4q2FbcyOAEUqpjUqpLQ0pmjaUUncopbYrpbbn5eV1r8RCCCEssiaHrixsaz1fgDMQB8wGIoH1SqkErXVRiydp/QrwCsDkyZNlzgEhHExtbS1ZWVlUVVXZuih2z93dncjISFxcXKx+jjUBPQuIavY4EsixcMwWrXUtcFQpdRAjwG+zuiRCCLuXlZWFj48PsbGxKGWpLiisobWmoKCArKwshgwZYvXzrEm5bAPilFJDlFKuwCJgZatjPgXOA1BKDcJIwaRZXQohhEOoqqoiKChIgnkPKaUICgrq8jedTgO61roOuBdYDewHPtJapyilnlRKLWg4bDVQoJTaB3wP/EprXdClkgghHIIE897Rnd+jVf3QtdargFWttv2+2c8aeKjhX5/aln6KtQdz+eWF8Tg5yY0jhBCN7G7of9KxIpZ8f4SymjpbF0UIIc4odhfQfdyNLxWlVRLQhRAtFRUV8eKLL3b5eZdccglFRUWdH9jK4sWLWb58eZef11fsLqB7NwT0MgnoQohW2gvo9fX1HT5v1apV+Pv791Wx+o3NJufqLh93o09maVWtjUsihOjIHz5PYV9OSa+ec3S4L49fNqbd/Y8++ihHjhwhMTERFxcXvL29CQsLIykpiX379nH55ZeTmZlJVVUVDzzwAHfccQdwem6psrIy5s2bx1lnncWmTZuIiIjgs88+w8PDo9OyrVmzhocffpi6ujqmTJnC0qVLcXNz49FHH2XlypU4Oztz0UUX8Y9//INly5bxhz/8AZPJhJ+fH+vWreuV34/dBXRvt4aUS7XU0IUQLf3lL38hOTmZpKQk1q5dy/z580lOTm7qy/3aa68RGBhIZWUlU6ZM4aqrriIoKKjFOQ4fPsz777/Pq6++yrXXXsvHH3/MTTfd1OHrVlVVsXjxYtasWcOIESO45ZZbWLp0KbfccgsrVqzgwIEDKKWa0jpPPvkkq1evJiIiolupnvbYXUD3lRy6EHaho5p0f5k6dWqLgTnPP/88K1asACAzM5PDhw+3CehDhgwhMTERgEmTJpGent7p6xw8eJAhQ4YwYsQIAG699VaWLFnCvffei7u7O7fffjvz58/n0ksvBWDWrFksXryYa6+9liuvvLI3LhWwwxx6Y8pFcuhCiM54eXk1/bx27Vq+/fZbNm/ezO7du5kwYYLFgTtubm5NP5tMJurqOo81Rs/ttpydnfnxxx+56qqr+PTTT5k715jm6qWXXuKpp54iMzOTxMRECgp6Z9iO3dXQvZtq6JJDF0K05OPjQ2lpqcV9xcXFBAQE4OnpyYEDB9iyZUuvve7IkSNJT08nNTWV4cOH8/bbb3PuuedSVlZGRUUFl1xyCdOnT2f48OEAHDlyhGnTpjFt2jQ+//xzMjMz23xT6A67C+heriaUgjLJoQshWgkKCmLWrFkkJCTg4eFBaGho0765c+fy0ksvMW7cOOLj45k+fXqvva67uzuvv/4611xzTVOj6J133smpU6dYuHAhVVVVaK159tlnAfjVr37F4cOH0VozZ84cxo8f3yvlUO19VehrkydP1t1dsWjsE6u5amIkTyywfY5OCHHa/v37GTVqlK2L4TAs/T6VUju01pMtHW93OXQAX3cXSiTlIoQQLdhdygWMrovSKCqE6C/33HMPGzdubLHtgQce4LbbbrNRiSyzy4Du4+4s3RaFEP1myZIlti6CVewy5eLj7iyNokII0YpdBnRvdxfptiiEEK3YZUCXGroQQrRlnwHdzZkSyaELIUQL9hnQ3Z2pqTNTXdfxlJhCCNERb2/vdvelp6eTkJDQj6XpObsM6I0zLkrXRSGEOM1Ouy02zoleR5C3WydHCyFs5vX5lrff9qXx/1ePwom9bffPfRrCxsGudyHpvbbPa8evf/1rYmJiuPvuuwF44oknUEqxbt06CgsLqa2t5amnnmLhwoVduoyqqiruuusutm/fjrOzM8888wznnXceKSkp3HbbbdTU1GA2m/n4448JDw/n2muvJSsri/r6en73u99x3XXXden1ustOA3pDDV0aRoUQzSxatIhf/OIXTQH9o48+4uuvv+bBBx/E19eX/Px8pk+fzoIFC1DK+kXmG/uh7927lwMHDnDRRRdx6NAhXnrpJR544AFuvPFGampqqK+vZ9WqVYSHh/Pll8aHT3Fxce9faDvsMqA3zrgow/+FOMN1UqNm3l863j/hRuOflSZMmEBubi45OTnk5eUREBBAWFgYDz74IOvWrcPJyYns7GxOnjzJ4MGDrT7vhg0buO+++wBjZsWYmBgOHTrEjBkz+NOf/kRWVhZXXnklcXFxjB07locffphf//rXXHrppZx99tlWv05P2WUO3VfmRBdCtOPqq69m+fLlfPjhhyxatIh3332XvLw8duzYQVJSEqGhoRbnQe9Ie5MY3nDDDaxcuRIPDw8uvvhivvvuO0aMGMGOHTsYO3Ysjz32GE8++WRvXJZV7LOG7iarFgkhLFu0aBE/+9nPyM/P54cffuCjjz4iJCQEFxcXvv/+ezIyMrp8znPOOYd3332X888/n0OHDnHs2DHi4+NJS0tj6NCh3H///aSlpbFnzx5GjhxJYGAgN910E97e3rzxxhu9f5HtsMuALjl0IUR7xowZQ2lpKREREYSFhXHjjTdy2WWXMXnyZBITExk5cmSXz3n33Xdz5513MnbsWJydnXnjjTdwc3Pjww8/5J133sHFxYXBgwfz+9//nm3btvGrX/0KJycnXFxcWLp0aR9cpWV2OR96dV098b/9mocvGsG958f1csmEEN0l86H3rgExH7qbswlXZydJuQghRDN2mXIB8HV3plRSLkKIHtq7dy8333xzi21ubm5s3brVRiXqPrsN6N5uMie6EGcirXWX+njb2tixY0lKSrJ1MdroTjrcLlMuYIwWLZN+6EKcUdzd3SkoKOhWMBKnaa0pKCjA3d29S8+TGroQotdERkaSlZVFXl6erYti99zd3YmMjOzSc+w2oPu4O3PsVIWtiyGEaMbFxYUhQ4bYuhgDlt2mXLxlXVEhhGjBbgO6ryxDJ4QQLdhtQG9chk4aX4QQwmBVQFdKzVVKHVRKpSqlHrWwf7FSKk8pldTw7/beL2pL3m7OmDWU18iqRUIIAVY0iiqlTMAS4EIgC9imlFqptd7X6tAPtdb39kEZLfJpNuNi42RdQggxkFlTQ58KpGqt07TWNcAHQNeW++gDjXOiSx5dCCEM1gT0CCCz2eOshm2tXaWU2qOUWq6UirJ0IqXUHUqp7Uqp7T3tp9o446IM/xdCCIM1Ad3SGN7WLZGfA7Fa63HAt8Cblk6ktX5Faz1Zaz05ODi4ayVtxUfmRBdCiBasCehZQPMadySQ0/wArXWB1rq64eGrwKTeKV77fGTVIiGEaMGagL4NiFNKDVFKuQKLgJXND1BKhTV7uADY33tFtMxHcuhCCNFCp91DtNZ1Sql7gdWACXhNa52ilHoS2K61Xgncr5RaANQBp4DFfVhm4HSjqKxaJIQQBqv6+2mtVwGrWm37fbOfHwMe692idczb1Sh6iaRchBACsOORok5OqmHGRUm5CCEE2HFAh4bh/1JDF0IIwM4DusyJLoQQp9l1QG+coEsIIYTdB3SZQlcIIRrZdUD3dneWof9CCNHArgO6r6xaJIQQTew6oHu7SS8XIYRoZNcB3cfdhcraemrrzbYuihBC2JxdB/TGhS2kli6EEHYe0H1kPhchhGhi5wHdmEK3RLouCiGEvQd0SbkIIUQjhwjo0nVRCCHsPKA3NYpKDl0IIew7oDfm0GX4vxBC2H1Al0UuhBCikV0HdDdnJ1xMSlIuQgiBnQd0pZTMuCiEEA3sOqCDzOcihBCN7D6g+8iMi0IIAThAQPd2kznRhRACHCCgGzl0CehCCOEAAd2ZsmppFBVCCIcI6FJDF0IIBwroWmtbF0UIIWzK7gO6t5sL9WZNVa2sWiSEGNjsPqCfnnFR8uhCiIHNcQK6dF0UQgxwjhPQpWFUCDHA2X1A93YzptCV4f9CiIHO7gO65NCFEMLgOAFdcuhCiAHO/gO6W+OqRRLQhRADm90HdG9JuQghBGBlQFdKzVVKHVRKpSqlHu3guKuVUlopNbn3itgxk5PC09UkjaJCiAGv04CulDIBS4B5wGjgeqXUaAvH+QD3A1t7u5CdkflchBDCuhr6VCBVa52mta4BPgAWWjjuj8DfgKpeLJ9VvN2cZV1RIcSAZ01AjwAymz3OatjWRCk1AYjSWn/R0YmUUncopbYrpbbn5eV1ubDt8XF3oURy6EKIAc6agK4sbGua2lAp5QQ8C/yysxNprV/RWk/WWk8ODg62vpSdMOZElxq6EGJgsyagZwFRzR5HAjnNHvsACcBapVQ6MB1Y2Z8No5JDF0II6wL6NiBOKTVEKeUKLAJWNu7UWhdrrQdprWO11rHAFmCB1np7n5TYAh83F+nlIoQY8DoN6FrrOuBeYDWwH/hIa52ilHpSKbWgrwtoDW93Z+mHLoQY8JytOUhrvQpY1Wrb79s5dnbPi9U1Pu7OlNfUU2/WmJwspfyFEMLx2f1IUTC6LQLSMCqEGNAcIqD7ujfO5yJpFyHEwOUQAb1xxkWpoQshBjKHCOjesmqREEI4RkD3cZdVi4QQwiECemOjqAz/F0IMZA4R0H0lhy6EEI4R0CWHLoQQDhLQPVxMmJyUdFsUQgxoDhHQlVLGjItSQxdCDGAOEdDBaBiVlIsQYiBzmIDu4+5CqTSKCiEGMMcJ6G4y46IQYmBznIAuqxYJIQY4hwno3rJqkRBigHOYgC7L0AkhBjoHCuiyDJ0QYmBzmIDu7eZMTb2Zqtp6WxdFCCFswmECusznIoQY6BwmoMt8LkKIgc5hArqPm8yJLoQY2BwmoAd6uwKQXlBu45IIIYRtOExAHx/pz2Bfd1bsyrZ1UYQQwiYcJqCbnBRXTozgh0N55JZU2bo4QgjR7xwmoANcPSmSerOWWroQYkByqIA+NNibSTEBLNuRhdba1sURQoh+5VABHeCaSZGk5paxO6vY1kURQoh+5XABff64MNxdnFi2PbPD47TW3PPeTl747nA/lUwIIfqWwwV0H3cX5iWEsXJ3TofTAHy8M5sv9xznk52SbxdCOAaHC+hgpF1Kq+r4376TFvcXltfw51X7cXZSpOWXc6q8pp9LKIQQvc8hA/r0oUFE+Hu0m3b569cHKK6s5fHLRgOwM6OwP4snhBB9wiEDupOT4qpJkWxIzSenqLLFvu3pp/hgWyY/PWsI10yOwtlJseOYBHQhhP1zyIAOcPXESLSmRZ/02nozv/00mXA/dx6YE4e7i4kxEX7skBq6EMIBOGxAjw7yZNqQQJZtz2zqk/76xqMcOFHKEwvG4OVmzM44KTqA3ZlF1NabbVlcIYToMYcN6ADXTI4ivaCCHRmFZBdV8uw3h7lgVAgXjRncdMykmACq68zsyymxYUmFEKLnrAroSqm5SqmDSqlUpdSjFvbfqZTaq5RKUkptUEqN7v2idt28hMF4uppYtj2LJ1amAPDEgjEtjpkY4w8gaRchhN3rNKArpUzAEmAeMBq43kLAfk9rPVZrnQj8DXim10vaDV5uzswfG8bHO7P4Zt9J7p8TR2SAZ4tjwvw8iPD36FHDaGpuGX9fbfScEUIIW3G24pipQKrWOg1AKfUBsBDY13iA1rp5vsILOGMmUrl6UiTLdmQxItSb288eYvGYiTEBbE8/1eVzF5bX8K81h3lnSwZ1Zk1kgCfXT43uaZGFEKJbrAnoEUDzDt1ZwLTWByml7gEeAlyB8y2dSCl1B3AHQHR0/wS+qUMCue/84cxLCMPFZPkLyaRofz7fnUNOUSXh/h6dnrOmzsxbm9N5fs1hyqrruH5qNCt355CcLfPHCCFsx5qArixsa1MD11ovAZYopW4AfgvcauGYV4BXACZPntwvtXilFL+8KL7DYybFBAJGHr2jgK615pt9J3n6qwMczS/n7LhB/Hb+aOIH+5CWV06yNKwKIWzImkbRLCCq2eNIIKeD4z8ALu9JofrbyDAfPFxMnTaMLtuRxR1v78DkpHj9tim89ZOpxA/2AWBMuC/7j5dI90chhM1YE9C3AXFKqSFKKVdgEbCy+QFKqbhmD+cDdjWFoYvJifFRfuzsoGFUa81/1qcxJtyXrx44m/PiQ1Dq9JeXhAg/aurMHMkr648iCyFEG50GdK11HXAvsBrYD3yktU5RSj2plFrQcNi9SqkUpVQSRh69Tbql15xMga//DyqLevW0k2ICSMkpoaKmzuL+zUcKOHSyjMUzYy3m4hMifAFIzpa0ixDCNqzJoaO1XgWsarXt981+fqCXy9W+kuOwZQmMvARiz+q1006KCaDerNmTVcz0oUFt9r+xKZ1AL1cuGx9u8flDBnnj4WIiObuYqydF9lq5hBDCWvY3UnTwWOP/E3t79bQTogIAywOMsgor+Hb/SRZNicLdxWTx+SYnxehwX1JypKeLEMI27C+g+4SCV0ivB/QAL1eGBXtZnEr3nS3HUEpx0/SYDs+REO7LvpwSzOYzphu+EGIAsb+ADkYt/cSeXj/tpJgAdhwrbLHAdFVtPR9sO8ZFo0M77aM+JsKP8pp60gvKe71sQgjRGfsN6LkHoK53VxqaFBNAUUUtafmnA/LKpByKKmq5ZUZsp89PCPcDkP7oQgibsM+AnnAlLHwBdO/2+Z4U0zKPrrXmjU3pxIf6MH1oYKfPjwv1xtXkRIqMGBVC2IB9BvSw8TB+Ebi49+pphw7yxs/DpSmPvj2jkH3HS7h1ZmyLPuftcTE5MTLMh2RpGBVC2IB9BnSAfSvh4Fe9ekonJ8XEaP+mGvqbm9LxdXfm8gmWuypaMibcj+TskhZ5eCGE6A/2G9A3/gs2L+n1006KCeBwbhmHTpbydfIJrpsShaerVd31AWOAUXFlLVmFlZ0fLIQQvch+A3pjT5derglPbMijP7J8D/Vac/P02C49v7FhVPqjCyH6m30H9KpiKM7s/NguGB/pj8lJkZRZxJyRIUQHeXb+pGbiB/tgclIyBYAQot/ZcUAfZ/zfywOMvNycGdkwg+KtM2O7/Hx3FxNxId7SMCqE6Hf2G9BDRwOq1wM6wPxxYUwdEsisYYO69XyjYbRYGkaFEP3KfgO6qxfMfgyi2iye1GN3zx7ORz+fgZNT510VLUmI8CW/rIbc0upeLpkQwlqbjuQz5U/fkltaZeui9Bv7DegAs38Nw86zdSnaSIhoGDEqA4yEsJlv9+WSV1rN2gN5ti5Kv7HvgF5eACmfQnWprUvSwqgwX5SCFJkCQPSjl344wh+/2Nf5gQNE44I1Pxzq/4C+/3gJc59bx4ni/v12YN8BPWcXLLsVju+2dUla8HZzZsggL6mhi3718Y4sPkvqaHXIgaOqtp6UnGKUgvWH86jr56Uh392awYETpazae7xfX9e+A3ofzY3eGxLC/aSGLvpNRU0dR/LKyC+rbnfVrYEkJaeY2nrNwvHhlFTVkZTZuyucdaS23syXe4xAvubAyX57XbD3gN5Hc6P3hoQIX7KLKjlV3rszQgphyf7jpTROw3/sVIVtC3MGaJy+4745cZicVL+mXTYczqewopZRYb5sTTtFSVVtv722fQd06LO50XtKRoyK/tT8PjtWYN8BPaOgnJq6nqVIdmYUERXowbBgbyZE+fdrQP8sKRt/Txd+d+ko6syadf342vYf0MPG9cnc6D01pnFudBkxKvpBcnYxnq7G8oj2XEPfcDif8/6xlr+vPtDtc2it2XmskInRxjQe544IZk9WMfllfd+NuKKmjv/tO8m8hDCmDQkiwNOFNftz+/x1G9l/QB86GybeDLVn1ipBfp4uRAV6yIhR0S9SckqYFBOAj7uz3Qb0zFMV3Pv+TswaVuzK6XZDZnZRJbml1U3rG5wbHwwYHxZ97dv9uVTU1LMwMRyTk+K8kSF8fzC33xplHSOgX/oseATYuiRtJIT7yWIXos9V19Vz6GQpCRF+xAR5kmGHKZeKmjrueHsHZrPm0XkjyS+rZnNaQbfOtfOY0QDaWENPCPcj0Mu1X9IuK5OyGezrztRYY0GcC0aFUlRR21Smvmb/AR2gKBNOnnn9bxMi/EgvqOjXRhEx8Bw+WUZtvSYh3I/oQE+7q6FrrXlk+R4OnCjh+esnsHhmLD5uzt3ugrkzoxAPF1PTnExOTopz4gax7lBeny7gXlhew9qDeSxIDG8aZX523CBcTIpv9/dPbxfHCOjLb4OvHrF1KdoYE+4LwD7pvthtNXVm3t2aQVVtva2LcsZqHO+QEOFLdKAXWYUV1Pdh4Optr6xL44s9x3n4onhmx4fg7mJibsJgvk4+0a2/+85jhYyL9MPZdDq8nRsfTEF5TZ92Jf4q+QR1Zs2C8acXxPFxd2H60CAJ6F3SR3Oj91Rjw+jeLEm7NKqqrae40vpvLO9tzeA3K5JZubvvBsxkF1Xyj9UH+33wSW9JzinGx82ZqABPogM9qa3XHC+2jwVW1h3K469fH+CSsYO5e/awpu2XT4igrLqO7w50rUGxqraefTklTesaNDonLhilYO3Bvmug/Cwpm2HBXk0VuUZzRoaQllfO0fy+b+dznIBeVQxFx2xdkhaCfdyID/Xhs93ZMvNig198kMT859dbVfOqrTfz6vqjAPxwsO/yn/9cfZAXvk9lj522d6TklDA63BcnJ0VMw/z9PUm7aK15bcNRsgqtP8cnO7N4+qv9XXqdjIJy7nt/FyNCffj71eNbrNs7fWgQIT5ufLoru0vn3JNVTJ1ZMym6ZUAP8nZjbIRfn+XRc4oq+TH9FAsTI9qsPzxnVCgAa/qhlu4gAb1v5kbvDTfNiCE5u4Rd/ThS7UyVUVDO6n0nyCqs5L2tnX/4fr47h+yiSmKDPPts+HZOUWVT7X9XPzVc9aa6ejP7j5c0TQgXHdgQ0HvQMLpydw5PfrGPf6w+aNXxNXVmnv7qAC//kEZqrnXzKlXV1vPzt3cA8PLNk/Bya7nMo8lJcdn4cNYezKO4wvpvdI0DiiZE+7fZd+6IYHYeK+zS+Rql5ZVx+GT71/bFnhy0pkW6pVFUoCcjB/v0S9rFMQJ6yGhQTmdkQL9yQgQ+bs68tSnd1kWxuTc3ZWBSirERfry4NrXDIepms+alH44QH+rDwxfHU1JV1ycfim9sSkcD/p4u7GqYzMmepOWXU1VrJiHC+Jof5ueOs5Pqdg29qraev35l9AH/cu9xq6aeXZ1ygryGqaLf2WLdt+QPt2Vy4EQpz143npggL4vHLEwMp6bezNcp1s+HsvNYIbFBngR5u7XZd+6IYMwaNh7pWvfFVXuPc8nz65n//AaW78iyeMxnSTmMj/IndpDla5kzKoRt6d37MOkKxwjorp4w6jLw6t6CFH3Jy82ZqyZF8uXe4003/UBUVl3Hsu2ZXDI2jCcWjCG/rIbXN6a3e/z3B3M5dLKMu2YP4+y4YExOqtfznyVVtby39RiXjA1j1vBBvVJD33+8pF8nZWt8rcb2GmeTE5EBHmR0M6D/Z30aOcVVPH3lWGrrNR/82PkSj29vziA60JMF48P5eEcW5dUdzyVTb9b8Z0MaE6P9OX9kaLvHjY3wY8ggLz7dZV37idaaXc0GFLWWGOWPj7uz1ek7rTXPrznM3e/uZEy4H5NiAnh42W7+vGp/i0bn1NxSUnJKWGihdt5ozqhQ6s2atYf6dpCRYwR0gGvfgqk/s3UpLLp5RkzDm+PMyvH3p+XbMymtruO2WbFMiglgzsgQXv7hSLsNpEvXHiHC34NLx4Xh5+HCxGh/1vZyHv2DH49RVl3HHWcPZUKUf8OAlJ5Nd/rI8j0semULR/LKeqmUHUvOLsHdxYmhzWqGUYGeZHYjoOeWVPHi2iNcPCaU66dGc86IYN7dmkFtB6muAydK+DH9FDdNj+bWmTGUVtfxaVLHee/VKSfIPFXJHecM7fA4pRQLE8PZcrTAqmloM09Vkl9W06ZBtJGzyYmz4wbxw6G8Ttu0qmrruf+DJJ755hBXTojg3dun8dZPp3Lz9BheWZfGT9/c1tQdeWVSDk4KLh0X1u75EiP9GeTtyrd9PGrUcQK61lCSA7Vn3uokw4K9OTtuEO9uPWa3PSl6wmzWvLk5g8QofyY01J4eumgEJVV1/Gd9Wpvjt6WfYntGIXecM7Sp69ns+BBSckrILemdv29NnZnXNqQzY2gQYyP9mnKuST2opVfV1rP/eAll1XXc9c6Ofpn1MCWnmFFhvi266HV3cNE//neQ2nozj80bBcCtM2I4WVLN6pQT7T7n7c0ZuDk7ce3kKCZGBzA6zJe3N2e0GzC11ry8Lo2YIE8uHD240zItGB+O1kZ7Smca5z9vr4YORtrlREkVh062/4GbW1LFda9s4Ys9OTwyN55/XjsedxcTLiYn/nh5Ak9dnsCGw/lcsWQjR/PLWbk7hxnDggjxdW/3nE5OivPiQ1h7MLfDD8iecpyAnrYWnhkFR9fZuiQW3TIjlhMlVXyzr3+n07Tk5R+O8NBHSZ1+Ne4ta2RIMlAAACAASURBVA/lcjS/nNtmxTZtGxPux/xxYby24SgFrebYWLr2CIFerlw7Oapp2+yG4dtre6mXwhd7cjhRUsUd5w5tKo+LSfUoT5+SU0KdWXPLjBgO55bx2Cd7+7R3k9ms2ZdT0jQRXKPoQE+KK2u7lK9Nzi5m2Y4sFs+MbcoDz44PITrQkzfbaf8pqaplxa5sFowPx9/TFaUUN8+I4cCJUrZnWG6P2J5RyO7MIm4/awgmK5Z4HBrszbhIPz7b3Xlvlx0ZhXi5mohvGFBkyTkjjPvoh3ZSH3uzilm4ZCOHTpTy0k2TuHv28Da9Vm6aHsNbP51KQXkN859fT3pBBQvHR3RavjmjQimtqmNb+qlOj+0uxwnoMTON4f97PrB1SSw6f2QIEf4evLk53ablMJuNGtInO7O57pXNvVbj7cjrG9MJ8XFjXkLLr6QPXjCCytp6lq490rRt//ESvjuQy20zY/FomGwKYHSYLyE+br3SfVFrzSvr0ogL8WZ2wxvc3cXE6DDfHjWM7m74MLh79nAeumAEnyXl8M6WjB6Xtz3HTlVQWl3X1CDaKDrQq2m/NbTWPPXlPgI8Xbn3/Lim7SYnxc3TY9iWXmhx1tBPdmRRUVPPzTNimrYtTAzHx92Ztzdbvu5X1qUR4OnC1ZOiLO63ZGFiBMnZJaTmdpzG2nmskPFR/h1+UIT5eRAf6tOi+2JheQ3vbMngmpc2cdkLG1DA8rtmcPGY9r9BzBw2iJX3nEVkgAderiYuTuj828bZcYNwdXbq08m6HCegO7vB2Gtg/xdQeeb1VjA5GbWXLWmnOHjCdkvm7cku5lR5DddNjiItr5wrXtzEoQ66Y/VUam4p6w/nc/P0GFydW95uw0O8uXJiJG9tyWgaCPPyD0fwcjVxy4zYFscqpTh3RHCvdF9cfzifAydK+dk5Q1vUviZEBxj9mLt5/j1ZRYT6ujHYz517zhvOefHBPPnFvj7rPdM48dsYCzV0sD6g/2/fSbakneLBC+Lw83Bpse/ayVG4uzjx1qaWAVprzdtbMhgf5c+4yNNdBD1dnbl6UiRfJbftBHAkr4xv95/k5ukxLT6sO3PZuDCclDFPSnsqauo4cKK0w3RLo3Pjg/nx6ClW7Mri9je3MfXP3/LbT5MprKjl4YtG8Pl9Z7X5nVoSHeTJynvP4puHzm3ze7PEy82ZmcOCWLP/ZJ99c3OcgA4w4Saor4a9y21dEouumxyFm7MTb21O7/JzK2rqWJ1yosdDur8/kItS8Ot5I/no5zOoqTdz1dJNbOpiV65GpVW1Hd6cr29Mx9XZiRumRVvc/8CcOLTW/Pu7VDJPVfD5nuPcMC0aP8+2b5DZ8SG90n3x1fVphPi4sTCxZa+ExCh/KmrqO8yvdmR3VnFTcHNyUjx7XSKhvu7c8+7OPlnoJDm7BBeTIi7Uu8X26IbBRRmnOh+ZWFNn5ulV+4kL8eb6qW3/Rn6eLlwxIYJPk7Ipqjh9DZuOFHAkr5xbpse0ec5N041OAB9ua9kJ4L8bjuJicuLmVh/WnQnxdWfmsEF8tjun3Xttd2Yx9WbdNMNiR2aPCKa2XvPgh7vZm13M4pmxfHHfWXzz4Dnce36cxS6P7XF3MRHu72H18XNGhZJeUMGRvL4ZNWpVQFdKzVVKHVRKpSqlHrWw/yGl1D6l1B6l1BqlVNu/cn8IGw+hYyHpXZu8fGcCvFxZMD6cFbuyuzRhV229mTvf2cnP397BQx8l9aiGuvZQHolR/gR6uZIQ4ceKu2cy2NedW1/7kRW7LPextaSoooYnVqaQ+OQ3LH59GyctpG6KK2r5ZGc2C8eHt/smiQr0ZNGUaD7alskfPk/BScFPz7Lc++GsuEE97r64L6eE9YfzWTwrFjfnlrXEpobRbnxgFFfUcjS/nMSo07VVf09Xlt44ifzyGh74YFevz6+SklPMiFCfNtfh7eZMkJerVT1d3tqcTnpBBb+ZP6pFw2pzt8yIpbrOzEfbT3dhfHtzBoFersy30LNjWLA3Zw0fxHvNOgHkl1Xz8Y4srpoYQbCP9QGz0YLEcDIKKtjdzjQajQ2ilgYUtTZ9aBB/WDCG926fxqZH5/Cb+aNJiPBrkyvvC3NGhgDwXR8tTddpQFdKmYAlwDxgNHC9Ump0q8N2AZO11uOA5cDferugVptxDww9D+rPzHUVb5kRS0VNPR+3M0ChNa01v12RzLpDeVw8JpTPknK4971d3VrRJb+smj1ZRZwXH9K0LTLAk+V3zWRSTAAPfribv319gJyi9ucBqa0388bGo5z797W8tTmdC0eFsvVoARc/t44v9rTsifDBtmNU1tZz26whHZbr3vOHY3JSfLs/lysmRDDYz3JvAT8PFyZFB/So++Kr69PwcjVx47S2dY7oQE8CvVy7lSLZk218CIyPbBlQxkb68eSCMaw/nM9z3x7qXqEt0FqTYqFBtFG0FT1dCstr+Neaw5w7IpjZze6J1kaF+TJ1SCBvbc6g3mzME/PN/pMN6RjLqZObpseQU1zFmoa5WN7enEF1nbndD+vOzE0YjKuzEy+tPUKZhcb8nRmFDA32wt/TtdNzOTkpbp0Zy8zhg6xqmO1N4f4efHzXTBbP7Pg90V3W1NCnAqla6zStdQ3wAbCw+QFa6++11o13zxYgsneL2QWJ18MFj4PJufNjbaCxi9zbmzOsmsrzhe9S+XB7JvefP5yXb57M7y8dzdcpJ7jznR1dnolu3aE8tKZFQAcjUL75k6lcOSGCF9ceYeZfvuOSf63nmf8dZE9WUVM5vz+Yy9zn1vHE5/tIiPBl1QNn89LNk1h1/9nEBHlx73u7uP/9XRRX1FJXb+atzRlMGxLI6FaTFbUW6uvO4lmxmJwUd5wzrMNjz40P7nb3xZyiSj7fncN1U6It5jyVUkyI8u9WSmdPQ81xbGTbAHvdlCiumRTJv79L5Z//O9gr+dPjxVWcKq9p0yDaKMaKaXT/t+8EpVV1PHxRfKevt3hmLFmFlXx/IJf3th7DrDU3tpNGA7hgVAhhfu68syWDypp63t6SwQWjQhke4t3uczri6+7CHWcP5euUE8z++9oWtX+tNbsyi6zKn58JJsUEtGlP6i3WnDUCaD5cLKthW3t+CnxlaYdS6g6l1Hal1Pa8vD6cbL48Hza/eMYtS9fo1hmxpOWXsyG147z1il1Z/LNhYMODF44A4CdnDeGpyxP47kAuP3trO5U11gf17w/mMcjbrc1scABuziaeuS6Rbx86h0fnjcTLzcQL36ey4IWNTH96DVe+uJHbXt+GWcN/bpnMOz+dxsjBxnmGBnvz8Z0zeOjCEazae5yLn1vH31YfJLuoskVXxY48fFE83zx4Tqdv+J50X3x7SwYa+MlZ7ZdpQrQ/qbllXZoREow0zdBBXu1+UDx95ViumxzFv79L5ZHle3rcF7lxhOjo9mrogZ7kFFV2+E1uY2oBwT5u7X4oNHfh6FAG+7rznw1pvP9jJufHhxDV0PhqibPJiRumRrP+cD7/+N9BTpXXdDqQqDMPXxzPirtnEhvkyf+t2Mslz69n7cFc0gsqOFVeYzcBvS9ZE9AtfSexWMVQSt0ETAb+bmm/1voVrfVkrfXk4OBg60vZVdk7YPVjcHh1371GD8wbO5hB3q489sle3tqcbrE/+KbUfB5ZvoeZw4L4y1XjWuT3bpoew9+vHsfG1Hxuff1Hi19BW6urN7PuUB6z44ObJt+3ZHiID3eeO4xld85k+28v5JlrxzMlNpCiylp+O38Uq39xDheMDm2Tb3Q2OXH/nDhW3D0Lb3dnXlmXRoS/h1WDRwBcTE4MDe689tbd7otaaz7fncPZcYOIDGg/ECVGGUFhdxdr6XuyihhnoXbeyNnkxF+uGssDc+JYtiOLn721vUfjAJJzSnBSMCrMcp/r6CAvzNqYGtgSrTWbjhQwc1iQVbljF5MTN06LZkvaKfLLqlt0VWzPdVOjcDEp/rvhKOOj/JkS2/OAOyE6gGV3zmDpjROprjOz+PVt3PrajwBWNYg6OmsCehbQvNNoJNBm2JZS6gLgN8ACrbVtJy0ZNge8B8OuM7Nx1M3ZxL+vn8ggHzd+/1kK059ew5++3NfUiHXoZCk/f2cHQwZ5sfSmSRa/nl0zOYrnFk1gR0Yht/x3K6WdNLImZRZRXFnbVMO1RqCXK1dOjGTJjRP57pezuf3soZ1+VRwb6ccX953FLy8cwdNXju31HGV3uy+m5JSQVVjJJQntD88GGBflh1Jdm3nxRHEVJ0uqGR/VcYOcUooHG34v6w7lcf2rW7q9cPG+nGKGBXvj6Wo5tdhZ18VDJ8vIL6tm1jDr5z+6flo0riYnYoM8OSeu8/soxMeduQ2/7zvOHtprjY5KKeaNDeN/D57Db+ePoriyliAvV+K6mc5xJNYkmrcBcUqpIUA2sAi4ofkBSqkJwMvAXK11/y1x3R6TM4xfBJv+DaUnwaf9CYBsZcawID67ZxY7jxXy+sZ0XtuYzn83HOXC0aEkZ5fg4WLi9dumdti/dcH4cFxNTtz97g6eX3OY38xv3VZ92tqDeZicFGcP78NvRg3cXUzcNyeu8wO76byRISzbkcWuzCKmNKzd2JlVe49jclJcOLrje8HX3YW4EG+SMq1vGN2dZQT/cZGd97AAuH5qNMHebtz7/k6uWrqJN2+b2u4sfe1Jzi5hxrCgdvc3zYteUA60/ZtvbEj3zRze/jlaG+Ttxt+vGUeor3uH3/Kae+jCEUQFeHDxmN5/D7o5m7j97KFcMzmKipo6q8vkyDqtoWut64B7gdXAfuAjrXWKUupJpdSChsP+DngDy5RSSUqplX1WYmtNuAl0/Rk7crTRxOgA/n39BDb8+jzuPHcYW4+eoqiihtcWTyHCiv6tcxMGc/mECN7anNFhQ+H3B3OZFB1gsX+3vZk1vGvdF7XWfJ18gulDAwnw6rwXxISoAHZlFlndeLk7swhnJ2WxbaI9F4wO5b2fTaekspYrl25i/WHrU0h5pdWcKKnq8PWCvd1wc3Zqt4a+6Ug+MUGeHaafLFmYGMH0odZ/CAwZ5MUjc0e22yWyN/h5uBDmZ31fcEdm1W9Za71Kaz1Caz1Ma/2nhm2/11qvbPj5Aq11qNY6seHfgo7P2A8GxUHUNCPtYgerBYX5efDI3JFseWwOPzxyXtOCBdZ4YE4c9WbNku9TLe4/WVJFSk4Js0f2fe28P3S1++Khk2Wk5Ze3mXqgPROi/SmqqCXdygmu9mQVEz/Yp90ufO2ZGB3Ax3fNJMjLlVte+5G/fn3AqsbSlHZGiDbn5KSIDrTcdbGu3szWtFPM7EK6RdgHxxop2tqFT8LlS21dii5xdzExqAsj1QBigry4ZnIk7/+YabERrLEBsXV3RXvWle6Lq/YeRym4yMqv/YkNg1Os6Y9uNmt2ZxV1mj9vz9Bgb1beexaLpkSxdO0Rrn15c6cDghoXOu6sO2h0O10X92QXU1pdx6wupFuEfXDsgB49HSInQT+MALO1xkmVXvjucJt93x/MZbCvOyM7mIXO3jQ27lqziPDXySeYEhNIiE/705s2Fxfig5eryaqG0fSCckqr6hjfQQ+Xzni4mnj6ynG8cMMEUk+Wccm/1rcZpNVcSk4xMUGenc4fEh1kBPTWqaNNDfnzGV1InQj7cGaOvulN6Rth/T/guneNlY0cVIS/BzdMi+btLRncee6wpmW9auvNbDicz6Xjw/plaHN/GR3my4hQb15dn8bVkyLbzdGm5ZVx8GQpv7+0/Qbj1kxOivFR/lZNAdDYINrdGnpzl44LZ3ykP/e9v4t739vFukN5jAn340RJFSeLqzhRYvw7VlBh1beN6EBPKmrqKSivafGtb2NqAaPCfLs0Z4mwD45dQwdAw5Hv4Ie/grlrIyvtzd2zh+HspPjXmtO19B0ZhZRW13HuCMdJt4DRde2hC+M5klfOig5Whv8q2VicYa4V05s2NyHan/3HSzoduLU7sxgPFxPDrehDb42oQE+W3TmDu2YPY9mOLB5fmcKr69LYklZAZW098aE+3DQ9hvut6EXU2NOleR69qraeHccKmdVBDxlhvxy/hh4zCxKugo3PQeaPcMVSCIi1dan6RIivO7fOjOU/69O4e/Ywhof48P3BXFxMyiHzpRePCWVcpB/PfXuYBYnhbSapAvgq+TiJUf5dmhEPjJ4udWZNck5xh10jd2cVMTbCr1d7cbiYnPj13JEsnhmLUjDIy61bXfIa50XPPFXRNOhme3ohNXVmZg2XBlFH5Pg1dKXgqv/C5S/ByWRYOguSP7F1qfrMz88ZioeLiWe/NWrpaw/kMSU2EB93+++u2JpSiocviie7qNLiYsaZpypIzi5hXhdr52Bdw2hNnZmUnJIOR4j2RKivOyE+1vf5bi0ywAOlWtbQNx7Jx9lJMXWIdf33hX1x/IAORlBPvB7u2gSRk8HHuu5r9ijI243bZg3hyz3HWbP/JAdPljpU75bWzo4bxLQhgfz7u9Q2a3h+3ZBusba7YnODvN2ICvTosGH00MlSaurMvZI/7wvuLiYG+7q36OmyKTWfxCh/vNwc/8v5QDQwAnoj/yi4+VOImQFmMyz/CaR+a+tS9bqfnT0UH3dnfvFBEgDnOUj/c0uUUvzq4njyy6p5s9WqOl8lH2dMuG/Tgg9dNSEqoMOA3thomniGBnQwcvLHGha6KK6sZW92MTMl3eKwBlZAh9NdGCsLIe8gvH8DZGy2bZma0xrquzbTX2t+nsZUo6XVdYzyr2dYLzXYnakmxwZyXnwwL/1wpGmWxOPFlew8VtStdEujCdH+nCipaloer7U9WUUEeLoQGXDmjlJsPo3ulrQCzBpmSoOowxp4Ab2RVxDc+jn4R8P718HJfbYukWHNH+DZMVBwpPNjO3DbWUO4wCuNFbV3og6dmbNO9qZfXhRPcWUt/1mfBsDqpt4t3U+vNTYkPvfNYYsTge3OLGZ8lP8Z3R00OtCTkyXVVNXWsyk1H3cXJ6tW9RH2aeAGdADPQLj5E3D2gHeugmLrl2DrE0WZsHkJlJ00ylNxqnvnMdfjnf4Nz963CNeQEfDRzXDY8VJLzSVE+DF/XBj/3XCU/LJqvko+wYhQ724vqAAwNsKPu2YP48PtmW2muy2vruNwbqnVE3LZSmO6KfNUBRuPFDAlNtBibyDhGAZ2QAejhn7Tx1BTBmv/YtuyrGuYRv6aN2DExeBm/WRPLex6B95fhE/+bpxu/gSC4+GDG+DI971W1DPRgxeMoKq2nqe+2Me29FM9qp2DkZ//9dyR/OmKBH44lMe1L29uWjs1ObsYs4bEqL7p4dJbGqfR3ZZeSGpumXRX7AsHvoSDX9u6FMBA6IdujcEJsPhLY0Kv3lJdZnSTPJkCwy+AgE4WBKitgrTvYdJtMOYK4x8Yef6g4eBkZa2quhS+ewqiphvzwisFN38Gb14G718PNy6DIWf37NrOUMNDvLlqYiRTd/+WC5xrGD7qnV45743TYgj38+Ce93ZyxZKNvPGTqU1LzvVpDb3kOPgM7tHUFY0jhj/YdgygS/Ofi07seBNMLrD1JSg9ATHbwN22H/BSQ28UNg5cPIwA+uUvu7bIdHXp6eM3vwgvTIGnI+G1i+HLh+C966Cmk5n7XNzhnm1w/m9ObytMh5fPha8ftX7GyPXPQHkuzP3z6UDgFQS3fAbBI6C2/QWgHcEDF8TxNTO41LSF+HX39toyhOeNDOGjn8+gzqy5aukmPtmVTYS/R8uJ1OprjV5Tx3f3/AW3vw7PjISvHunRaQI8XfBxc2ZPVjF+Hi6dTujVryq7sBi32Wx889z6MlQV912ZrFVyHFb/H6SsgEufg/I8WPOkrUslNfQ2MjbCtv/AqaPG5F6+4eAbAVFTwdXLCBC5KcYyd9k7jf/zDsJPv4GoKeDkbNSoE66CsESorYCk96Cuqv25ZIoyjRq4b7gR2BsFxMKUn8LmF8AvCmbd33HZCzOMHPy46yBiUst93sHws7Xg5GR8OJxKg6COF2S2Wn2tUVMBI6CVF0BFAYSOgSHn9N/kaDlJRAbEsuDqWzmU7cmIbY/DssVGCsu583nQ22U2Q9r3JERNZcU9s7jt9R/Zf7yES8Y29KApOAJblkLKJ8Z1o+Cch+HcR7u/WLmLp/E3//EVCE2ASbd2/pzaKjDXgtvpSdiUUkQFerLveAkzhgb1/ir3WhuBuTwfPAKM+6wz9bVG4/+mf8PohXDJP8C7g7ESZbmw4k44ssZ4vOZJSLwBZj9mtIPZwje/M65j3l8hcChM/blRUx+3yIgD7Sk4Yrz34i7sk2JJQG9t8k+MxsitL52+gQDuT4LAIUaAOPilsc1zkBE4x1xx+kaedofxr7kxVxhBzWw2Ampr//stpG+Ah/aBc6sJky78o9FY+83vwC/C+KBoz9aXQTnBnMct72987fX/NNIyCVcZb4pBw9s/Z0cOfAlfPmxc+8/XGds+Wgw1paePGTwOZj0AY660fO0dObbFKOtlz4NvJ/nwslx49xoIT2Thjcsg8RcQ7AWrHobltxlB3dSF0bIHVsGut+Hat4wP7HeuBGUiImw8n4+YznL/CMYOb+iuWFlkHBs/DxKuhoNfGe0h0dONdFtX5OyC8Akw/jrj7/PeNcY3xkEjjPET7SnMMBrSy/Ng3t9g3LVNH6QxQUZA77XpH9J+MO7Z8jzjn7nh22nMWXDblx0/t/QELLsNjm2C4RfCodVGMOwooKeuMSpa858x3m9bX4KUT+GCJ4z9+alG5afw6Ok0Z10VXPSUsb/keOf3T1ccXQ97l8E5jxjBHIxv1vtXwhe/gDvWWr7XCo4YqU9thvt3GRmB3qa1tsm/SZMm6TNebZXWBWlaH12vdV2NsS11jdZ7P9b6VLrWZrP158raofUL07QuONJye85urR/31XrNU+0/t6ZS6/9erPUfgrTevLT94+pqtM7c1nlZKk5p/c0TWj81WOsn/LVecZfWp45adRlaa62rSrX+7F6j3EvP0vrHV0/vy96pdX6q1qW5Wu94U+vnJ2m9dNbp31VtVefnz9ii9ZsLjfP/bZjWR9ZqXVut9cbnT/8dmquvN47/Y4jWJ1Ja7tvyktZ/G2799dXXaf3tk8Zrv3SOcR01Fcbffc0ftX5tntZPBhv7X5tnPMds1rqyuOV5jv3Y7Heyy7rX3rzUOG/yitPbKk5p/a8JWj8zxvgdWJKzW+u/x2n9dJTWr5xnnGPFXU27//zlPh3z6y90am5py+dte03rp8K0/vJXWlcUdly2E8mny5W5Tet3r9X607uN+2jTEq13vad17kFj//E9Wqeta3uO2iqtn0kw7rvdHxrbSnON/81m4z1QlNVwbLXWB1ad3leY0fJcNZUNv59C4xqeCDCu+3Ff455++Vzjefs+N+6L/V92fH3Wqqsx3sfPJGhdXd5y3/4vtH5uvBEzWivL1/qfo7T+S6zWx/f2qAjAdt1OXJWA3l8KM4w33Mvntnxjvnudsb2zN1TFKa0/+bnWB74yHpeeNAKN1saN25WA3Kg0V+uv/8+44f8Q2DYYWpK5zbhpH/cz3sztBZlG9fVaF+cYPx/fa9zQK+/Xeu1ftd7+uvGmzT1g7M89oPVblxtvyr8O1XrDv7SuLjP27VtpbH/7qtPbGq37p7Fv++uWy9D4u62p1Lqutv2yluWf/iD59J7TQaO1mkqtj23VuuR4x9eutdYHVxvn+/zB038vSzYtMY57/4a2v9PcA1pnbW//uftWGgHm5D7jA2nTC1onf2Lsq63Sh0+U6Be/T9Vms1nrzO1ap35n7CtI0/q9Rcbf8m/DjKDcupJSX298kD45SOvnxnX+99Za6w9vNq7lkzu1LsszzllfZ+zb+7Hl++zkPq3/GKr1nyON13t5tnGOE8kdv1Ztlda73tV69W+03vmO8eHZ/O9WXmB8yD0RoPWeZZ2XvTOFx7ReMt34oGivPJaYzVqv/VuPg7nWHQd0pW20PNvkyZP19u3bbfLaNrP/C/jwRph+j9FombUd/jMHzv+dkXPtig9uNHL45/2f8fXus3vgJ/8zFvToqpLjsOdDIzWiFHx0C3gGGTNVxsw0cvuN9iwzcphXvmzs64rcA/DdH430UlWzIfVjrjBSInmH4I35MPNemHK70WbR3I434IsHIXwi3PCR0dib+SO8NhdGL4CrX28/X2+uN1IytRXG0oSBQ4yv6QFDwC8SSo8b5ynLhUv+bl3O2hp1NfDdk0a+2HswhI42XnfirRCeCFUlsON1+Ob3MGoBXP1a+6mh+jrjq/74RcZ15iRB2Hjj59qqlu0vjT6927imybfBj68aPakiJsPPmqUTc3YZqbPs7RA9Axa9Z+SmizLh07sgfT3Ez4cFz4OXFb1kaiqMNQg2Pm/8DQePhYiJxgpiHTl1FD6/H46uM3qLLHjB+Lv2VHWp0TEhY5NxDRNv6dn56uuMNq/27rXCDOP9dM6vjHx5/mGIn9uz12xGKbVDaz3Z4j4J6P1s1a+Mhq7rPzT+P54ED+wBty4OgEnfAN88brwJwXhjNzZ69oS53ujemLHpdC48YAiMuvR0TrKmoueLhdRVG/nXspPGwK7QhgUomjewWrL/C/j4p0aD4U0fw56PIOkdI4ffWZexH181Gi+LjhmNh40e3Gd0D/ziF0a30YiJPbs2S1LXwM63oCjD6L10xcvGWIPNL8Lqx4zGwav+2/G171kGn9xutJE4uxm9LK54xci3W6K1cY99+4TxQeYVAjPvM9qJWt9vZjMkvQuHvobr3jEat5f/1MiPz/sLTLi5643buQeMD+CsH417Z/pdnT9Ha6MMg8cZbUa9pabCGGCX+i1c+qzxO+iqjc9D3EUQMrLj47YsNXqmzXncuOcac+a9tMCOBPQzSW0V/PcCKMmBG5cbvQNGXNS9c2kN+z83euVc8Hjbni09UV9nNDBlbDIapArT4aZPwMe6dTn7eMljUQAABTNJREFUVMZmY7qGkZfB5UuMWq57F7rjmeuhJNuoERYehQm39PyDsKu0NgLk8T1Go+uYyztvtNXa+DBL/th4PGoBXPmq5Zp5c4XpRlfKuIusb4jLO2hUPi577nTDX3dobXQz9DgDRtTWVRsfMNPuNLopVxUbvYna+72b66G6xLi/cnbBsluNWvf5v+34dcz18Or5RmXNMwhuWWmMdeklEtDPNPmpxlex7gZyYaRn/CLapmUcXU2FEViC4+GCP1g/4Ey0ZK6HJxu6PDq5GIHd1dMIwHdtNLa/ch7k7Dz9HL9ouGerdTXtE8lGz7QL/9irwRwkoAshREv1tUY6qqbCSEfVVkBNuTHw7spXjW9sSe8Zfezd/YxpOKJnWNfPvo91FNClH7oQYuAxucCMezo+JvGG/ilLL5Kh/0II4SAkoAshhIOQgC6EEA5CAroQQjgICehCCOEgJKALIYSDkIAuhBAOQgK6EEI4CJuNFFVK5QEZ3Xz6ICC/F4tjLwbqdcPAvXa57oHFmuuO0VpbHLJqs4DeE0qp7e0NfXVkA/W6YeBeu1z3wNLT65aUixBCOAgJ6EII4SDsNaC/YusC2MhAvW4YuNcu1z2w9Oi67TKHLoQQoi17raELIYRoRQK6EEI4CLsL6EqpuUqpg0qpVKXUo7YuT19RSr2mlMpVSiU32xaolPpGKXW44f8AW5axLyilopRS3yul9iulUpRSDzRsd+hrV0q5K6V+VErtbrjuPzRsH6KU2tpw3R8qpVxtXda+oJQyKaV2KaW+aHjs8NetlEpXSu1VSiUppbY3bOvRfW5XAV0pZQKWAPOA0cD1SqnRti1Vn3kDmNtq26PAGq11HLCm4bGjqQN+qbUeBUwH7mn4Gzv6tVcD52utxwOJwFyl1HTgr8CzDdddCPzUhmXsSw8A+5s9HijXfZ7WOrFZ3/Me3ed2FdCBqUCq1jpNa10DfAAstHGZ+oTWeh1wqtXmhcCbDT+/CVzer4XqB1rr41rrnQ0/l2K8ySNw8GvXhrKGhy4N/zRwPrC8YbvDXTeAUioSmA/8p+GxYgBcdzt6dJ/bW0CPADKbPc5q2DZQhGqtj4MR+IAQG5enTymlYoEJwFYGwLU3pB2SgFzgG+AIUKS1rms4xFHv9+eARwBzw+MgBsZ1a+B/SqkdSqk7Grb16D63t0WilYVt0u/SASmlvIGPgV9orUuMSptj01rXA4lKKX9gBTDK0mH9W6q+pZS6FMjVWu9QSs1u3GzhUIe67gaztNY5SqkQ4Bul1IGentDeauhZQFSzx5FAjo3KYgsnlVJhAA3/59q4PH1CKeWCEczf1Vp/0rB5QFw7gNa6CFiL0Ybgr5RqrHg54v0+C1iglErHSKGej1Fjd/TrRmud0/B/LsYH+FR6eJ/bW0DfBsQ1tIC7AouAlTYuU39aCdza8POtwGc2LEufaMif/hfYr7V+ptkuh752pVRwQ80cpZQHcAFG+8H3wNUNhzncdWutH9NaR2qtYzHez99prW/Ewa9bKeWllPJp/Bm4CEimh/e53Y0UVUpdgvEJbgJe01r/ycZF6hNKqfeB2RjTaZ4EHgc+BT4CooFjwDVa69YNp3ZNKXUWsB7Yy+mc6v9h5NEd9tqVUuMwGsFMGBWtj7TWTyqlhmLUXAOBXcBNWutq25W07zSkXB7WWl/q6NfdcH0rGh46A+9prf+klAqiB/e53QV0IYQQltlbykUIIUQ7JKALIYSDkIAuhBAOQgK6EEI4CAnoQgjhICSgCyGEg5CALoQQDuL/AbNv2GRwLykKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model_conv,train_losses,val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n",
    "                       num_epochs=300)\n",
    "dump_output(model_conv,train_losses[0:50],val_losses[0:50],'with-pretrained-resnet50_lrscheduler_lastlayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.6072 Acc: 0.6230\n",
      "train Rajat Best_Acc: 0.0000 Epoch_Acc: 0.6230\n",
      "val Loss: 0.2913 Acc: 0.8954\n",
      "val Rajat Best_Acc: 0.0000 Epoch_Acc: 0.8954\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.3361 Acc: 0.8770\n",
      "train Rajat Best_Acc: 0.8954 Epoch_Acc: 0.8770\n",
      "val Loss: 0.1834 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.8954 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.2387 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9281 Epoch_Acc: 0.9098\n",
      "val Loss: 0.2209 Acc: 0.9085\n",
      "val Rajat Best_Acc: 0.9281 Epoch_Acc: 0.9085\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2992 Acc: 0.8484\n",
      "train Rajat Best_Acc: 0.9281 Epoch_Acc: 0.8484\n",
      "val Loss: 0.2348 Acc: 0.8758\n",
      "val Rajat Best_Acc: 0.9281 Epoch_Acc: 0.8758\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2662 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9281 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1805 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9281 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.3469 Acc: 0.8566\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.8566\n",
      "val Loss: 0.1607 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.2365 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1731 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.2377 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1428 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.1711 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1606 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.2446 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1474 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.1881 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1411 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.1568 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1447 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.2553 Acc: 0.8689\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.8689\n",
      "val Loss: 0.1510 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.1784 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1469 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.1976 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1591 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1879 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1483 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.2937 Acc: 0.8852\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.8852\n",
      "val Loss: 0.1626 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.1670 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1452 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.2434 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1454 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.1575 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1389 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.2253 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1378 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.2363 Acc: 0.8893\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.8893\n",
      "val Loss: 0.1382 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.1757 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1439 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.2060 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1528 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1556 Acc: 0.9590\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9590\n",
      "val Loss: 0.1397 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.1301 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1358 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.2080 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1429 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.1940 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1431 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.1584 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1480 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.2659 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1479 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9412 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.1725 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1343 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.1538 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1539 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.1451 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1396 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.2131 Acc: 0.9180\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9180\n",
      "val Loss: 0.1395 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.2185 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1727 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.2245 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1526 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.1735 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1486 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1642 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1396 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.1730 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1614 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.2189 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1447 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.2707 Acc: 0.8934\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.8934\n",
      "val Loss: 0.1510 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.1602 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1453 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1641 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1374 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.1918 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1487 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.1799 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1356 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.1716 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1436 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.1781 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1364 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.1537 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1386 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.2838 Acc: 0.8648\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.8648\n",
      "val Loss: 0.1404 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.2474 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1479 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.2100 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1412 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.2068 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1470 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.1199 Acc: 0.9672\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9672\n",
      "val Loss: 0.1349 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.1782 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1380 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.1710 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1451 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.2026 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1497 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.1933 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1392 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.1524 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1430 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.1623 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1496 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.1794 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1444 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.1643 Acc: 0.9344\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9344\n",
      "val Loss: 0.1447 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.1458 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1364 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.2068 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1453 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.1570 Acc: 0.9426\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9426\n",
      "val Loss: 0.1410 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.2270 Acc: 0.9016\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9016\n",
      "val Loss: 0.1509 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.2117 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1422 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.1958 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1408 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.2286 Acc: 0.9057\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9057\n",
      "val Loss: 0.1439 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.1897 Acc: 0.9262\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9262\n",
      "val Loss: 0.1362 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.1602 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1497 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.1526 Acc: 0.9508\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9508\n",
      "val Loss: 0.1423 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.1740 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1501 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.1529 Acc: 0.9631\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9631\n",
      "val Loss: 0.1618 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.2024 Acc: 0.9139\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9139\n",
      "val Loss: 0.1497 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.2125 Acc: 0.8975\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.8975\n",
      "val Loss: 0.1423 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.1712 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1437 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.1384 Acc: 0.9549\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9549\n",
      "val Loss: 0.1383 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.2045 Acc: 0.9303\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9303\n",
      "val Loss: 0.1439 Acc: 0.9412\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9412\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.1559 Acc: 0.9467\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9467\n",
      "val Loss: 0.1386 Acc: 0.9346\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9346\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.2035 Acc: 0.9221\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9221\n",
      "val Loss: 0.1377 Acc: 0.9281\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9281\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.1781 Acc: 0.9385\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9385\n",
      "val Loss: 0.1423 Acc: 0.9216\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9216\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.1963 Acc: 0.9098\n",
      "train Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9098\n",
      "val Loss: 0.1369 Acc: 0.9477\n",
      "val Rajat Best_Acc: 0.9477 Epoch_Acc: 0.9477\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9f553873911b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m model_conv,train_losses,val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n\u001b[0;32m---> 20\u001b[0;31m                        num_epochs=300)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdump_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_conv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'with-pretrained-resnet101_lrscheduler_lastlayer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c907b7687a7c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpuenv/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 340\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet101(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model_conv,train_losses,val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n",
    "                       num_epochs=300)\n",
    "dump_output(model_conv,train_losses[0:50],val_losses[0:50],'with-pretrained-resnet101_lrscheduler_lastlayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet152(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model_conv,train_losses,val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n",
    "                       num_epochs=300)\n",
    "dump_output(model_conv,train_losses[0:50],val_losses[0:50],'without-pretrained-resnet152_lrscheduler_lastlayer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
